<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="description" content="With  publish  command you can publish your dataset to a data publishing platform like CKAN:">
<meta name="keywords" content="publish">
<link rel="icon" href="../../assets/logo.png">
<title>Publish | Frictionless Framework</title>
<link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">
<link rel="stylesheet" href="https://unpkg.com/bootstrap@4.6.0/dist/css/bootstrap.min.css">
<link rel="stylesheet" href="https://unpkg.com/prismjs@1.23.0/themes/prism.css">
<style>

@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');

/* Base */

html {
  font-family: sans-serif;
  line-height: 1.15;
  -webkit-text-size-adjust: 100%;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}

body {
  margin: 0;
  font-family: Roboto, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  font-size: 16px;
  font-weight: 400;
  line-height: 1.5;
  color: #222;
  text-align: left;
  background-color: #fff;
}

*,
::after,
::before {
	box-sizing: border-box;
}

ul,
ol {
	margin-top: 0;
	margin-bottom: 1rem;
}

a {
	color: #007bff;
	text-decoration: none;
	background-color: transparent;
}

a:hover {
	color: #0056b3;
	text-decoration: underline;
}

hr {
  box-sizing: content-box;
  height: 0;
  overflow: visible;
}

p {
  margin-top: 0;
  margin-bottom: 1rem;
}

/* Main */

#livemark-main {
  padding: 24px 20px;
  min-height: 100vh;
}

#livemark-main h1 {
  margin-top: 4px !important;
  padding-bottom: 12px !important;
}

#livemark-main h1 a.heading,
#livemark-main h2 a.heading,
#livemark-main h3 a.heading,
#livemark-main h4 a.heading,
#livemark-main h5 a.heading,
#livemark-main h6 a.heading {
  display: none;
}

#livemark-main h2:hover a.heading,
#livemark-main h3:hover a.heading,
#livemark-main h4:hover a.heading,
#livemark-main h5:hover a.heading,
#livemark-main h6:hover a.heading {
  display: inline;
  margin-left: 8px;
  color: #aaa;
  font-weight: normal;
  text-decoration: none;
}

#livemark-main h2 a.heading:hover,
#livemark-main h3 a.heading:hover,
#livemark-main h4 a.heading:hover,
#livemark-main h5 a.heading:hover,
#livemark-main h6 a.heading:hover {
  text-decoration: underline;
}

@media only screen and (min-width: 768px) {
  #livemark-main {
    margin-left: 300px;
    border-left: dashed 1px #ccc;
  }
}

@media only screen and (min-width: 992px) {
  #livemark-main {
    margin-right: 300px;
    border-right: dashed 1px #ccc;
  }
}

#livemark-main .octicon {
  display: inline-block;
  fill: currentColor;
  vertical-align: text-bottom;
}

#livemark-main .anchor {
  float: left;
  line-height: 1;
  margin-left: -20px;
  padding-right: 4px;
}

#livemark-main .anchor:focus {
  outline: none;
}

#livemark-main h1 .octicon-link,
#livemark-main h2 .octicon-link,
#livemark-main h3 .octicon-link,
#livemark-main h4 .octicon-link,
#livemark-main h5 .octicon-link,
#livemark-main h6 .octicon-link {
  color: #1b1f23;
  vertical-align: middle;
  visibility: hidden;
}

#livemark-main h1:hover .anchor,
#livemark-main h2:hover .anchor,
#livemark-main h3:hover .anchor,
#livemark-main h4:hover .anchor,
#livemark-main h5:hover .anchor,
#livemark-main h6:hover .anchor {
  text-decoration: none;
}

#livemark-main h1:hover .anchor .octicon-link,
#livemark-main h2:hover .anchor .octicon-link,
#livemark-main h3:hover .anchor .octicon-link,
#livemark-main h4:hover .anchor .octicon-link,
#livemark-main h5:hover .anchor .octicon-link,
#livemark-main h6:hover .anchor .octicon-link {
  visibility: visible;
}

#livemark-main h1:hover .anchor .octicon-link:before,
#livemark-main h2:hover .anchor .octicon-link:before,
#livemark-main h3:hover .anchor .octicon-link:before,
#livemark-main h4:hover .anchor .octicon-link:before,
#livemark-main h5:hover .anchor .octicon-link:before,
#livemark-main h6:hover .anchor .octicon-link:before {
  width: 16px;
  height: 16px;
  content: ' ';
  display: inline-block;
  background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' version='1.1' width='16' height='16' aria-hidden='true'%3E%3Cpath fill-rule='evenodd' d='M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z'%3E%3C/path%3E%3C/svg%3E");
}

#livemark-main {
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  line-height: 1.5;
  color: #222;
  font-family: Roboto, system-ui, -apple-system,BlinkMacSystemFont,Segoe UI,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji;
  line-height: 1.5;
  word-wrap: break-word;
}

#livemark-main details {
  display: block;
}

#livemark-main summary {
  display: list-item;
}

#livemark-main a {
  background-color: initial;
}

#livemark-main a:active,
#livemark-main a:hover {
  outline-width: 0;
}

#livemark-main strong {
  font-weight: inherit;
  font-weight: bolder;
}

#livemark-main h1 {
  font-size: 2em;
  margin: .67em 0;
}

#livemark-main img {
  border-style: none;
}

#livemark-main code,
#livemark-main kbd,
#livemark-main pre {
  font-family: monospace,monospace;
  font-size: 1em;
}

#livemark-main hr {
  box-sizing: initial;
  height: 0;
  overflow: visible;
}

#livemark-main input {
  font: inherit;
  margin: 0;
}

#livemark-main input {
  overflow: visible;
}

#livemark-main [type=checkbox] {
  box-sizing: border-box;
  padding: 0;
}

#livemark-main * {
  box-sizing: border-box;
}

#livemark-main input {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}

#livemark-main a {
  color: #0366d6;
  text-decoration: none;
}

#livemark-main a:hover {
  text-decoration: underline;
}

#livemark-main strong {
  font-weight: 600;
}

#livemark-main hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #dfe2e5;
}

#livemark-main hr:after,
#livemark-main hr:before {
  display: table;
  content: "";
}

#livemark-main hr:after {
  clear: both;
}

/* NOTE: is it possible to find a better fix for not breaking TablePlugin's styles? */
#livemark-main table:not(.dataTable) {
  border-spacing: 0;
  border-collapse: collapse;
}

#livemark-main td,
#livemark-main th {
  padding: 0;
}

#livemark-main details summary {
  cursor: pointer;
}

#livemark-main kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  line-height: 10px;
  color: #444d56;
  vertical-align: middle;
  background-color: #fafbfc;
  border: 1px solid #d1d5da;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #d1d5da;
}

#livemark-main h1,
#livemark-main h2,
#livemark-main h3,
#livemark-main h4,
#livemark-main h5,
#livemark-main h6 {
  margin-top: 0;
  margin-bottom: 0;
}

#livemark-main h1 {
  font-size: 32px;
}

#livemark-main h1,
#livemark-main h2 {
  font-weight: 600;
}

#livemark-main h2 {
  font-size: 24px;
}

#livemark-main h3 {
  font-size: 20px;
}

#livemark-main h3,
#livemark-main h4 {
  font-weight: 600;
}

#livemark-main h4 {
  font-size: 16px;
}

#livemark-main h5 {
  font-size: 14px;
}

#livemark-main h5,
#livemark-main h6 {
  font-weight: 600;
}

#livemark-main h6 {
  font-size: 12px;
}

#livemark-main p {
  margin-top: 0;
  margin-bottom: 10px;
}

#livemark-main blockquote {
  margin: 0;
}

#livemark-main ol,
#livemark-main ul {
  padding-left: 0;
  margin-top: 0;
  margin-bottom: 0;
}

#livemark-main ol ol,
#livemark-main ul ol {
  list-style-type: lower-roman;
}

#livemark-main ol ol ol,
#livemark-main ol ul ol,
#livemark-main ul ol ol,
#livemark-main ul ul ol {
  list-style-type: lower-alpha;
}

#livemark-main dd {
  margin-left: 0;
}

#livemark-main code,
#livemark-main pre {
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 12px;
}

#livemark-main pre {
  margin-top: 0;
  margin-bottom: 0;
}

#livemark-main input::-webkit-inner-spin-button,
#livemark-main input::-webkit-outer-spin-button {
  margin: 0;
  -webkit-appearance: none;
  appearance: none;
}

#livemark-main :checked+.radio-label {
  position: relative;
  z-index: 1;
  border-color: #0366d6;
}

#livemark-main .border {
  border: 1px solid #e1e4e8!important;
}

#livemark-main .border-0 {
  border: 0!important;
}

#livemark-main .border-bottom {
  border-bottom: 1px solid #e1e4e8!important;
}

#livemark-main .rounded-1 {
  border-radius: 3px!important;
}

#livemark-main .bg-white {
  background-color: #fff!important;
}

#livemark-main .bg-gray-light {
  background-color: #fafbfc!important;
}

#livemark-main .text-gray-light {
  color: #666!important;
}

#livemark-main .mb-0 {
  margin-bottom: 0!important;
}

#livemark-main .my-2 {
  margin-top: 8px!important;
  margin-bottom: 8px!important;
}

#livemark-main .pl-0 {
  padding-left: 0!important;
}

#livemark-main .py-0 {
  padding-top: 0!important;
  padding-bottom: 0!important;
}

#livemark-main .pl-1 {
  padding-left: 4px!important;
}

#livemark-main .pl-2 {
  padding-left: 8px!important;
}

#livemark-main .py-2 {
  padding-top: 8px!important;
  padding-bottom: 8px!important;
}

#livemark-main .pl-3,
#livemark-main .px-3 {
  padding-left: 16px!important;
}

#livemark-main .px-3 {
  padding-right: 16px!important;
}

#livemark-main .pl-4 {
  padding-left: 24px!important;
}

#livemark-main .pl-5 {
  padding-left: 32px!important;
}

#livemark-main .pl-6 {
  padding-left: 40px!important;
}

#livemark-main .f6 {
  font-size: 12px!important;
}

#livemark-main .lh-condensed {
  line-height: 1.25!important;
}

#livemark-main .text-bold {
  font-weight: 600!important;
}

#livemark-main .pl-c {
  color: #666;
}

#livemark-main .pl-c1,
#livemark-main .pl-s .pl-v {
  color: #005cc5;
}

#livemark-main .pl-e,
#livemark-main .pl-en {
  color: #6f42c1;
}

#livemark-main .pl-s .pl-s1,
#livemark-main .pl-smi {
  color: #222;
}

#livemark-main .pl-ent {
  color: #22863a;
}

#livemark-main .pl-k {
  color: #d73a49;
}

#livemark-main .pl-pds,
#livemark-main .pl-s,
#livemark-main .pl-s .pl-pse .pl-s1,
#livemark-main .pl-sr,
#livemark-main .pl-sr .pl-cce,
#livemark-main .pl-sr .pl-sra,
#livemark-main .pl-sr .pl-sre {
  color: #032f62;
}

#livemark-main .pl-smw,
#livemark-main .pl-v {
  color: #e36209;
}

#livemark-main .pl-bu {
  color: #b31d28;
}

#livemark-main .pl-ii {
  color: #fafbfc;
  background-color: #b31d28;
}

#livemark-main .pl-c2 {
  color: #fafbfc;
  background-color: #d73a49;
}

#livemark-main .pl-c2:before {
  content: "^M";
}

#livemark-main .pl-sr .pl-cce {
  font-weight: 700;
  color: #22863a;
}

#livemark-main .pl-ml {
  color: #735c0f;
}

#livemark-main .pl-mh,
#livemark-main .pl-mh .pl-en,
#livemark-main .pl-ms {
  font-weight: 700;
  color: #005cc5;
}

#livemark-main .pl-mi {
  font-style: italic;
  color: #222;
}

#livemark-main .pl-mb {
  font-weight: 700;
  color: #222;
}

#livemark-main .pl-md {
  color: #b31d28;
  background-color: #ffeef0;
}

#livemark-main .pl-mi1 {
  color: #22863a;
  background-color: #f0fff4;
}

#livemark-main .pl-mc {
  color: #e36209;
  background-color: #ffebda;
}

#livemark-main .pl-mi2 {
  color: #f6f8fa;
  background-color: #005cc5;
}

#livemark-main .pl-mdr {
  font-weight: 700;
  color: #6f42c1;
}

#livemark-main .pl-ba {
  color: #586069;
}

#livemark-main .pl-sg {
  color: #959da5;
}

#livemark-main .pl-corl {
  text-decoration: underline;
  color: #032f62;
}

#livemark-main .mb-0 {
  margin-bottom: 0!important;
}

#livemark-main .my-2 {
  margin-bottom: 8px!important;
}

#livemark-main .my-2 {
  margin-top: 8px!important;
}

#livemark-main .pl-0 {
  padding-left: 0!important;
}

#livemark-main .py-0 {
  padding-top: 0!important;
  padding-bottom: 0!important;
}

#livemark-main .pl-1 {
  padding-left: 4px!important;
}

#livemark-main .pl-2 {
  padding-left: 8px!important;
}

#livemark-main .py-2 {
  padding-top: 8px!important;
  padding-bottom: 8px!important;
}

#livemark-main .pl-3 {
  padding-left: 16px!important;
}

#livemark-main .pl-4 {
  padding-left: 24px!important;
}

#livemark-main .pl-5 {
  padding-left: 32px!important;
}

#livemark-main .pl-6 {
  padding-left: 40px!important;
}

#livemark-main .pl-7 {
  padding-left: 48px!important;
}

#livemark-main .pl-8 {
  padding-left: 64px!important;
}

#livemark-main .pl-9 {
  padding-left: 80px!important;
}

#livemark-main .pl-10 {
  padding-left: 96px!important;
}

#livemark-main .pl-11 {
  padding-left: 112px!important;
}

#livemark-main .pl-12 {
  padding-left: 128px!important;
}

#livemark-main hr {
  border-bottom-color: #eee;
}

#livemark-main kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  line-height: 10px;
  color: #444d56;
  vertical-align: middle;
  background-color: #fafbfc;
  border: 1px solid #d1d5da;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #d1d5da;
}

#livemark-main:after,
#livemark-main:before {
  display: table;
  content: "";
}

#livemark-main:after {
  clear: both;
}

#livemark-main a:not([href]) {
  color: inherit;
  text-decoration: none;
}

#livemark-main blockquote,
#livemark-main details,
#livemark-main dl,
#livemark-main ol,
#livemark-main p,
#livemark-main pre,
#livemark-main table:not(.dataTable),
#livemark-main ul {
  margin-top: 0;
  margin-bottom: 16px;
}

#livemark-main hr {
  /* height: .25em; */
  height: 1px;
  padding: 0;
  margin: 24px 0;
  background-color: #e1e4e8;
  border: 0;
}

#livemark-main blockquote {
  padding: 0 1em;
  color: #666;
  border-left: .25em solid #dfe2e5;
}

#livemark-main blockquote>:first-child {
  margin-top: 0;
}

#livemark-main blockquote>:last-child {
  margin-bottom: 0;
}

#livemark-main h1,
#livemark-main h2,
#livemark-main h3,
#livemark-main h4,
#livemark-main h5,
#livemark-main h6 {
  margin-top: 24px;
  margin-bottom: 16px;
  font-weight: 600;
  line-height: 1.25;
}

#livemark-main h1 {
  font-size: 2em;
}

#livemark-main h1,
#livemark-main h2 {
  padding-bottom: .3em;
  border-bottom: 1px solid #eaecef;
}

#livemark-main h2 {
  font-size: 1.5em;
}

#livemark-main h3 {
  font-size: 1.25em;
}

#livemark-main h4 {
  font-size: 1em;
}

#livemark-main h5 {
  font-size: .875em;
}

#livemark-main h6 {
  font-size: .85em;
  color: #666;
}

#livemark-main ol,
#livemark-main ul {
  padding-left: 2em;
}

#livemark-main ol ol,
#livemark-main ol ul,
#livemark-main ul ol,
#livemark-main ul ul {
  margin-top: 0;
  margin-bottom: 0;
}

#livemark-main li {
  word-wrap: break-all;
}

#livemark-main li>p {
  margin-top: 16px;
}

#livemark-main li+li {
  margin-top: .25em;
}

#livemark-main dl {
  padding: 0;
}

#livemark-main dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: 600;
}

#livemark-main dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

#livemark-main table:not(.dataTable) {
  display: block;
  width: 100%;
  overflow: auto;
}

#livemark-main table:not(.dataTable) th {
  font-weight: 600;
}

#livemark-main table:not(.dataTable) td,
#livemark-main table:not(.dataTable) th {
  padding: 6px 13px;
  border: 1px solid #dfe2e5;
}

#livemark-main table:not(.dataTable) tr {
  background-color: #fff;
  border-top: 1px solid #c6cbd1;
}

#livemark-main table:not(.dataTable) tr:nth-child(2n) {
  background-color: #f6f8fa;
}

#livemark-main img {
  max-width: 100%;
  box-sizing: initial;
  background-color: #fff;
}

#livemark-main img[align=right] {
  padding-left: 20px;
}

#livemark-main img[align=left] {
  padding-right: 20px;
}

#livemark-main code {
  padding: .2em .4em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(27,31,35,.05);
  border-radius: 3px;
}

#livemark-main pre {
  word-wrap: normal;
}

#livemark-main pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

#livemark-main .highlight {
  margin-bottom: 16px;
}

#livemark-main .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

#livemark-main .highlight pre,
#livemark-main pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f6f8fa;
  border-radius: 3px;
}

#livemark-main pre code {
  display: inline;
  max-width: auto;
  padding: 0;
  margin: 0;
  overflow: visible;
  line-height: inherit;
  word-wrap: normal;
  background-color: initial;
  border: 0;
}

#livemark-main .commit-tease-sha {
  display: inline-block;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 90%;
  color: #444d56;
}

#livemark-main .full-commit .btn-outline:not(:disabled):hover {
  color: #005cc5;
  border-color: #005cc5;
}

#livemark-main .blob-wrapper {
  overflow-x: auto;
  overflow-y: hidden;
}

#livemark-main .blob-wrapper-embedded {
  max-height: 240px;
  overflow-y: auto;
}

#livemark-main .blob-num {
  width: 1%;
  min-width: 50px;
  padding-right: 10px;
  padding-left: 10px;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 12px;
  line-height: 20px;
  color: rgba(27,31,35,.3);
  text-align: right;
  white-space: nowrap;
  vertical-align: top;
  cursor: pointer;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

#livemark-main .blob-num:hover {
  color: rgba(27,31,35,.6);
}

#livemark-main .blob-num:before {
  content: attr(data-line-number);
}

#livemark-main .blob-code {
  position: relative;
  padding-right: 10px;
  padding-left: 10px;
  line-height: 20px;
  vertical-align: top;
}

#livemark-main .blob-code-inner {
  overflow: visible;
  font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;
  font-size: 12px;
  color: #222;
  word-wrap: normal;
  white-space: pre;
}

#livemark-main .pl-token.active,
#livemark-main .pl-token:hover {
  cursor: pointer;
  background: #ffea7f;
}

#livemark-main .tab-size[data-tab-size="1"] {
  -moz-tab-size: 1;
  tab-size: 1;
}

#livemark-main .tab-size[data-tab-size="2"] {
  -moz-tab-size: 2;
  tab-size: 2;
}

#livemark-main .tab-size[data-tab-size="3"] {
  -moz-tab-size: 3;
  tab-size: 3;
}

#livemark-main .tab-size[data-tab-size="4"] {
  -moz-tab-size: 4;
  tab-size: 4;
}

#livemark-main .tab-size[data-tab-size="5"] {
  -moz-tab-size: 5;
  tab-size: 5;
}

#livemark-main .tab-size[data-tab-size="6"] {
  -moz-tab-size: 6;
  tab-size: 6;
}

#livemark-main .tab-size[data-tab-size="7"] {
  -moz-tab-size: 7;
  tab-size: 7;
}

#livemark-main .tab-size[data-tab-size="8"] {
  -moz-tab-size: 8;
  tab-size: 8;
}

#livemark-main .tab-size[data-tab-size="9"] {
  -moz-tab-size: 9;
  tab-size: 9;
}

#livemark-main .tab-size[data-tab-size="10"] {
  -moz-tab-size: 10;
  tab-size: 10;
}

#livemark-main .tab-size[data-tab-size="11"] {
  -moz-tab-size: 11;
  tab-size: 11;
}

#livemark-main .tab-size[data-tab-size="12"] {
  -moz-tab-size: 12;
  tab-size: 12;
}

#livemark-main .task-list-item {
  list-style-type: none;
}

#livemark-main .task-list-item+.task-list-item {
  margin-top: 3px;
}

#livemark-main .task-list-item input {
  margin: 0 .2em .25em -1.6em;
  vertical-align: middle;
}

/* Left */

#livemark-left {
  visibility: hidden;
  position: fixed;
  width: 300px;
  padding: 10px 40px;
  padding-bottom: 100px;
  left: 0px;
  top: 25px;
  font-size: 125%;
  height: 100vh;
  overflow-y: auto;
  scrollbar-width: none;  /* Firefox */
  -ms-overflow-style: none;  /* Internet Explorer 10+ */
}

/* NOTE: we need to move it to DisplayPlugin (currently unsetting doesn't work) */
body:not(.with-readability) #livemark-left::-webkit-scrollbar {
  display: none; /* Chrome; Safari */
}

#livemark-left > div:not(:last-child) {
  border-bottom: 1px solid #eaecef;
  padding-bottom: 15px;
  margin-bottom: 15px;
}

@media only screen and (min-width: 768px) {
  #livemark-left {
    visibility: visible;
  }
}

/* Right */

#livemark-right {
  visibility: hidden;
  position: fixed;
  width: 300px;
  padding: 10px 40px;
  padding-bottom: 100px;
  right: 0px;
  top: 25px;
  font-size: 125%;
  height: 100vh;
  overflow-y: auto;
  scrollbar-width: none;  /* Firefox */
  -ms-overflow-style: none;  /* Internet Explorer 10+ */
}

/* NOTE: we need to move it to DisplayPlugin (currently unsetting doesn't work) */
body:not(.with-readability) #livemark-right::-webkit-scrollbar {
  display: none; /* Chrome; Safari */
}

#livemark-right > div:not(:last-child) {
  border-bottom: 1px solid #eaecef;
  padding-bottom: 15px;
  margin-bottom: 15px;
}

@media only screen and (min-width: 992px) {
  #livemark-right {
    visibility: visible;
  }
}

</style>
<style>



</style>
<style>

#livemark-brand {
  color: #888;
}

#livemark-brand ul {
  overflow: hidden;
  position: relative;
  padding-left: 0;
  margin: 0;
}

#livemark-brand li {
  list-style: none;
}

#livemark-brand a {
  display: inline-block;
  color: currentColor;
  position: relative;
  width: 100%;
  line-height: 100%;
  padding-top: 5px;
  padding-bottom: 5px;
}

#livemark-brand a.active {
  font-weight: 700;
}

#livemark-brand a:hover {
  color: #80b2e6;
}

</style>
<style>

#livemark-pages {
  color: #888;
}

#livemark-pages ul {
  overflow: hidden;
  position: relative;
  padding-left: 0;
  margin: 0;
}

#livemark-pages ul.secondary {
  margin-left: 20px;
  display: none;
}

#livemark-pages li.active ul.secondary {
  display: block;
}

#livemark-pages li {
  list-style: none;
}

#livemark-pages a {
  display: inline-block;
  color: currentColor;
  position: relative;
  width: 100%;
  line-height: 100%;
  padding-top: 5px;
  padding-bottom: 5px;
}

#livemark-pages li.active > a {
  font-weight: 700;
}

#livemark-pages li.group.active > a {
  font-weight: normal;
}

#livemark-pages li.group a.primary::after {
  content: "\f054";
  font-size: 16px;
  font-family: "Font Awesome 5 Free";
  font-weight: 900;
  position: absolute;
  top: 2px;
  right: 0px;
}

#livemark-pages li.group.active a.primary::after {
  content: "\f078";
}

#livemark-pages a:hover {
  color: #80b2e6;
}

</style>
<style>

.livemark-reference {
  display: block;
  /* font-family: ui-monospace, Menlo, Monaco, "Cascadia Mono", "Segoe UI Mono", "Roboto Mono", "Oxygen Mono", "Ubuntu Monospace", "Source Code Pro", "Fira Mono", "Droid Sans Mono", "Courier New", monospace;; */
}

.livemark-reference-heading {
    background-color: #f6f8fa;
    font-family: "Roboto Mono", ui-monospace, monospace;
    padding: 1em 0;
}

</style>
<style>

#livemark-notes {
  color: #aaa;
  text-align: right;
  font-size: 14px;
  float: right;
  visibility: hidden;
}

@media (min-width: 768px) {
  #livemark-notes {
    visibility: visible;
  }
}

#livemark-notes a {
  color: inherit;
}

#livemark-notes a:hover {
  color: #80b2e6;
}

#livemark-notes a[target="_blank"]:after {
  content: "\f35d";
  font-family: "Font Awesome 5 Free";
  font-weight: 900;
  vertical-align: text-top;
  text-decoration: none;
  display: inline-block;
  color: #ccc;
  font-size: 10px;
  margin-left: -1px;
}

#livemark-notes a[target="_blank"]:hover:after {
  color: #80b2e6;
}

</style>
<style>

#livemark-signs {
  color: #888;
  border-top: 1px solid #eaecef;
  margin-top: 24px;
  padding-top: 20px;
  height: 50px;
}

#livemark-signs .next {
  float: right;
}

#livemark-signs a {
  font-size: 20px;
  color: currentColor;
}

#livemark-signs a:hover {
  color: #80b2e6;
}

</style>
<style>

#livemark-rating {
  height: 46px;
}

#livemark-rating iframe {
  margin-top: -5px;
  border: none;
  opacity: 0.5;
}

</style>
<style>

#livemark-topics {
  color: #888;
}

#livemark-topics .toc>.toc-list {
  overflow: hidden;
  position: relative;
}

#livemark-topics .toc>.toc-list li {
  list-style: none;
}

#livemark-topics .toc-list {
  padding-left: 0;
  margin: 0;
}

#livemark-topics a.toc-link {
  color: currentColor;
}

#livemark-topics a.toc-link:hover {
  color: #80b2e6;
}

#livemark-topics .is-active-link {
  font-weight: 700;
}

#livemark-topics ul.secondary {
  margin-left: 20px;
  display: none;
}

#livemark-topics li.is-active-li ul.secondary {
  display: block;
}

#livemark-topics li.group a.primary::after {
  content: "\f054";
  font-size: 16px;
  font-family: "Font Awesome 5 Free";
  font-weight: 900;
  position: absolute;
  top: 2px;
  right: 0px;
}

#livemark-topics li.group.is-active-li a.primary::after {
  content: "\f078";
}

#livemark-topics a {
  display: inline-block;
  color: currentColor;
  position: relative;
  width: 100%;
  line-height: 100%;
  padding-top: 5px;
  padding-bottom: 5px;
}

#livemark-topics a:hover {
  color: #80b2e6;
}

</style>
<style>

.livemark-audio {
  padding-bottom: 10px;
}

</style>
<style>

.livemark-blog-item {
  margin-bottom: 16px;
  text-align: justify;
}

.livemark-blog-item h2 a:not(.heading) {
  color: #222 !important;
}

</style>
<style>

#livemark-cards .modal-lg {
    max-width: 1000px;
}

</style>

<script defer data-domain="framework.frictionlessdata.io" src="https://plausible.io/js/plausible.js"></script>
<style>

#livemark-display {
  display: flex;
  position: fixed;
  visibility: hidden;
  justify-content: space-between;
  font-size: 16px;
  color: #888;
  width: 240px;
  bottom: 20px;
  right: 30px;
}

@media only screen and (min-width: 992px) {
  #livemark-display {
    visibility: visible;
  }
}

#livemark-display .control {
  cursor: pointer;
  background-color:#fff;
  box-shadow: 0px 7px 10px #eee;
  border-radius: 50%;
  border: solid 1px #ddd;
  z-index: 100;
}

#livemark-display .control .fa {
  display: inline-block !important;
  opacity: 1 !important;
  padding: 15px;
}

.with-readability {
  font-size: 20px;
}

.with-readability #livemark-main {
  color: #000;
}

.with-readability #livemark-left > *,
.with-readability #livemark-right > * {
  color: #444;
}

.with-readability #livemark-left,
.with-readability #livemark-right {
  scrollbar-width: unset;  /* Firefox */
  -ms-overflow-style: unset;  /* Internet Explorer 10+ */
}

/* NOTE: */
/* temporarily implemented in HtmlPlugin */
/* .with-readability #livemark-left::-webkit-scrollbar, */
/* .with-readability #livemark-right::-webkit-scrollbar { */
  /* display: unset !important; [> Chrome; Safari <] */
/* } */

</style>
<style>

.livemark-image {
  padding-bottom: 10px;
}

</style>
<style>

.livemark-infinity {
  display: none;
}

</style>
<style>

#livemark-mobile {
  position: absolute;
  visibility: hidden;
  z-index: 10000;
  /* NOTE: We can't use "right" because of Mobile Chrome and #34 */
  left: calc(100vw - 60px);
  top: 24px;
}

#livemark-mobile .stack {
  margin-top: 15px;
  display: block;
  cursor: pointer;
}

#livemark-mobile .bar {
  display: block;
  width: 25px;
  height: 3px;
  margin: 5px auto;
  -webkit-transition: all 0.3s ease-in-out;
  transition: all 0.3s ease-in-out;
  background-color: #aaa;
}

@media only screen and (max-width: 768px) {
  #livemark-mobile {
    visibility: visible;
  }

  #livemark-mobile.active {
    position: fixed;
  }

  #livemark-mobile.active .bar:nth-child(2) {
    opacity: 0;
  }

  #livemark-mobile.active .bar:nth-child(1) {
    transform: translateY(8px) rotate(45deg);
  }

  #livemark-mobile.active .bar:nth-child(3) {
    transform: translateY(-8px) rotate(-45deg);
  }

  #livemark-left {
      position: fixed;
      top: 0;
      left: -100vw;
      padding-top: 35px;
      background-color: #fff;
      width: 100vw;
      border-radius: 10px;
      text-align: center;
      transition: 0.3s;
      box-shadow: 0 10px 27px rgba(0, 0, 0, 0.05);
      visibility: visible;
      z-index: 1000;
  }

  #livemark-left.active {
    left: 0;
  }
}

</style>
<link rel="stylesheet" href="https://unpkg.com/paginationjs@2.1.5/dist/pagination.css">
<style>

.livemark-pagination {
  display: none;
}

</style>
<style>

.livemark-remark {
  display: block;
}

</style>
<style>

#livemark-search {
  position: fixed;
  left: 30px;
  width: 240px;
  bottom: 20px;
  z-index:100;
  visibility: hidden;
}

@media only screen and (min-width: 992px) {
  #livemark-search {
    visibility: visible;
  }
}

#livemark-search-input {
  width: 100%;
  outline: none;
  font-size: 20px;
  padding: 7px 10px;
  border-radius: 20px;
  border: solid 1px #ddd;
  box-shadow: 0px 7px 10px #eee;
  color: #888;
}

#livemark-search-input::placeholder {
  color: #888;
}

#livemark-search-input::-webkit-search-decoration,
#livemark-search-input::-webkit-search-cancel-button,
#livemark-search-input::-webkit-search-results-button,
#livemark-search-input::-webkit-search-results-decoration {
  -webkit-appearance:none;
}

#livemark-search-output {
  visibility: hidden;
  width: 100%;
  font-size: 16px;
  padding: 10px 10px;
  border-radius: 20px;
  border: solid 1px #ddd;
  box-shadow: 0px 7px 10px #eee;
  background: white;
  margin-bottom: 10px;
}

#livemark-search-output ul {
  margin: 0;
  padding: 0;
  list-style-type: none;
}

#livemark-search-output li.active {
  font-size: 20px;
  font-weight: bold;
}

#livemark-search-output a {
  color: #5CC820;
  text-decoration: underline;
}

.livemark-search-found {
  background-color: #5CC820;
  color: white;
  font-weight: bold;
  padding: 5px;
  border-radius: 5px;
}

</style>
<style>

#livemark-main h2 .livemark-source-button {
  float: right;
  display: none;
}

#livemark-main h2:hover .livemark-source-button {
  display: inline;
  margin-left: 8px;
  color: #aaa;
  font-weight: normal;
  text-decoration: none;
}

#livemark-main h2 .livemark-source-button:hover {
  text-decoration: underline;
}

.livemark-source-section {
  border: dashed 1px #ccc;
}

</style>
<style>

.nav-tabs {
    padding-left: 0 !important;
}

.nav-item {
    margin-top: 0 !important;
}

</style>
<style>

.livemark-task pre:first-child {
  margin-bottom: 0 !important;
  border-bottom: dashed 1px #ccc;
}

</style>
<style>

.livemark-video {
  padding-bottom: 10px;
}

</style>
</head>
<body>
<div id="livemark-left">

<div id="livemark-brand">
  <ul>
    <li>
      <a class="active" href="../..">
        Frictionless Framework
      </a>
    </li>
  </ul>
</div>

<div id="livemark-pages">
  <ul class="primary">
        <li class="primary  ">
      <a href="../../index.html" class="primary">
        Introduction
      </a>
          </li>
        <li class="primary  ">
      <a href="../getting-started.html" class="primary">
        Getting Started
      </a>
          </li>
        <li class="primary  ">
      <a href="../basic-examples.html" class="primary">
        Basic Examples
      </a>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Management
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../guides/describing-data.html" class="secondary">
            Describing Data
          </a>
        </li>
                <li class="secondary ">
          <a href="../guides/extracting-data.html" class="secondary">
            Extracting Data
          </a>
        </li>
                <li class="secondary ">
          <a href="../guides/validating-data.html" class="secondary">
            Validating Data
          </a>
        </li>
                <li class="secondary ">
          <a href="../guides/transforming-data.html" class="secondary">
            Transforming Data
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group active">
      <a href="" class="primary">
        Working in Console
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="overview.html" class="secondary">
            Overview
          </a>
        </li>
                <li class="secondary ">
          <a href="describe.html" class="secondary">
            Describe
          </a>
        </li>
                <li class="secondary ">
          <a href="convert.html" class="secondary">
            Convert
          </a>
        </li>
                <li class="secondary ">
          <a href="explore.html" class="secondary">
            Explore
          </a>
        </li>
                <li class="secondary ">
          <a href="extract.html" class="secondary">
            Extract
          </a>
        </li>
                <li class="secondary ">
          <a href="index.html" class="secondary">
            Index
          </a>
        </li>
                <li class="secondary ">
          <a href="list.html" class="secondary">
            List
          </a>
        </li>
                <li class="secondary active">
          <a href="publish.html" class="secondary">
            Publish
          </a>
        </li>
                <li class="secondary ">
          <a href="query.html" class="secondary">
            Query
          </a>
        </li>
                <li class="secondary ">
          <a href="script.html" class="secondary">
            Script
          </a>
        </li>
                <li class="secondary ">
          <a href="validate.html" class="secondary">
            Validate
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Working in Python
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../framework/actions.html" class="secondary">
            Actions
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/catalog.html" class="secondary">
            Catalog
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/package.html" class="secondary">
            Package
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/resource.html" class="secondary">
            Resource
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/dialect.html" class="secondary">
            Dialect
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/schema.html" class="secondary">
            Schema
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/checklist.html" class="secondary">
            Checklist
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/pipeline.html" class="secondary">
            Pipeline
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/detector.html" class="secondary">
            Detector
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/inquiry.html" class="secondary">
            Inquiry
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/report.html" class="secondary">
            Report
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/table.html" class="secondary">
            Table
          </a>
        </li>
                <li class="secondary ">
          <a href="../framework/error.html" class="secondary">
            Error
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Advanced Topics
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../advanced/design.html" class="secondary">
            Design
          </a>
        </li>
                <li class="secondary ">
          <a href="../advanced/system.html" class="secondary">
            System
          </a>
        </li>
                <li class="secondary ">
          <a href="../advanced/extending.html" class="secondary">
            Extension
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Resources
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../resources/file.html" class="secondary">
            File
          </a>
        </li>
                <li class="secondary ">
          <a href="../resources/text.html" class="secondary">
            Text
          </a>
        </li>
                <li class="secondary ">
          <a href="../resources/json.html" class="secondary">
            Json
          </a>
        </li>
                <li class="secondary ">
          <a href="../resources/table.html" class="secondary">
            Table
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Sources
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../schemes/aws.html" class="secondary">
            Aws
          </a>
        </li>
                <li class="secondary ">
          <a href="../schemes/buffer.html" class="secondary">
            Buffer
          </a>
        </li>
                <li class="secondary ">
          <a href="../schemes/local.html" class="secondary">
            Local
          </a>
        </li>
                <li class="secondary ">
          <a href="../schemes/multipart.html" class="secondary">
            Multipart
          </a>
        </li>
                <li class="secondary ">
          <a href="../schemes/remote.html" class="secondary">
            Remote
          </a>
        </li>
                <li class="secondary ">
          <a href="../schemes/stream.html" class="secondary">
            Stream
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Formats
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../formats/csv.html" class="secondary">
            Csv
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/erd.html" class="secondary">
            Erd
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/excel.html" class="secondary">
            Excel
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/gsheets.html" class="secondary">
            Gsheets
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/html.html" class="secondary">
            Html
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/inline.html" class="secondary">
            Inline
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/json.html" class="secondary">
            Json
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/jsonschema.html" class="secondary">
            JsonSchema
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/markdown.html" class="secondary">
            Markdown
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/ods.html" class="secondary">
            Ods
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/pandas.html" class="secondary">
            Pandas
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/parquet.html" class="secondary">
            Parquet
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/spss.html" class="secondary">
            Spss
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/sql.html" class="secondary">
            Sql
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/yaml.html" class="secondary">
            Yaml
          </a>
        </li>
                <li class="secondary ">
          <a href="../formats/zip.html" class="secondary">
            Zip
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Portals
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../portals/ckan.html" class="secondary">
            Ckan
          </a>
        </li>
                <li class="secondary ">
          <a href="../portals/github.html" class="secondary">
            Github
          </a>
        </li>
                <li class="secondary ">
          <a href="../portals/zenodo.html" class="secondary">
            Zenodo
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Checks
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../checks/baseline.html" class="secondary">
            Baseline
          </a>
        </li>
                <li class="secondary ">
          <a href="../checks/table.html" class="secondary">
            Table
          </a>
        </li>
                <li class="secondary ">
          <a href="../checks/row.html" class="secondary">
            Row
          </a>
        </li>
                <li class="secondary ">
          <a href="../checks/cell.html" class="secondary">
            Cell
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Steps
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../steps/resource.html" class="secondary">
            Resource
          </a>
        </li>
                <li class="secondary ">
          <a href="../steps/table.html" class="secondary">
            Table
          </a>
        </li>
                <li class="secondary ">
          <a href="../steps/field.html" class="secondary">
            Field
          </a>
        </li>
                <li class="secondary ">
          <a href="../steps/row.html" class="secondary">
            Row
          </a>
        </li>
                <li class="secondary ">
          <a href="../steps/cell.html" class="secondary">
            Cell
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Fields
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../fields/any.html" class="secondary">
            Any
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/array.html" class="secondary">
            Array
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/boolean.html" class="secondary">
            Boolean
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/date.html" class="secondary">
            Date
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/datetime.html" class="secondary">
            Datetime
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/duration.html" class="secondary">
            Duration
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/geojson.html" class="secondary">
            Geojson
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/geopoint.html" class="secondary">
            Geopoint
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/integer.html" class="secondary">
            Integer
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/number.html" class="secondary">
            Number
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/object.html" class="secondary">
            Object
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/string.html" class="secondary">
            String
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/time.html" class="secondary">
            Time
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/year.html" class="secondary">
            Year
          </a>
        </li>
                <li class="secondary ">
          <a href="../fields/yearmonth.html" class="secondary">
            Yearmonth
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Data Errors
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../errors/metadata.html" class="secondary">
            Metadata
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/resource.html" class="secondary">
            Resource
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/data.html" class="secondary">
            Data
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/file.html" class="secondary">
            File
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/table.html" class="secondary">
            Table
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/header.html" class="secondary">
            Header
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/label.html" class="secondary">
            Label
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/row.html" class="secondary">
            Row
          </a>
        </li>
                <li class="secondary ">
          <a href="../errors/cell.html" class="secondary">
            Cell
          </a>
        </li>
              </ul>
          </li>
        <li class="primary group ">
      <a href="" class="primary">
        Codebase
      </a>
            <ul class="secondary">
                <li class="secondary ">
          <a href="../codebase/authors.html" class="secondary">
            Authors
          </a>
        </li>
                <li class="secondary ">
          <a href="../codebase/license.html" class="secondary">
            License
          </a>
        </li>
                <li class="secondary ">
          <a href="../codebase/migration.html" class="secondary">
            Migration
          </a>
        </li>
                <li class="secondary ">
          <a href="../codebase/changelog.html" class="secondary">
            Changelog
          </a>
        </li>
                <li class="secondary ">
          <a href="../codebase/contributing.html" class="secondary">
            Contributing
          </a>
        </li>
              </ul>
          </li>
        <li class="primary  ">
      <a href="../universe.html" class="primary">
        Universe
      </a>
          </li>
        <li class="primary  ">
      <a href="../../blog/index.html" class="primary">
        Blog
      </a>
          </li>
      </ul>
</div>
</div>
<div id="livemark-main">
<div id="livemark-notes">
      <a href="https://github.com/frictionlessdata/frictionless-py/edit/main/docs/console/publish.md" target="_blank">Edit page </a> in <a href="https://livemark.frictionlessdata.io" target="_blank"> Livemark </a><br>
    (2025-07-21 07:43)
</div>

<h1>Publish</h1>
<div data-type="warning"><div class="livemark-remark">
  <div class="alert alert-warning" role="alert">
    Currently, only publishing to CKAN is supported; Github and Zenodo are in active development.
  </div>
</div></div><p>With <code>publish</code> command you can publish your dataset to a data publishing platform like CKAN:</p>
<div><pre><code class="language-bash">frictionless publish data/tables/*.csv --target http://ckan:5000/dataset/my-best --title "My best dataset"
</code></pre>
</div><p>It will ask for an API Key to upload your metadata and data. As a result:</p>
<div><div class="livemark-image">
  <img class="" width="unset" height="unset" src="../../assets/publish.png">
</div></div>

<div id="livemark-signs">
  <div>
        <div class="next">
      <a href="query.html">
        Query »
      </a>
    </div>
            <div class="prev">
      <a href="list.html">
        « List
      </a>
    </div>
      </div>
</div>
</div>
<div id="livemark-right">

<div id="livemark-rating">
  <iframe src="https://ghbtns.com/github-btn.html?user=frictionlessdata&amp;repo=frictionless-py&amp;type=star&amp;count=true&amp;size=large" width="160px" height="30px" title="GitHub">
  </iframe>
</div>

<div id="livemark-topics">
  <div class="toc">
  </div>
</div>
</div>
<script src="https://unpkg.com/lodash@4.17.21/lodash.min.js"></script>
<script src="https://unpkg.com/jquery@3.6.0/dist/jquery.min.js"></script>
<script src="https://unpkg.com/popper.js@1.16.1/dist/umd/popper.min.js"></script>
<script src="https://unpkg.com/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script>
<script src="https://unpkg.com/prismjs@1.23.0/components/prism-core.min.js"></script>
<script src="https://unpkg.com/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const content = document.querySelector("#livemark-main");
  const headings = content.querySelectorAll("h1, h2, h3, h4, h5, h6, h7");
  const headingMap = {};

  // Add identifiers
  Array.prototype.forEach.call(headings, function (heading) {
    const id = heading.id
      ? heading.id
      : heading.textContent
          .trim()
          .toLowerCase()
          .split(" ")
          .join("-")
          .replace(/[!@#$%^&*():]/gi, "")
          .replace(/\//gi, "-");
    headingMap[id] = !isNaN(headingMap[id]) ? ++headingMap[id] : 0;
    if (headingMap[id]) {
      heading.id = id + "-" + headingMap[id];
    } else {
      heading.id = id;
    }
  });

  // Add links
  Array.prototype.forEach.call(headings, function (heading) {
    const link = document.createElement("a");
    link.href = "#" + heading.id;
    link.innerText = "#";
    link.classList.add("heading");
    heading.appendChild(link);
  });
});

</script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const groups = $("#livemark-pages li.group");
  for (const group of groups) {
    $(group)
      .children("a")
      .click((ev) => {
        ev.preventDefault();
        $(group).toggleClass("active");
        // $(group).find(".fa").toggleClass("fa-chevron-right");
        // $(group).find(".fa").toggleClass("fa-chevron-down");
      });
  }
});

</script>
<script src="https://unpkg.com/tocbot@4.12.3/dist/tocbot.min.js"></script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  // Start tocbot
  tocbot.init({
    // Where to render the table of contents.
    tocSelector: ".toc",
    // Where to grab the headings to build the table of contents.
    contentSelector: "#livemark-main",
    // Which headings to grab inside of the contentSelector element.
    headingSelector: "h2",
    // For headings inside relative or absolute positioned containers within content.
    hasInnerContainers: true,
    // Called each time a heading is parsed. Expects a string in return.
    headingLabelCallback: (label) => {
      label = label.replace(/(^#|#$)/g, "").trim();
      // label = label.replace(/\(.*?\)$/g, "");
      return label;
    },
    // Disable generating ordered lists (ol)
    orderedList: false,
    // Fix active link class
    onClick: syncList,
    scrollEndCallback: syncList,
  });

  // Style list
  $("#livemark-topics .toc > ul").addClass("primary");
  $("#livemark-topics .toc > ul > li").addClass("primary");
  $("#livemark-topics .toc > ul > li > a").addClass("primary");
  $("#livemark-topics .toc ul.is-collapsible").addClass("secondary");
  $("#livemark-topics .toc ul.is-collapsible li").addClass("secondary");
  $("#livemark-topics .toc ul.is-collapsible li > a").addClass("secondary");
  for (const element of $("#livemark-topics .primary")) {
    if ($(element).find(".secondary").length) {
      $(element).addClass("group");
    }
  }

  // Sync list
  function syncList() {
    for (const element of $("#livemark-topics li.primary")) {
      if ($(element).find(".is-active-li").length) {
        $(element).addClass("is-active-li");
      }
    }
  }
  syncList();
});

</script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const handlePopstate = async () => {
    const href = location.hash;
    if (href.startsWith("#card=")) {
      const code = href.split("=")[1];
      const response = await fetch(`/assets/cards/${code}.html`);
      const html = await response.text();
      $("#livemark-cards .modal-title").html("");
      $("#livemark-cards .modal-body").html(html);
      $("#livemark-cards h1").appendTo("#livemark-cards .modal-title");
      $("#livemark-cards .modal").modal();
      $("#livemark-cards .modal").on("hidden.bs.modal", () => {
        history.pushState("", document.title, window.location.pathname);
      });
    }
  };
  window.addEventListener("popstate", handlePopstate);
  handlePopstate();
});

</script>

<div id="livemark-cards">
  <div class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog modal-lg" role="document">
      <div class="modal-content">
        <div class="modal-header">
          <h5 class="modal-title"></h5>
          <button type="button" class="close" data-dismiss="modal" aria-label="Close">
            <span aria-hidden="true">×</span>
          </button>
        </div>
        <div class="modal-body">
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
        </div>
      </div>
    </div>
  </div>
</div>
<script src="https://unpkg.com/ue-scroll-js@2.0.2/dist/ue-scroll.min.js"></script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  // Init
  const readability = localStorage.getItem("livemark-display-readability");
  if (readability === "plus") {
    document.body.classList.add("with-readability");
  } else {
    document.body.classList.remove("with-readability");
  }

  // Plus
  document
    .getElementById("livemark-display-plus")
    .addEventListener("click", function () {
      document.body.classList.add("with-readability");
      localStorage.setItem("livemark-display-readability", "plus");
    });

  // Minus
  document
    .getElementById("livemark-display-minus")
    .addEventListener("click", function () {
      document.body.classList.remove("with-readability");
      localStorage.setItem("livemark-display-readability", "minus");
    });

  // Print
  document
    .getElementById("livemark-display-print")
    .addEventListener("click", function () {
      window.print();
    });

  // Scroll
  const scrollSpeed = parseInt("10");
  UeScroll.init({ element: "#livemark-display-scroll .fa", scrollSpeed });
});

</script>

<div id="livemark-display">
  <div class="control" id="livemark-display-print" title="Print">
    <span class="fa fa-print"></span>
  </div>
  <div class="control" id="livemark-display-plus" title="Increase readability">
    <span class="fa fa-plus"></span>
  </div>
  <div class="control" id="livemark-display-minus" title="Decrease readability">
    <span class="fa fa-minus"></span>
  </div>
  <div class="control" id="livemark-display-scroll" title="Back to top">
    <span class="fa fa-chevron-up"></span>
  </div>
</div>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const container = $(".livemark-infinity");
  if (container.length) {
    const elements = container
      .children()
      .map((index, element) => element.outerHTML)
      .get();
    container.html(elements.splice(0, 100));
    container.show();
    window.addEventListener("scroll", () => {
      const element = container.get(0);
      const position = window.scrollY + window.innerHeight + 100;
      const threshold = element.offsetTop + element.scrollHeight;
      if (position > threshold) {
        container.append(elements.splice(0, 100));
      }
    });
  }
});

</script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const left = document.getElementById("livemark-left");
  const mobile = document.getElementById("livemark-mobile");
  mobile.addEventListener("click", () => {
    left.classList.toggle("active");
    mobile.classList.toggle("active");
  });
  // NOTE: We can replace the selector by 'a:not[href=""]' after #57
  left.querySelectorAll("li:not(.group) a").forEach((link) => {
    link.addEventListener("click", () => {
      if (left.classList.contains("active")) {
        left.classList.remove("active");
        mobile.classList.remove("active");
      }
    });
  });
});

</script>

<div id="livemark-mobile">
  <div class="stack" title="Toggle menu">
    <span class="bar"></span>
    <span class="bar"></span>
    <span class="bar"></span>
  </div>
</div>
<script src="https://unpkg.com/paginationjs@2.1.5/dist/pagination.min.js"></script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const container = $(".livemark-pagination");
  if (container.length) {
    const elements = container
      .children()
      .map((index, element) => element.outerHTML)
      .get();
    container.html(`
      <div class="livemark-pagination-data"></div>
      <div class="livemark-pagination-navs"></div>
    `);
    container.find(".livemark-pagination-navs").pagination({
      dataSource: elements,
      callback: (html) => {
        container.find(".livemark-pagination-data").html(html);
      },
    });
    container.show();
  }
});

</script>
<script src="https://unpkg.com/lunr@2.3.9/lunr.min.js"></script>
<script src="https://unpkg.com/jquery-highlight@3.5.0/jquery.highlight.js"></script>
<script src="https://unpkg.com/jquery.scrollto@2.1.3/jquery.scrollTo.js"></script>
<script>

document.addEventListener("DOMContentLoaded", function () {
  const prepare = () => {
    const searchParams = new URLSearchParams(window.location.search);
    const query = searchParams.get('query') || ''
    if (query.length >= 3) {
      searchInput.value = query
    }
  }
  const search = () => {
    unhighlight()
    query = searchInput.value
    searchOutput.innerHTML = ''
    searchOutput.style.visibility = 'hidden'
    const searchParams = new URLSearchParams(window.location.search);
    if (query.length < 3) return
    const results = searchIndex.search(query)
    if (!results.length) return
    searchParams.set('query', query)
    const newRelativePathQuery = window.location.pathname + '?' + searchParams.toString();
    history.pushState(null, '', newRelativePathQuery);
    const elements = []
    for (const result of results) {
      const item = searchItems[result.ref]
      const link = `${item.relpath}.html`
      const cls = window.location.pathname === link ? 'class="active"' : ''
      elements.push(`<li ${cls}><a href="${link}?query=${query}">${item.name}</a></li>`)
    }
    searchOutput.innerHTML = `<ul>\n${elements.join('\n')}\n</ul>`
    searchOutput.style.visibility = 'visible'
    highlight()
  }
  const highlight = () => {
    const stem = lunr.stemmer(new lunr.Token(query)).str
    $('#livemark-main').highlight(stem, {className: 'livemark-search-found'});
    setTimeout(() => {
      $(window).scrollTo($('.livemark-search-found').first(), 1000)
    }, 1000)
  }
  const unhighlight = () => {
    $('#livemark-main').unhighlight({className: 'livemark-search-found'});
  }
  const searchItems = {
          '404': {
          'name': 'Not Found',
          'path': '404',
          'relpath': '../../404',
          'text': "# Not Found ```markdown remark type=danger This page is not found ``` Return to the home page.",
      },
          'index': {
          'name': 'frictionless-py',
          'path': 'index',
          'relpath': '../../index',
          'text': "# frictionless-py [![Build](https://img.shields.io/github/actions/workflow/status/frictionlessdata/frictionless-py/general.yaml?branch=main)](https://github.com/frictionlessdata/frictionless-py/actions) [![Coverage](https://img.shields.io/codecov/c/github/frictionlessdata/frictionless-py/main)](https://codecov.io/gh/frictionlessdata/frictionless-py) [![Release](https://img.shields.io/pypi/v/frictionless.svg)](https://pypi.python.org/pypi/frictionless) [![Citation](https://zenodo.org/badge/28409905.svg)](https://zenodo.org/badge/latestdoi/28409905) [![Codebase](https://img.shields.io/badge/codebase-github-brightgreen)](https://github.com/frictionlessdata/frictionless-py) [![Support](https://img.shields.io/badge/support-slack-brightgreen)](https://join.slack.com/t/frictionlessdata/shared_invite/zt-17kpbffnm-tRfDW_wJgOw8tJVLvZTrBg) ```markdown remark type=primary Migrating from an older version? Please read **[v5](blog/2022/08-22-frictionless-framework-v5.html)** announcement and migration guide. ``` Data management framework for Python that provides functionality to describe, extract, validate, and transform tabular data (DEVT Framework). It supports a great deal of data sources and formats, as well as provides popular platforms integrations. The framework is powered by the lightweight yet comprehensive [Frictionless Standards](https://specs.frictionlessdata.io/). ## Purpose - **Describe your data**: You can infer, edit and save metadata of your data tables. It\u0027s a first step for ensuring data quality and usability. Frictionless metadata includes general information about your data like textual description, as well as, field types and other tabular data details. - **Extract your data**: You can read your data using a unified tabular interface. Data quality and consistency are guaranteed by a schema. Frictionless supports various file schemes like HTTP, FTP, and S3 and data formats like CSV, XLS, JSON, SQL, and others. - **Validate your data**: You can validate data tables, resources, and datasets. Frictionless generates a unified validation report, as well as supports a lot of options to customize the validation process. - **Transform your data**: You can clean, reshape, and transfer your data tables and datasets. Frictionless provides a pipeline capability and a lower-level interface to work with the data. ## Features - Open Source (MIT) - Powerful Python framework - Convenient command-line interface - Low memory consumption for data of any size - Reasonable performance on big data - Support for compressed files - Custom checks and formats - Fully pluggable architecture - More than 1000+ tests ## Installation ```bash $ pip install frictionless ``` ## Example ```bash $ frictionless validate data/invalid.csv [invalid] data/invalid.csv row field code message ----- ------- ---------------- -------------------------------------------- 3 blank-header Header in field at position \"3\" is blank 4 duplicate-header Header \"name\" in field \"4\" is duplicated 2 3 missing-cell Row \"2\" has a missing cell in field \"field3\" 2 4 missing-cell Row \"2\" has a missing cell in field \"name2\" 3 3 missing-cell Row \"3\" has a missing cell in field \"field3\" 3 4 missing-cell Row \"3\" has a missing cell in field \"name2\" 4 blank-row Row \"4\" is completely blank 5 5 extra-cell Row \"5\" has an extra value in field \"5\" ``` ## Documentation Please visit our documentation portal: - https://framework.frictionlessdata.io",
      },
          'docs/getting-started': {
          'name': 'Getting Started',
          'path': 'docs/getting-started',
          'relpath': '../getting-started',
          'text': "# Getting Started Let\u0027s get started with Frictionless! We will learn how to install and use the framework. The simple example below will showcase the framework\u0027s basic functionality. ## Installation \u003e The framework requires Python3.8+. Versioning follows the [SemVer Standard](https://semver.org/). ```bash tabs=CLI pip install frictionless pip install frictionless[sql] # to install a core plugin (optional) pip install \u0027frictionless[sql]\u0027 # for zsh shell ``` The framework supports CSV, Excel, and JSON formats by default. The second command above installs a plugin for SQL support. There are plugins for SQL, Pandas, HTML, and others (all supported plugins are listed in the \"File Formats\" and schemes in \"File Schemes\" menu). Usually, you don\u0027t need to think about it in advance\u2013frictionless will display a useful error message about a missing plugin with installation instructions. ### Troubleshooting Did you have an error installing Frictionless? Here are some dependencies and common errors: - `pip: command not found`. Please see the [pip docs](https://pip.pypa.io/en/stable/installing/) for help installing pip. - [Installing Python help (Mac)](https://docs.python.org/3/using/mac.html) - [Installing Python help (Windows)](https://docs.python.org/3/using/windows.html) Still having a problem? Ask us for help on our [Discord](https://discord.com/invite/j9DNFNw) chat or open an [issue](https://github.com/frictionlessdata/frictionless-py/issues). We\u0027re happy to help! ## Usage The framework can be used: - as a Python library - as a command-line interface For instance, both examples below do the same thing: ```bash tabs=CLI frictionless extract data/table.csv ``` ```python tabs=Python from frictionless import extract rows = extract(\u0027data/table.csv\u0027) ``` The interfaces are as much alike as possible regarding naming conventions and the way you interact with them. Usually, it\u0027s straightforward to translate, for instance, Python code to a command-line call. Frictionless provides code completion for Python and the command-line, which should help to get useful hints in real time. Arguments conform to the following naming convention: - for Python interfaces, they are snake_cased, e.g. `missing_values` - within dictionaries or JSON objects, they are camelCased, e.g. `missingValues` - in the command line, they use dashes, e.g. `--missing-values` To get the documentation for a command-line interface just use the `--help` flag: ```bash tabs=CLI frictionless --help frictionless describe --help frictionless extract --help frictionless validate --help frictionless transform --help ``` ## Example \u003e Download [`invalid.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/invalid.csv) to reproduce the examples (right-click and \"Save link as\"). For more examples, please take a look at the [Basic Examples](basic-examples.md) article. We will take a very messy data file: ```bash script tabs=CLI cat invalid.csv ``` ```python script tabs=Python with open(\u0027invalid.csv\u0027) as file: print(file.read()) ``` First of all, let\u0027s use `describe` to infer the metadata directly from the tabular data. We can then edit and save it to provide others with useful information about the data: \u003e The CLI output is in [YAML](https://yaml.org/), it is a default Frictionless output format. ```bash script tabs=CLI output=yaml frictionless describe invalid.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import describe resource = describe(\u0027invalid.csv\u0027) pprint(resource) ``` Now that we have inferred a table schema from the data file (e.g., expected format of the table, expected type of each value in a column, etc.), we can use `extract` to read the normalized tabular data from the source CSV file: ```bash script tabs=CLI frictionless extract invalid.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import extract rows = extract(\u0027invalid.csv\u0027) pprint(rows) ``` Last but not least, let\u0027s get a validation report. This report will help us to identify and fix all the errors present in the tabular data, as comprehensive information is provided for every problem: ```bash script tabs=CLI frictionless validate invalid.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import validate report = validate(\u0027invalid.csv\u0027) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` Now that we have all this information: - we can clean up the table to ensure the data quality - we can use the metadata to describe and share the dataset - we can include the validation into our workflow to guarantee the validity - and much more: don\u0027t hesitate and read the following sections of the documentation!",
      },
          'docs/basic-examples': {
          'name': 'Basic Examples',
          'path': 'docs/basic-examples',
          'relpath': '../basic-examples',
          'text': "# Basic Examples Let\u0027s start with an example dataset. We will look at a few raw data files that have recently been collected by an anthropologist. The anthropologist wants to publish this data in an open repository so her colleagues can also use this data. Before publishing the data, she wants to add metadata and check the data for errors. We are here to help, so let\u2019s start by exploring the data. We see that the quality of data is far from perfect. In fact, the first row contains comments from the anthropologist! To be able to use this data, we need to clean it up a bit. \u003e Download [`countries.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/countries.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat countries.csv ``` ```python script tabs=Python with open(\u0027countries.csv\u0027) as file: print(file.read()) ``` As we can see, this is data containing information about European countries and their populations. Also, it looks like there are two fields having a relationship based on a country\u0027s identifier: neighbor_id is a Foreign Key to id. ## Describing Data First of all, we\u0027re going to describe our dataset. Frictionless uses the powerful [Frictionless Data Specifications](https://specs.frictionlessdata.io/). They are very handy to describe: - a data table - using [Table Schema](https://specs.frictionlessdata.io/table-schema/) - a data resource - using [Data Resource](https://specs.frictionlessdata.io/data-resource/) - a data package - using [Data Package](https://specs.frictionlessdata.io/data-package/) - and other objects Let\u0027s describe the `countries` table: ```bash script tabs=CLI output=yaml frictionless describe countries.csv # optionally add --stats to get statistics ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import describe resource = describe(\u0027countries.csv\u0027) pprint(resource) ``` As we can see, Frictionless was smart enough to understand that the first row contains a comment. It\u0027s good, but we still have a few problems: - we use `n/a` as a missing values marker - `neighbor_id` must be numerical: let\u0027s edit the schema - `population` must be numerical: setting proper missing values will solve it - there is a relation between the `id` and `neighbor_id` fields Let\u0027s update our metadata and save it to the disc: \u003e Open this file in your favorite editor and update as it\u0027s shown below ```bash tabs=CLI frictionless describe countries.csv --yaml \u003e countries.resource.yaml editor countries.resource.yaml ``` ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_missing_values=[\"\", \"n/a\"]) resource = describe(\"countries.csv\", detector=detector) resource.schema.set_field_type(\"neighbor_id\", \"integer\") resource.schema.foreign_keys.append( {\"fields\": [\"neighbor_id\"], \"reference\": {\"resource\": \"\", \"fields\": [\"id\"]}} ) resource.to_yaml(\"countries.resource.yaml\") ``` Let\u0027s see what we have created: ```bash script tabs=CLI output=yaml cat countries.resource.yaml ``` ```python script tabs=Python output=yaml with open(\u0027countries.resource.yaml\u0027) as file: print(file.read()) ``` It has the same metadata as we saw above but also includes our editing related to missing values and data types. We didn\u0027t change all the wrong data types manually because providing proper missing values had fixed it automatically. Now we have a resource descriptor. In the next section, we will show why metadata matters and how to use it. ## Extracting Data It\u0027s time to try extracting our data as a table. As a first naive attempt, we will ignore the metadata we saved on the previous step: ```bash script tabs=CLI frictionless extract countries.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import extract rows = extract(\u0027countries.csv\u0027) pprint(rows) ``` Actually, it doesn\u0027t look terrible, but in reality, data like this is not quite useful: - it\u0027s not possible to export this data e.g., to SQL because integers are mixed with strings - there is still a basically empty row we don\u0027t want to have - there are some mistakes in the neighbor_id column The output of the extract is in \u0027utf-8\u0027 encoding scheme. Let\u0027s use the metadata we save to try extracting data with the help of Frictionless Data specifications: ```bash script tabs=CLI frictionless extract countries.resource.yaml ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import extract rows = extract(\u0027countries.resource.yaml\u0027) pprint(rows) ``` It\u0027s now much better! Numerical fields are numerical fields, and there are no more textual missing values markers. We can\u0027t see in the command-line, but missing values are now `None` values in Python, and the data can be e.g., exported to SQL. Although, it\u0027s still not ready for being published. In the next section, we will validate it! ## Validating Data Data validation with Frictionless is as easy as describing or extracting data: ```bash script tabs=CLI frictionless validate countries.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import validate report = validate(\u0027countries.csv\u0027) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` Ahh, we had seen that coming. The data is not valid; there are some missing and extra cells. But wait a minute, in the first step, we created the metadata file with more information about our table. We have to use it. ```bash script tabs=CLI frictionless validate countries.resource.yaml ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import validate report = validate(\u0027countries.resource.yaml\u0027) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` Now it\u0027s even worse, but regarding data validation errors, the more, the better, actually. Thanks to the metadata, we were able to reveal some critical errors: - the bad data types, i.e. `Ireland` instead of an id - the bad relation between `id` and `neighbor_id`: we don\u0027t have a country with id 22 In the next section, we will clean up the data. ## Transforming Data We will use metadata to fix all the data type problems automatically. The only two things we need to handle manually: - France\u0027s population - Germany\u0027s neighborhood ```bash script tabs=CLI cat \u003e countries.pipeline.yaml \u003c\u003cEOF steps: - type: cell-replace fieldName: neighbor_id pattern: \u002722\u0027 replace: \u00272\u0027 - type: cell-replace fieldName: population pattern: \u0027n/a\u0027 replace: \u002767\u0027 - type: row-filter formula: population - type: field-update name: neighbor_id descriptor: type: integer - type: field-update name: population descriptor: type: integer - type: table-normalize - type: table-write path: countries-cleaned.csv EOF frictionless transform countries.csv --pipeline countries.pipeline.yaml ``` ```python script tabs=Python output=Python from pprint import pprint from frictionless import Resource, Pipeline, describe, transform, steps pipeline = Pipeline(steps=[ steps.cell_replace(field_name=\u0027neighbor_id\u0027, pattern=\u002722\u0027, replace=\u00272\u0027), steps.cell_replace(field_name=\u0027population\u0027, pattern=\u0027n/a\u0027, replace=\u002767\u0027), steps.row_filter(formula=\u0027population\u0027), steps.field_update(name=\u0027neighbor_id\u0027, descriptor={\"type\": \"integer\"}), steps.table_normalize(), steps.table_write(path=\"countries-cleaned.csv\"), ]) source = Resource(\u0027countries.csv\u0027) target = source.transform(pipeline) pprint(target.read_rows()) ``` Finally, we\u0027ve got the cleaned version of our data, which can be exported to a database or published. We have used a CSV as an output format but could have used Excel, JSON, SQL, and others. ```bash script tabs=CLI cat countries-cleaned.csv ``` ```python script tabs=Python with open(\u0027countries-cleaned.csv\u0027) as file: print(file.read()) ``` Basically, that\u0027s it; now, we have a valid data file and a corresponding metadata file. It can be shared with other people or stored without fear of type errors or other problems making research data not reproducible. ```bash script tabs=CLI ls countries-cleaned.* ``` ```python script tabs=Python import os files = [f for f in os.listdir(\u0027.\u0027) if os.path.isfile(f) and f.startswith(\u0027countries-cleaned.\u0027)] print(files) ``` In the next articles, we will explore more advanced Frictionless functionality.",
      },
          'docs/guides/describing-data': {
          'name': 'Describing Data',
          'path': 'docs/guides/describing-data',
          'relpath': '../guides/describing-data',
          'text': "# Describing Data \u003e This guide assumes basic familiarity with the Frictionless Framework. To learn more, please read the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction) and [Quick Start](https://framework.frictionlessdata.io/docs/guides/quick-start). Also, this guide is meant to be read in order from top to bottom, and reuses examples throughout the text. You can use the menu to skip sections, but please note that you might need to run code from earlier sections to make all the examples work. In Frictionless terms, \"Describing data\" means creating metadata for your data files. Having metadata is important because data files by themselves usually do not provide enough information to fully understand the data. For example, if you have a data table in a CSV format without metadata, you are missing a few critical pieces of information: - the meaning of the fields e.g., what the `size` field means (does that field mean geographic size? Or does it refer to the size of the file?) - data type information e.g., is this field a string or an integer? - data constraints e.g., the minimum temperature for your measurements - data relations e.g., identifier connections - and others For a dataset, there is even more information that can be provided, like the general purpose of a dataset, information about data sources, list of authors, and more. Also, when there are many tabular files, relational rules can be very important. Usually, there are foreign keys ensuring the integrity of the dataset; for example, think of a reference table containing country names and other data tables using it as a reference. Data in this form is called \"normalized data\" and it occurs very often in scientific and other kinds of research. Now that we have a general understanding of what \"describing data\" is, we can discuss why it is important: - **data validation**: metadata helps to reveal problems in your data during early stages of your workflow - **data publication**: metadata provides additional information that your data doesn\u0027t include These are not the only positives of having metadata, but they are two of the most important. Please continue reading to learn how Frictionless helps to achieve these advantages by describing your data. This guide will discuss the main `describe` functions (`describe`, `Schema.describe`, `Resource.describe`, `Package.describe`) and will then go into more detail about how to create and edit metadata in Frictionless. For the following examples, you will need to have Frictionless installed. See our [Quick Start Guide](https://framework.frictionlessdata.io/docs/guides/quick-start) if you need help. ```bash tabs=CLI pip install frictionless ``` ## Describe Functions The `describe` functions are the main Frictionless tool for describing data. In many cases, this high-level interface is enough for data exploration and other needs. The frictionless framework provides 4 different `describe` functions in Python: - `describe`: detects the source type and returns Data Resource or Data Package metadata - `Schema.describe`: always returns Table Schema metadata - `Resource.describe`: always returns Data Resource metadata - `Package.describe`: always returns Data Package metadata As described in more detail in the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction), a resource is a single file, such as a data file, and a package is a set of files, such as a data file and a schema. In the command-line, there is only 1 command (`describe`) but there is also a flag to adjust the behavior: ```bash tabs=CLI frictionless describe your-table.csv frictionless describe your-table.csv --type schema frictionless describe your-table.csv --type resource frictionless describe your-table.csv --type package ``` Please take into account that file names might be used by Frictionless to detect a metadata type for data extraction or validation. It\u0027s recommended to use corresponding suffixes when you save your metadata to the disk. For example, you might name your Table Schema as `table.schema.yaml`, Data Resource as `table.resource.yaml`, and Data Package as `table.package.yaml`. If there is no hint in the file name Frictionless will assume that it\u0027s a resource descriptor by default. For example, if we want a Data Package descriptor for a single file: \u003e Download [`table.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/table.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI output=yaml frictionless describe table.csv --type package ``` ```python script tabs=Python output=yaml from frictionless import describe package = describe(\"table.csv\", type=\"package\") print(package.to_yaml()) ``` ## Describing a Schema Table Schema is a specification for providing a \"schema\" (similar to a database schema) for tabular data. This information includes the expected data type for each value in a column (\"string\", \"number\", \"date\", etc.), constraints on the value (\"this string can only be at most 10 characters long\"), and the expected format of the data (\"this field should only contain strings that look like email addresses\"). Table Schema can also specify relations between data tables. We\u0027re going to use this file for the examples in this section. For this guide, we only use CSV files because of their demonstrativeness, but in general Frictionless can handle data in Excel, JSON, SQL, and many other formats: \u003e Download [`country-1.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/country-1.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat country-1.csv ``` ```python script tabs=Python with open(\u0027country-1.csv\u0027) as file: print(file.read()) ``` Let\u0027s get a Table Schema using the Frictionless framework (note: this example uses YAML for the schema format, but Frictionless also supports JSON format): ```python script tabs=Python from frictionless import Schema schema = Schema.describe(\"country-1.csv\") schema.to_yaml(\"country.schema.yaml\") # use schema.to_json for JSON ``` The high-level functions of Frictionless operate on the dataset and resource levels so we have to use a little bit of Python programming to get the schema information. Below we will show how to use a command-line interface for similar tasks. ```bash script tabs=CLI output=yaml cat country.schema.yaml ``` ```python script tabs=Python output=yaml with open(\u0027country.schema.yaml\u0027) as file: print(file.read()) ``` As we can see, we were able to infer basic metadata from our data file. But describing data doesn\u0027t end here - we can provide additional information that we discussed earlier: \u003e You can edit \"country.schema.yaml\" manually instead of running Python ```python script tabs=Python from frictionless import Schema schema = Schema.describe(\"country-1.csv\") schema.get_field(\"id\").title = \"Identifier\" schema.get_field(\"neighbor_id\").title = \"Identifier of the neighbor\" schema.get_field(\"name\").title = \"Name of the country\" schema.get_field(\"population\").title = \"Population\" schema.get_field(\"population\").description = \"According to the year 2020\u0027s data\" schema.get_field(\"population\").constraints[\"minimum\"] = 0 schema.foreign_keys.append( {\"fields\": [\"neighbor_id\"], \"reference\": {\"resource\": \"\", \"fields\": [\"id\"]}} ) schema.to_yaml(\"country.schema-full.yaml\") ``` Let\u0027s break it down: - we added a title for all the fields - we added a description to the \"Population\" field; the year information can be critical to interpret the data - we set a constraint to the \"Population\" field because it can\u0027t be less than 0 - we added a foreign key saying that \"Identifier of the neighbor\" should be present in the \"Identifier\" field ```bash script tabs=CLI output=yaml cat country.schema-full.yaml ``` ```python script tabs=Python output=yaml with open(\u0027country.schema-full.yaml\u0027) as file: print(file.read()) ``` Later we\u0027re going to show how to use the schema we created to ensure the validity of your data; in the next few sections, we will focus on Data Resource and Data Package metadata. To continue learning about table schemas please read: - [Table Schema Spec](https://specs.frictionlessdata.io/table-schema/) - [API Reference: Schema](../../docs/framework/schema.html#reference-schema) ## Describing a Resource The Data Resource format describes a data resource such as an individual file or data table. The essence of a Data Resource is a path to the data file it describes. A range of other properties can be declared to provide a richer set of metadata including Table Schema for tabular data. For this section, we will use a file that is slightly more complex to handle. In this example, cells are separated by the \";\" character and there is a comment on the top: \u003e Download [`country-2.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/country-2.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat country-2.csv ``` ```python script tabs=Python with open(\u0027country-2.csv\u0027) as file: print(file.read()) ``` Let\u0027s describe it: ```bash script tabs=CLI output=yaml frictionless describe country-2.csv ``` ```python script tabs=Python output=yaml from frictionless import describe resource = describe(\u0027country-2.csv\u0027) print(resource.to_yaml()) ``` OK, that looks wrong -- for example, the schema has only inferred one field, and that field does not seem correct either. As we have seen in the \"Introductory Guide\" Frictionless is capable of inferring some complicated cases\u0027 metadata but our data table is too complex for it to automatically infer. We need to manually program it: \u003e You can edit \"country.resource.yaml\" manually instead of running Python ```python script tabs=Python from frictionless import Schema, describe resource = describe(\"country-2.csv\") resource.dialect.header_rows = [2] resource.dialect.get_control(\u0027csv\u0027).delimiter = \";\" resource.schema = \"country.schema.yaml\" resource.to_yaml(\"country.resource-cleaned.yaml\") ``` So what we did here: - we set the header rows to be row number 2; as humans, we can easily see that was the proper row - we set the CSV Delimiter to be \";\" - we reuse the schema we created [earlier](#describing-a-schema) as the data has the same structure and meaning ```bash script tabs=CLI output=yaml cat country.resource-cleaned.yaml ``` ```python script tabs=Python output=yaml with open(\u0027country.resource-cleaned.yaml\u0027) as file: print(file.read()) ``` Our resource metadata includes the schema metadata we created earlier, but it also has: - general information about the file\u0027s schema, format, and compression - information about CSV Dialect which helps software understand how to read it - checksum information like hash, bytes, and rows But the most important difference is that the resource metadata contains the `path` property. This is a conceptual distinction of the Data Resource specification compared to the Table Schema specification. While a Table Schema descriptor can describe a class of data files, a Data Resource descriptor describes only one exact data file, `data/country-2.csv` in our case. Using programming terminology we could say that: - Table Schema descriptor is abstract (for a class of files) - Data Resource descriptor is concrete (for an individual file) We will show the practical difference in the \"Using Metadata\" section, but in the next section, we will overview the Data Package specification. To continue learning about data resources please read: - [Data Resource Spec](https://specs.frictionlessdata.io/data-resource/) - [API Reference: Resource](../../docs/framework/resource.html#reference-resource) ## Describing a Package A Data Package consists of: - Metadata that describes the structure and contents of the package - Resources such as data files that form the contents of the package The Data Package metadata is stored in a \"descriptor\". This descriptor is what makes a collection of data a Data Package. The structure of this descriptor is the main content of the specification below. In addition to this descriptor, a data package will include other resources such as data files. The Data Package specification does NOT impose any requirements on their form or structure and can, therefore, be used for packaging any kind of data. The data included in the package may be provided as: - Files bundled locally with the package descriptor - Remote resources, referenced by URL (see the [schemes tutorial](https://framework.frictionlessdata.io/docs/tutorials/schemes/local-tutorial) for more information about supported URLs) - \"Inline\" data (see below) which is included directly in the descriptor For this section, we will use the following files: \u003e Download [`country-3.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/country-3.csv) to reproduce the examples (right-click and \"Save link as\") ```bash script tabs=CLI cat country-3.csv ``` ```python script tabs=Python with open(\u0027country-3.csv\u0027) as file: print(file.read()) ``` \u003e Download [`capital-3.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/capital-3.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat capital-3.csv ``` ```python script tabs=Python with open(\u0027capital-3.csv\u0027) as file: print(file.read()) ``` First of all, let\u0027s describe our package now. We did it before for a resource but now we\u0027re going to use a glob pattern to indicate that there are multiple files: ```bash script tabs=CLI output=yaml frictionless describe *-3.csv ``` ```python script tabs=Python output=yaml from frictionless import describe package = describe(\"*-3.csv\") print(package.to_yaml()) ``` We have already learned about many concepts that are reflected in this metadata. We can see resources, schemas, fields, and other familiar entities. The difference is that this descriptor has information about multiple files which is a popular way of sharing data - in datasets. Very often you have not only one data file but also additional data files, some textual documents e.g. PDF, and others. To package all of these files with the corresponding metadata we use data packages. Following the pattern that is already familiar to the guide reader, we add some additional metadata: \u003e You can edit \"country.package.yaml\" manually instead of running Python ```python script tabs=Python output=yaml from frictionless import describe package = describe(\"*-3.csv\") package.title = \"Countries and their capitals\" package.description = \"The data was collected as a research project\" package.get_resource(\"country-3\").name = \"country\" package.get_resource(\"capital-3\").name = \"capital\" package.get_resource(\"country\").schema.foreign_keys.append( {\"fields\": [\"capital_id\"], \"reference\": {\"resource\": \"capital\", \"fields\": [\"id\"]}} ) package.to_yaml(\"country.package.yaml\") ``` In this case, we add a relation between different files connecting `id` and `capital_id`. Also, we provide dataset-level metadata to explain the purpose of this dataset. We haven\u0027t added individual fields\u0027 titles and descriptions, but that can be done as it was shown in the \"Table Schema\" section. ```bash script tabs=CLI output=yaml cat country.package.yaml ``` ```python script tabs=Python output=yaml with open(\u0027country.package.yaml\u0027) as file: print(file.read()) ``` The main role of the Data Package descriptor is describing a dataset; as we can see, it includes previously shown descriptors like `schema`, `dialect`, and `resource`. But it would be a mistake to think that Data Package is the least important specification; actually, it completes the Frictionless Data suite making it possible to share and validate not only individual files but also complete datasets. To continue learning about data resources please read: - [Data Package Spec](https://specs.frictionlessdata.io/data-package/) - [API Reference: Package](../../docs/framework/package.html#reference-package) ## Metadata Importance This documentation contains a great deal of information on how to use metadata and why it\u0027s vital for your data. In this section, we\u0027re going to provide a quick example based on the \"Data Resource\" section but please read other documents to get the full picture. Let\u0027s get back to this complex data table: ```bash script tabs=CLI cat country-2.csv ``` ```python script tabs=Python with open(\u0027country-2.csv\u0027) as file: print(file.read()) ``` As we tried before, by default Frictionless can\u0027t properly describe this file so we got something like: ```bash script tabs=CLI output=yaml frictionless describe country-2.csv ``` ```python script tabs=Python output=yaml from frictionless import describe resource = describe(\"country-2.csv\") print(resource.to_yaml()) ``` Trying to extract the data will fail this way: ```bash script tabs=CLI frictionless extract country-2.csv ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import extract rows = extract(\"country-2.csv\") pprint(rows) ``` This example highlights a really important idea - without metadata many software will not be able to even read this data file. Furthermore, without metadata people cannot understand the purpose of this data. To see how we can use metadata to fix our data, let\u0027s now use the `country.resource-full.yaml` file we created in the \"Data Resource\" section with Frictionless `extract`: ```bash script tabs=CLI frictionless extract country.resource-cleaned.yaml ``` ```python script tabs=Python from pprint import pprint from frictionless import extract rows = extract(\"country.resource-cleaned.yaml\") pprint(rows) ``` As we can see, the data is now fixed. The metadata we had saved the day! If we explore this data in Python we can discover that it also corrected data types - e.g. `id` is Python\u0027s integer not string. We can now export and share this data without any worries. ## Inferring Metadata \u003e Many Frictionless Framework\u0027s classes are metadata classes as though Schema, Resource, or Package. All the sections below are applicable for all these classes. You can read about the base Metadata class in more detail in [API Reference](../references/api-reference.md#metadata). Many Frictionless functions infer metadata under the hood such as `describe`, `extract`, and many more. On a lower-level, it\u0027s possible to control this process. To see this, let\u0027s create a `Resource`. ```python script tabsl=Python output=python from frictionless import Resource resource = Resource(\"country-1.csv\") print(resource) ``` ``` {\u0027path\u0027: \u0027country-1.csv\u0027} ``` Frictionless always tries to be as explicit as possible. We didn\u0027t provide any metadata except for `path` so we got the expected result. But now, we\u0027d like to `infer` additional metadata: \u003e We can ask for stats using CLI with `frictionless describe data/table.csv --stats`. Note that we use the `stats` argument for the `resource.infer` function. ```bash script tabs=CLI output=json frictionless describe country-1.csv --stats --json ``` ```python script tabs=Python output=python from pprint import pprint from frictionless import Resource resource = Resource(\"country-1.csv\") resource.infer(stats=True) pprint(resource) ``` The result is really familiar to us already. We have seen it a lot as an output of the `describe` function or command. Basically, that\u0027s what this high-level function does under the hood: create a resource and then infer additional metadata. All the main `Metadata` classes have this method with different available options but with the same conceptual purpose: - `package.infer` - `resource.infer` For more advanced detection options, please read the [Detector Guide](framework/detector-guide.md) ## Validating Metadata Metadata validity is an important topic, and we recommend validating your metadata before publishing. For example, let\u0027s first make it invalid: ```python script tabs=Python import yaml from frictionless import Resource descriptor = {} descriptor[\u0027path\u0027] = \u0027country-1.csv\u0027 descriptor[\u0027title\u0027] = 1 try: Resource(descriptor) except Exception as exception: print(exception.error) print(exception.reasons) ``` ``` False [{\u0027code\u0027: \u0027resource-error\u0027, \u0027name\u0027: \u0027Resource Error\u0027, \u0027tags\u0027: [], \u0027note\u0027: \u0027\"1 is not of type \\\u0027string\\\u0027\" at \"title\" in metadata and at \"properties/title/type\" in profile\u0027, \u0027message\u0027: \u0027The data resource has an error: \"1 is not of type \\\u0027string\\\u0027\" at \"title\" in metadata and at \"properties/title/type\" in profile\u0027, \u0027description\u0027: \u0027A validation cannot be processed.\u0027}] ``` We see this error`\u0027\"1 is not of type \\\u0027string\\\u0027\" at \"title\" in metadata and at \"properties/title/type\" in profile\u0027` as we set `title` to be an integer. Frictionless\u0027 high-level functions like `validate` runs all metadata checks by default. ## Transforming Metadata We have seen this before but let\u0027s re-iterate; it\u0027s possible to transform core metadata properties using Python\u0027s interface: ```python script tabs=Python from frictionless import Resource resource = Resource(\"country.resource-cleaned.yaml\") resource.title = \"Countries\" resource.description = \"It\u0027s a research project\" resource.dialect.header_rows = [2] resource.dialect.get_control(\u0027csv\u0027).delimiter = \";\" resource.to_yaml(\"country.resource-updated.yaml\") ``` We can add custom options using the `custom` property: ```python script tabs=Python from frictionless import Resource resource = Resource(\"country.resource-updated.yaml\") resource.custom[\"customKey1\"] = \"Value1\" resource.custom[\"customKey2\"] = \"Value2\" resource.to_yaml(\"country.resource-updated2.yaml\") ``` Let\u0027s check it out: ```bash script tabs=CLI output=yaml cat country.resource-updated2.yaml ``` ```python script tabs=Python output=yaml with open(\u0027country.resource-updated2.yaml\u0027) as file: print(file.read()) ```",
      },
          'docs/guides/extracting-data': {
          'name': 'Extracting Data',
          'path': 'docs/guides/extracting-data',
          'relpath': '../guides/extracting-data',
          'text': "# Extracting Data \u003e This guide assumes basic familiarity with the Frictionless Framework. To learn more, please read the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction) and [Quick Start](https://framework.frictionlessdata.io/docs/guides/quick-start). Extracting data means reading tabular data from a source. We can use various customizations for this process such as providing a file format, table schema, limiting fields or rows amount, and much more. This guide will discuss the main `extract` functions (`extract`, `extract_resource`, `extract_package`) and will then go into more advanced details about the `Resource Class`, `Package Class`, `Header Class`, and `Row Class`. The output from the extract function is in \u0027utf-8\u0027 encoding scheme. Let\u0027s see this with some real files: \u003e Download [`country-3.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/country-3.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat country-3.csv ``` ```python script tabs=Python with open(\u0027country-3.csv\u0027) as file: print(file.read()) ``` \u003e Download [`capital-3.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/capital-3.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat capital-3.csv ``` ```python script tabs=Python with open(\u0027capital-3.csv\u0027) as file: print(file.read()) ``` To start, we will extract data from a resource: ```bash script tabs=CLI frictionless extract country-3.csv ``` ```python script tabs=Python from pprint import pprint from frictionless import extract rows = extract(\u0027country-3.csv\u0027) pprint(rows) ``` ## Extract Functions The high-level interface for extracting data provided by Frictionless is a set of `extract` functions: - `extract`: detects the source file type and extracts data accordingly - `resource.extract`: returns a data table - `package.extract`: returns a map of the package\u0027s tables As described in more detail in the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction), a resource is a single file, such as a data file, and a package is a set of files, such as a data file and a schema. The command/function would be used as follows: ```bash tabs=CLI frictionless extract your-table.csv frictionless extract your-resource.json --type resource frictionless extract your-package.json --type package ``` ```python tabs=Python from frictionless import extract rows = extract(\u0027capital-3.csv\u0027) resource = extract(\u0027capital-3.csv\u0027, type=\"resource\") package = extract(\u0027capital-3.csv\u0027, type=\"package\") ``` The `extract` functions always reads data in the form of rows, into memory. The lower-level interfaces will allow you to stream data, which you can read about in the [Resource Class](#resource-class) section below. ## Extracting a Resource A resource contains only one file. To extract a resource, we have three options. First, we can use the same approach as above, extracting from the data file itself: ```bash script tabs=CLI frictionless extract capital-3.csv ``` ```python script tabs=Python from pprint import pprint from frictionless import extract rows = extract(\u0027capital-3.csv\u0027) pprint(rows) ``` Our second option is to extract the resource from a descriptor file by using the `extract_resource` function. A descriptor file is useful because it can contain different metadata and be stored on the disc. As an example of how to use `extract_resource`, let\u0027s first create a descriptor file (note: this example uses YAML for the descriptor, but Frictionless also supports JSON): ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027capital-3.csv\u0027) resource.infer() # as an example, in the next line we will append the schema resource.schema.missing_values.append(\u00273\u0027) # will interpret 3 as a missing value resource.to_yaml(\u0027capital.resource-test.yaml\u0027) # use resource.to_json for JSON format ``` You can also use a pre-made descriptor file. Now, this descriptor file can be used to extract the resource: ```bash script tabs=CLI frictionless extract capital.resource-test.yaml ``` ```python script tabs=Python from pprint import pprint from frictionless import extract rows = extract(\u0027capital.resource.yaml\u0027) pprint(rows) ``` So what has happened in this example? We set the textual representation of the number \"3\" to be a missing value. In the output we can see how the `id` number 3 now appears as `None` representing a missing value. This toy example demonstrates how the metadata in a descriptor can be used; other values like \"NA\" are more common for missing values. You can read more advanced details about the [Resource Class below](#resource-class). ## Extracting a Package The third way we can extract information is from a package, which is a set of two or more files, for instance, two data files and a corresponding metadata file. As a primary example, we provide two data files to the `extract` command which will be enough to detect that it\u0027s a dataset. Let\u0027s start by using the command-line interface: ```bash script tabs=CLI frictionless extract *-3.csv ``` ```python script tabs=Python from pprint import pprint from frictionless import extract data = extract(\u0027*-3.csv\u0027) pprint(data) ``` We can also extract the package from a descriptor file using the `package.extract` function (Note: see the [Package Class section](#package-class) for the creation of the `country.package.yaml` file): ```bash script tabs=CLI frictionless extract country.package.yaml ``` ```python script tabs=Python from frictionless import Package package = Package(\u0027country.package.yaml\u0027) pprint(package.extract()) ``` You can read more advanced details about the [Package Class below](#package-class). \u003e The following sections contain further, advanced details about the `Resource Class`, `Package Class`, `Header Class`, and `Row Class`. ## Resource Class The Resource class provides metadata about a resource with read and stream functions. The `extract` functions always read rows into memory; Resource can do the same but it also gives a choice regarding output data which can be `rows`, `data`, `text`, or `bytes`. Let\u0027s try reading all of them. ### Reading Bytes It\u0027s a byte representation of the contents: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(\u0027country-3.csv\u0027) pprint(resource.read_bytes()) ``` ### Reading Text It\u0027s a textual representation of the contents: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027country-3.csv\u0027) pprint(resource.read_text()) ``` ### Reading Cells For a tabular data there are raw representaion of the tabular contents: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027country-3.csv\u0027) pprint(resource.read_cells()) ``` ### Reading Rows For a tabular data there are row available which is are normalized lists presented as dictionaries: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027country-3.csv\u0027) pprint(resource.read_rows()) ``` ### Reading a Header For a tabular data there is the Header object available: ```python script tabs=Python from frictionless import Resource with Resource(\u0027country-3.csv\u0027) as resource: pprint(resource.header) ``` ### Streaming Interfaces It\u0027s really handy to read all your data into memory but it\u0027s not always possible if a file is very big. For such cases, Frictionless provides streaming functions: ```python tabs=Python from frictionless import Resource with Resource(\u0027country-3.csv\u0027) as resource: resource.byte_stream resource.text_stream resource.list_stream resource.row_stream ``` ## Package Class The Package class provides functions to read the contents of a package. First of all, let\u0027s create a package descriptor: ```bash script tabs=CLI frictionless describe *-3.csv --json \u003e country.package.json ``` ```python script tabs=Python from frictionless import describe package = describe(\u0027*-3.csv\u0027) package.to_json(\u0027country.package.json\u0027) ``` Note that --json is used here to output the descriptor in JSON format. Without this, the default output is in YAML format as we saw above. We can create a package from data files (using their paths) and then read the package\u0027s resources: ```python script tabs=Python from frictionless import Package package = Package(\u0027*-3.csv\u0027) pprint(package.get_resource(\u0027country-3\u0027).read_rows()) pprint(package.get_resource(\u0027capital-3\u0027).read_rows()) ``` The package by itself doesn\u0027t provide any read functions directly because it\u0027s just a contrainer. You can select a pacakge\u0027s resource and use the Resource API from above for data reading.",
      },
          'docs/guides/validating-data': {
          'name': 'Validating Data',
          'path': 'docs/guides/validating-data',
          'relpath': '../guides/validating-data',
          'text': "# Validating Data \u003e This guide assumes basic familiarity with the Frictionless Framework. To learn more, please read the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction) and [Quick Start](https://framework.frictionlessdata.io/docs/guides/quick-start). Tabular data validation is a process of identifying problems that have occured in your data so you can correct them. Let\u0027s explore how Frictionless helps to achieve this task using an invalid data table example: \u003e Download [`capital-invalid.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/capital-invalid.csv) to reproduce the examples (right-click and \"Save link as\").. ```bash script tabs=CLI cat capital-invalid.csv ``` ```python script tabs=Python with open(\u0027capital-invalid.csv\u0027) as file: print(file.read()) ``` We can validate this file by using both command-line interface and high-level functions. Frictionless provides comprehensive error details so that errors can be understood by the user. Continue reading to learn the validation process in detail. ```bash script tabs=CLI frictionless validate capital-invalid.csv ``` ```python script tabs=Python from pprint import pprint from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027) print(report) ``` ## Validate Functions The high-level interface for validating data provided by Frictionless is a set of `validate` functions: - `validate`: detects the source type and validates data accordingly - `Schema.validate_descriptor`: validates a schema\u0027s metadata - `resource.validate`: validates a resource\u0027s data and metadata - `package.validate`: validates a package\u0027s data and metadata - `inquiry.validate`: validates a special `Inquiry` object which represents a validation task instruction On the command-line, there is only one command but there is a flag to adjust the behavior. It\u0027s useful when you have a file which has a ambiguous type, for example, a json file containing a data instead of metadata: ```bash tabs=CLI frictionless validate your-data.csv frictionless validate your-schema.yaml --type schema frictionless validate your-data.csv --type resource frictionless validate your-package.json --type package frictionless validate your-inquiry.yaml --type inquiry ``` As a reminder, in the Frictionless ecosystem, a resource is a single file, such as a data file, and a package is a set of files, such as a data file and a schema. This concept is described in more detail in the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction). ## Validating a Schema The `Schema.validate_descriptor` function is the only function validating solely metadata. To see this work, let\u0027s create an invalid table schema: ```python script tabs=Python import yaml from frictionless import Schema descriptor = {} descriptor[\u0027fields\u0027] = \u0027bad\u0027 # must be a list with open(\u0027bad.schema.yaml\u0027, \u0027w\u0027) as file: yaml.dump(descriptor, file) ``` And let\u0027s validate this schema: ```bash script tabs=CLI frictionless validate bad.schema.yaml ``` ```python script tabs=Python from pprint import pprint from frictionless import validate report = validate(\u0027bad.schema.yaml\u0027) pprint(report) ``` We see that the schema is invalid and the error is displayed. Schema validation can be very useful when you work with different classes of tables and create schemas for them. Using this function will ensure that the metadata is valid. ## Validating a Resource As was shown in the [\"Describing Data\" guide](https://framework.frictionlessdata.io/docs/guides/describing-data), a resource is a container having both metadata and data. We need to create a resource descriptor and then we can validate it: ```bash script tabs=CLI frictionless describe capital-invalid.csv \u003e capital.resource.yaml ``` ```python script tabs=Python from frictionless import describe resource = describe(\u0027capital-invalid.csv\u0027) resource.to_yaml(\u0027capital.resource.yaml\u0027) ``` Note: this example uses YAML for the resource descriptor format, but Frictionless also supports JSON format also. Let\u0027s now validate to ensure that we are getting the same result that we got without using a resource: ```bash script tabs=CLI frictionless validate capital.resource.yaml ``` ```python script tabs=Python output=python from frictionless import validate report = validate(\u0027capital.resource.yaml\u0027) print(report) ``` Okay, why do we need to use a resource descriptor if the result is the same? The reason is metadata + data packaging. Let\u0027s extend our resource descriptor to show how you can edit and validate metadata: ```python script tabs=Python from frictionless import describe resource = describe(\u0027capital-invalid.csv\u0027) resource.add_defined(\u0027stats\u0027) # TODO: fix and remove this line resource.stats.md5 = \u0027ae23c74693ca2d3f0e38b9ba3570775b\u0027 # this is a made up incorrect resource.stats.bytes = 100 # this is wrong resource.to_yaml(\u0027capital.resource-bad.yaml\u0027) ``` We have added a few incorrect, made up attributes to our resource descriptor as an example. Now, the validation below reports these errors in addition to all the errors we had before. This example shows how concepts like Data Resource can be extremely useful when working with data. ```bash script tabs=CLI frictionless validate capital.resource-bad.yaml # TODO: it should have 7 errors ``` ```python script tabs=Python from frictionless import validate report = validate(\u0027capital.resource-bad.yaml\u0027) print(report) ``` ## Validating a Package A package is a set of resources + additional metadata. To showcase a package validation we need to use one more tabular file: \u003e Download [`capital-valid.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/capital-valid.csv) to reproduce the examples (right-click and \"Save link as\"). ```bash script tabs=CLI cat capital-valid.csv ``` ```python script tabs=Python with open(\u0027capital-valid.csv\u0027) as file: print(file.read()) ``` Now let\u0027s describe and validate a package which contains the data files we have seen so far: ```bash script tabs=CLI frictionless describe capital-*id.csv \u003e capital.package.yaml frictionless validate capital.package.yaml ``` ```python script tabs=Python from frictionless import describe, validate # create package descriptor package = describe(\"capital-*id.csv\") package.to_yaml(\"capital.package.yaml\") # validate report = validate(\"capital.package.yaml\") print(report) ``` As we can see, the result is in a similar format to what we have already seen, and shows errors as we expected: we have one invalid resource and one valid resource. ## Validating an Inquiry \u003e The Inquiry is an advanced concept mostly used by software integrators. For example, under the hood, Frictionless Framework uses inquiries to implement client-server validation within the built-in API. Please skip this section if this information feels unnecessary for you. Inquiry is a declarative representation of a validation job. It gives you an ability to create, export, and share arbitrary validation jobs containing a set of individual validation tasks. Tasks in the Inquiry accept the same arguments written in camelCase as the corresponding `validate` functions. Let\u0027s create an Inquiry that includes an individual file validation and a resource validation. In this example we will use the data file, `capital-valid.csv` and the resource, `capital.resource.json` which describes the invalid data file we have already seen: ```python script tabs=Python from frictionless import Inquiry, InquiryTask inquiry = Inquiry(tasks=[ InquiryTask(path=\u0027capital-valid.csv\u0027), InquiryTask(resource=\u0027capital.resource.yaml\u0027), ]) inquiry.to_yaml(\u0027capital.inquiry.yaml\u0027) ``` As usual, let\u0027s run validation: ```bash script tabs=CLI frictionless validate capital.inquiry.yaml ``` ```python script tabs=Python from frictionless import validate report = validate(\"capital.inquiry.yaml\") print(report) ``` At first sight, it might not be clear why such a construct exists, but when your validation workflow gets complex, the Inquiry can provide a lot of flexibility and power. \u003e The Inquiry will use multiprocessing if there is the `parallel` flag provided. It might speed up your validation dramatically especially on a 4+ cores processor. ## Validation Report All the `validate` functions return a Validation Report. This is a unified object containing information about a validation: source details, the error, etc. Let\u0027s explore a report: ```python script tabs=Python output=python from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027, pick_errors=[\u0027duplicate-label\u0027]) print(report) ``` As we can see, there is a lot of information; you can find a detailed description of the Validation Report in the [API Reference](../docs/framework/report.html#reference). Errors are grouped by tasks (i.e. data files); for some validation there can be dozens of tasks. Let\u0027s use the `report.flatten` function to simplify the representation of errors. This function helps to represent a report as a list of errors: ```python script tabs=Python output=python from pprint import pprint from frictionless import validate report = validate(\"capital-invalid.csv\", pick_errors=[\"duplicate-label\"]) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"code\", \"message\"])) ``` In some situations, an error can\u0027t be associated with a task; then it goes to the top-level `report.errors` property: ```python script tabs=Python output=python from frictionless import validate report = validate(\"bad.json\", type=\u0027schema\u0027) print(report) ``` ## Validation Errors The Error object is at the heart of the validation process. The Report has `report.errors` and `report.tasks[].errors`, properties that can contain the Error object. Let\u0027s explore it by taking a deeper look at the `duplicate-label` error: ```python script tabs=Python from frictionless import validate report = validate(\"capital-invalid.csv\", pick_errors=[\"duplicate-label\"]) error = report.error # this is only available for one table / one error sitution print(f\u0027Type: \"{error.type}\"\u0027) print(f\u0027Title: \"{error.title}\"\u0027) print(f\u0027Tags: \"{error.tags}\"\u0027) print(f\u0027Note: \"{error.note}\"\u0027) print(f\u0027Message: \"{error.message}\"\u0027) print(f\u0027Description: \"{error.description}\"\u0027) ``` Above, we have listed universal error properties. Depending on the type of an error there can be additional ones. For example, for our `duplicate-label` error: ```python script tabs=Python output=python from frictionless import validate report = validate(\"capital-invalid.csv\", pick_errors=[\"duplicate-label\"]) error = report.error # this is only available for one table / one error sitution print(error) ``` ``` {\u0027code\u0027: \u0027duplicate-label\u0027, \u0027description\u0027: \u0027Two columns in the header row have the same value. Column \u0027 \u0027names should be unique.\u0027, \u0027fieldName\u0027: \u0027name2\u0027, \u0027fieldNumber\u0027: 3, \u0027fieldPosition\u0027: 3, \u0027label\u0027: \u0027name\u0027, \u0027labels\u0027: [\u0027id\u0027, \u0027name\u0027, \u0027name\u0027], \u0027message\u0027: \u0027Label \"name\" in the header at position \"3\" is duplicated to a \u0027 \u0027label: at position \"2\"\u0027, \u0027name\u0027: \u0027Duplicate Label\u0027, \u0027note\u0027: \u0027at position \"2\"\u0027, \u0027rowPositions\u0027: [1], \u0027tags\u0027: [\u0027#table\u0027, \u0027#header\u0027, \u0027#label\u0027]} ``` Please explore the [Errors Reference](/docs/references/errors-reference) to learn about all the available errors and their properties. ## Available Checks There are various validation checks included in the core Frictionless Framework along with an ability to create custom checks. See [Validation Checks](../checks/cell.html) for a list of available checks. ```python script tabs=Python from pprint import pprint from frictionless import validate, checks checks = [checks.sequential_value(field_name=\u0027id\u0027)] report = validate(\u0027capital-invalid.csv\u0027, checks=checks) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"type\", \"note\"])) ``` ``` [[None, 3, \u0027duplicate-label\u0027, \u0027at position \"2\"\u0027], [10, 3, \u0027missing-cell\u0027, \u0027\u0027], [10, 1, \u0027sequential-value\u0027, \u0027the value is not sequential\u0027], [11, None, \u0027blank-row\u0027, \u0027\u0027], [12, 1, \u0027type-error\u0027, \u0027type is \"integer/default\"\u0027], [12, 4, \u0027extra-cell\u0027, \u0027\u0027]] ``` \u003e Note that only the Baseline Check is enabled by default. Other built-in checks need to be activated as shown below. ## Custom Checks There are many cases when built-in Frictionless checks are not enough. For instance, you might want to create a business logic rule or specific quality requirement for the data. With Frictionless it\u0027s very easy to use your own custom checks. Let\u0027s see with an example: ```python script tabs=Python from pprint import pprint from frictionless import Check, validate, errors # Create check class forbidden_two(Check): Errors = [errors.CellError] def validate_row(self, row): if row[\u0027header\u0027] == 2: note = \u00272 is forbidden!\u0027 yield errors.CellError.from_row(row, note=note, field_name=\u0027header\u0027) # Validate table source = b\u0027header\\n1\\n2\\n3\u0027 report = validate(source, format=\u0027csv\u0027, checks=[forbidden_two()]) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"code\", \"note\"])) ``` Usually, it also makes sense to create a custom error for your custom check. The Check class provides other useful methods like `validate_header` etc. Please read the [API Reference](../references/api-reference.md) for more details. Learn more about custom checks in the [Check Guide](.../docs/checks/baseline.html#reference). ## Pick/Skip Errors We can pick or skip errors by providing a list of error codes. This is useful when you already know your data has some errors, but you want to ignore them for now. For instance, if you have a data table with repeating header names. Let\u0027s see an example of how to pick and skip errors: ```python script tabs=Python from pprint import pprint from frictionless import validate report1 = validate(\"capital-invalid.csv\", pick_errors=[\"duplicate-label\"]) report2 = validate(\"capital-invalid.csv\", skip_errors=[\"duplicate-label\"]) pprint(report1.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) pprint(report2.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` It\u0027s also possible to use error tags (for more information please consult the [Errors Reference](../references/errors-reference.md)): ```python script tabs=Python from pprint import pprint from frictionless import validate report1 = validate(\"capital-invalid.csv\", pick_errors=[\"#header\"]) report2 = validate(\"capital-invalid.csv\", skip_errors=[\"#row\"]) pprint(report1.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) pprint(report2.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` ## Limit Errors This option allows you to limit the amount of errors, and can be used when you need to do a quick check or want to \"fail fast\". For instance, here we use `limit_errors` to find just the 1st error and add it to our report: ```python title=\"Python\" from pprint import pprint from frictionless import validate report = validate(\"capital-invalid.csv\", limit_errors=1) pprint(report.flatten([\"rowNumber\", \"fieldNumber\", \"type\"])) ``` ``` [[None, 3, \u0027duplicate-label\u0027]] ```",
      },
          'docs/guides/transforming-data': {
          'name': 'Transforming Data',
          'path': 'docs/guides/transforming-data',
          'relpath': '../guides/transforming-data',
          'text': "# Transforming Data \u003e This guide assumes basic familiarity with the Frictionless Framework. To learn more, please read the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction) and [Quick Start](https://framework.frictionlessdata.io/docs/guides/quick-start). Transforming data in Frictionless means modifying data and metadata from state A to state B. For example, it could be transforming a messy Excel file to a cleaned CSV file, or transforming a folder of data files to a data package we can publish more easily. To read more about the concepts behind Frictionless Transform, please check out the [Transform Principles](#transform-principles) sections belows. In comparison to similiar Python software like Pandas, Frictionless provides better control over metadata, has a modular API, and fully supports Frictionless Specifications. Also, it is a streaming framework with an ability to work with large data. As a downside of the Frictionless architecture, it might be slower compared to other Python packages, especially to projects like Pandas. Keep reading below to learn about the principles underlying Frictionless Transform, or [skip ahead](/docs/guides/transform-guide#transform-functions) to see how to use the Transform code. ## Transform Principles Frictionless Transform is based on a few core principles which are shared with other parts of the framework: ### Conceptual Simplicity Frictionless Transform can be thought of as a list of functions that accept a source resource/package object and return a target resource/package object. Every function updates the input\u0027s metadata and data - and nothing more. We tried to make this straightforward and conceptually simple, because we want our users to be able to understand the tools and master them. ### Metadata Matters There are plenty of great ETL-frameworks written in Python and other languages. We use one of them (PETL) under the hood (described in more detail later). The core difference between Frictionless and others is that we treat metadata as a first-class citizen. This means that you don\u0027t lose type and other important information during the pipeline evaluation. ### Data Streaming Whenever possible, Frictionless streams the data instead of reading it into memory. For example, for sorting big tables we use a memory usage threshold and when it is met we use the file system to unload the data. The ability to stream data gives users power to work with files of any size, even very large files. ### Lazy Evaluation With Frictionless all data manipulation happens on-demand. For example, if you reshape one table in a data package containing 10 big csv files, Frictionless will not even read the 9 other tables. Frictionless tries to be as explicit as possible regarding actions taken. For example, it will not use CPU resources to cast data unless a user adds a `normalize` step. So it\u0027s possible to transform a rather big file without even casting types, for example, if you only need to reshape it. ### Software Reuse For the core transform functions, Frictionless uses the amazing [PETL](https://petl.readthedocs.io/en/stable/) project under the hood. This library provides lazy-loading functionality in running data pipelines. On top of PETL, Frictionless adds metadata management and a bridge between Frictionless concepts like Package/Resource and PETL\u0027s processors. ## Transform Functions Frictionless supports a few different kinds of data and metadata transformations: - resource and package transformations - transformations based on a declarative pipeline The main difference between these is that resource and package transforms are imperative while pipelines can be created beforehand or shared as a JSON file. We\u0027ll talk more about pipelines in the [Transforming Pipeline](#transforming-pipeline) section below. First, we will introduce the transform functions, then go into detail about how to transform a resource and a package. As a reminder, in the Frictionless ecosystem, a resource is a single file, such as a data file, and a package is a set of files, such as a data file and a schema. This concept is described in more detail in the [Introduction](https://framework.frictionlessdata.io/docs/guides/introduction). \u003e Download [`transform.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/transform.csv) to reproduce the examples (right-click and \"Save link as\". You might need to change the file extension from .txt to .csv). ```bash script tabs=CLI cat transform.csv ``` The high-level interface to transform data is a set of `transform` functions: - `transform`: detects the source type and transforms data accordingly - `resource.transform`: transforms a resource - `package.transform`: transforms a package We\u0027ll see examples of these functions in the next few sections. ## Transforming a Resource Let\u0027s write our first transformation. Here, we will transform a data file (a resource) by defining a source resource, applying transform steps and getting back a resulting target resource: ```python script tabs=Python from frictionless import Resource, Pipeline, steps # Define source resource source = Resource(path=\"transform.csv\") # Create a pipeline pipeline = Pipeline(steps=[ steps.table_normalize(), steps.field_add(name=\"cars\", formula=\u0027population*2\u0027, descriptor={\u0027type\u0027: \u0027integer\u0027}), ]) # Apply transform pipeline target = source.transform(pipeline) # Print resulting schema and data print(target.schema) print(target.to_view()) ``` Let\u0027s break down the transforming steps we applied: 1. `steps.table_normalize` - cast data types and shape the table according to the schema, inferred or provided 1. `steps.field_add` - adds a field to data and metadata based on the information provided by the user There are many more available steps that we will cover below. ## Transforming a Package A package is a set of resources. Transforming a package means adding or removing resources and/or transforming those resources themselves. This example shows how transforming a package is similar to transforming a single resource: ```python script tabs=Python from frictionless import Package, Resource, transform, steps # Define source package source = Package(resources=[Resource(name=\u0027main\u0027, path=\"transform.csv\")]) # Create a pipeline pipeline = Pipeline(steps=[ steps.resource_add(name=\"extra\", descriptor={\"data\": [[\u0027id\u0027, \u0027cars\u0027], [1, 166], [2, 132], [3, 94]]}), steps.resource_transform( name=\"main\", steps=[ steps.table_normalize(), steps.table_join(resource=\"extra\", field_name=\"id\"), ], ), steps.resource_remove(name=\"extra\"), ]) # Apply transform steps target = source.transform(pipeline) # Print resulting resources, schema and data print(target.resource_names) print(target.get_resource(\"main\").schema) print(target.get_resource(\"main\").to_view()) ``` We have basically done the same as in [Transforming a Resource](#transforming-a-resource) section. This example is quite artificial and created only to show how to join two resources, but hopefully it provides a basic understanding of how flexible package transformations can be. ## Transforming Pipeline A pipeline is a declarative way to write out metadata transform steps. With a pipeline, you can transform a resource, package, or write custom plugins too. For resource and package types it\u0027s mostly the same functionality as we have seen above, but written declaratively. So let\u0027s run the same resource transformation as we did in the [Transforming a Resource](#transforming-a-resource) section: ```python script tabs=Python from frictionless import Pipeline, transform pipeline = Pipeline.from_descriptor({ \"steps\": [ {\"type\": \"table-normalize\"}, { \"type\": \"field-add\", \"name\": \"cars\", \"formula\": \"population*2\", \"descriptor\": {\"type\": \"integer\"} }, ], }) print(pipeline) ``` So what\u0027s the reason to use declarative pipelines if it works the same as the Python code? The main difference is that pipelines can be saved as JSON files which can be shared among different users and used with CLI and API. For example, if you implement your own UI based on Frictionless Framework you can serialize the whole pipeline as a JSON file and send it to the server. This is the same for CLI - if your colleague has given you a `pipeline.json` file, you can run `frictionless transform pipeline.json` in the CLI to get the same results as they got. ## Available Steps Frictionless includes more than 40+ built-in transform steps. They are grouped by the object so you can find them easily using code auto completion in a code editor. For example, start typing `steps.table...` and you will see all the available steps for that group. The available groups are: - resource - table - field - row - cell See [Transform Steps](../steps/cell.html) for a list of all available steps. It is also possible to write custom transform steps: see the next section. ## Custom Steps Here is an example of a custom step written as a Python function. This example step removes a field from a data table (note: Frictionless already has a built-in function that does this same thing: `steps.field_remove`). ```python script tabs=Python from frictionless import Package, Resource, Step, transform, steps class custom_step(Step): def transform_resource(self, resource): current = resource.to_copy() # Data def data(): with current: for list in current.cell_stream: yield list[1:] # Meta resource.data = data resource.schema.remove_field(\"id\") source = Resource(\"transform.csv\") pipeline = Pipeline(steps=[custom_step()]) target = source.transform(pipeline) print(target.schema) print(target.to_view()) ``` As you can see you can implement any custom steps within a Python script. To make it work within a declarative pipeline you need to implement a plugin. Learn more about [Custom Steps](extension/step-guide.md) and [Plugins](extension/plugin-guide.md). ## Transform Utils \u003e Transform Utils is under construction. ## Working with PETL In some cases, it\u0027s better to use a lower-level API to achieve your goal. A resource can be exported as a PETL table. For more information please visit PETL\u0027s [documentation portal](https://petl.readthedocs.io/en/stable/). ```python script tabs=Python from frictionless import Resource resource = Resource(path=\u0027transform.csv\u0027) petl_table = resource.to_petl() # Use it with PETL framework print(petl_table) ```",
      },
          'docs/console/overview': {
          'name': 'Overview',
          'path': 'docs/console/overview',
          'relpath': 'overview',
          'text': "# Overview The Command-Line interface is a vital part for the Frictionless Framework. While working within Python provides more flexibility, CLI is the easist way to interact with Frictionless. ```yaml video/youtube code: 7a_rL9j-gn8 ``` ## Install To install the package please follow the [Getting Started](../getting-started.html) guide. Usually, a simple installation using Pip or Anaconda will install the `frictionless` binary on your computer so you don\u0027t need to install CLI aditionally. ## Commands The `frictionless` binary requires providing a command like `describe` or `validate`: ```bash tabs=CLI frictionless describe # to describe your data frictionless explore # to explore your data frictionless extract # to extract your data frictionless index # to index your data frictionless list # to list your data frictionless publish # to publish your data frictionless query # to query your data frictionless script # to script your data frictionless validate # to validate your data frictionless --help # to get list of the command frictionless --version # to get the version ``` ## Arguments All the arguments for the main CLI command are the same as they are in Python. You can read [Guides](../guides/describing-data.html) and use almost all the information from there within the command-line. There is an important different in how arguments are written (note the dashes): ``` Python: validate(\u0027data/table.csv\u0027, limit_errors=1) CLI: $ validate data/table.csv --limit-errors 1 ``` To get help for a command and its arguments you can use the help flag with the command: ```bash tabs=CLI frictionless describe --help # to get help for describe frictionless extract --help # to get help for extract frictionless validate --help # to get help for validate frictionless transform --help # to get help for transform ``` ## Outputs Usually, Frictionless commands returns pretty-formatted tabular data like `extract` or `validate` do. For the `describe` command you get a metadata back and you can choose in what format to return it: ```bash tabs=CLI frictionless describe # default YAML with a commented front-matter frictionless describe --yaml # standard YAML frictionless describe --json # standard JSON ``` ## Errors The Frictionless\u0027 CLI interface should not fail with any internal Python errors with a traceback (a long listing of related code). If you see something like this please create an issue in the [Issue Tracker](https://github.com/frictionlessdata/frictionless-py/issues). ## Debug To debug a problem please use: ```bash tabs=CLI frictionless command --debug ```",
      },
          'docs/console/describe': {
          'name': 'Describe',
          'path': 'docs/console/describe',
          'relpath': 'describe',
          'text': "# Describe ```markdown remark type=info The difference with between `describe` and `list` command: if `datapackage.json` is not provided `describe` will load a sample from every tabular data file in a dataset and infer a schema while `list` is a very lean and quick command operating only with available metadata and not touching actual data files. ``` With Frtictionless `describe` command you can get a metadata of file or a dataset. ## Normal Mode By default, it outputs metadata visually formatted: ```bash script tabs=CLI frictionless describe tables/*.csv ``` ## Yaml/Json Mode It\u0027s possible to output as `YAML` or `JSON`, for example: ```bash script tabs=CLI frictionless describe tables/*.csv --yaml ```",
      },
          'docs/console/convert': {
          'name': 'Convert',
          'path': 'docs/console/convert',
          'relpath': 'convert',
          'text': "# Convert ```markdown remark type=warning This command currenlty is in active development and for dialect updated there are very few options available ``` ```markdown remark type=info Wit this command Frictionless will drop all invalid data like type errors in cells. Use `validate` if needed. ``` With `convert` command you can quickly convert a tabular data file from one format to another (or the same format with different dialect): ## Format Conversion For example, let\u0027s convert a CSV file into an Excel: ```bash tabs=CLI frictionless convert table.csv table.xlsx ``` ## Downloading Files The command can be used for downloading files as well. For example, let\u0027s cherry-pick one CSV file from a Zenodo dataset: ```bash tabs=CLI frictionless convert https://zenodo.org/record/3977957 --name aaawrestlers --to-path test.csv ``` ## Dialect Updates Consider, we want to change the CSV delimiter: ```bash tabs=CLI frictionless convert table.csv table-copy.csv --csv-delimiter ; ```",
      },
          'docs/console/explore': {
          'name': 'Explore',
          'path': 'docs/console/explore',
          'relpath': 'explore',
          'text': "# Explore ```markdown remark type=info If you started an exploration session and can\u0027t get out: press \"q\" on your keyboard. ``` With the `explore` command you can open your dataset in [Visidata](https://www.visidata.org/) which is an amazing visual tool for working with tabular data in Console. For example try \"Shift+F\" for creating data histograms! ## Installation ```bash tabs=CLI pip install frictionless[visidata] pip install frictionless[visidata,zenodo] # for examples in this tutorial ``` ## Example For example, let\u0027s expore this interesing dataset: ```bash tabs=CLI frictionless explore https://zenodo.org/record/3977957 ``` ```yaml image path: ../../assets/explore.png width: unset height: unset ``` ## Documentation Before entering Visidata, it\u0027s highly recommended to read its documentation: - https://www.visidata.org/docs/ You can get it in Console as well: ```bash script tabs=CLI vd --help ```",
      },
          'docs/console/extract': {
          'name': 'Extract',
          'path': 'docs/console/extract',
          'relpath': 'extract',
          'text': "# Extract ```markdown remark type=info Wit this command Frictionless will drop all invalid data like type errors in cells. Use `validate` if needed. ``` With Frtictionless `extract` command you can extract data from a file or a dataset. ## Normal Mode By default, it outputs metadata visually formatted: ```bash script tabs=CLI frictionless extract tables/*.csv ``` ## Yaml/Json Mode It\u0027s possible to output as `YAML` or `JSON`, for example: ```bash script tabs=CLI frictionless extract tables/*.csv --yaml ```",
      },
          'docs/console/index': {
          'name': 'Index',
          'path': 'docs/console/index',
          'relpath': 'index',
          'text': "# Index ```markdown remark type=info Wit this command Frictionless will drop all invalid data like type errors in cells. Use `validate` if needed. ``` ```markdown remark type=warning This functionality has been published in `frictionless@5.5` as a feature preview and request for comments. The implementation is raw and doesn\u0027t cover many edge cases. ``` Indexing resource in Frictionless terms means loading a data table into a database. Let\u0027s explore how this feature works in different modes. ## Installation ```bash tabs=CLI pip install frictionless[sql] ``` ## Normal Mode This mode is supported for any database that is supported by `sqlalchemy`. Under the hood, Frictionless will infer Table Schema and populate the data table as it normally reads data. It means that type errors will be replaced by `null` values and in-general it guarantees to finish successfully for any data even very invalid. ```bash script tabs=CLI frictionless index table.csv --database sqlite:///index/project.db frictionless extract sqlite:///index/project.db --table table --json ``` ## Fast Mode ```markdown remark type=warning For the SQLite in fast mode, it requires `sqlite3@3.34+` command to be available. ``` Fast mode is supported for SQLite and Postgresql databases. It will infer Table Schema using a data sample and index data using `COPY` in Potgresql and `.import` in SQLite. For big data files this mode will be 10-30x faster than normal indexing but the speed comes with the price -- if there is invalid data the indexing will fail. ```bash script tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --fast frictionless extract sqlite:///index/project.db --table table --json ``` ### Solution 1: Fallback To ensure that the data will be successfully indexed it\u0027s possible to use `fallback` option. If the fast indexing fails Frictionless will start over in normal mode and finish the process successfully. ```bash tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table --fast --fallback ``` ### Solution 2: QSV Another option is to provide a path to [QSV](https://github.com/jqnatividad/qsv) binary. In this case, initial schema inferring will be done based on the whole data file and will guarantee that the table is valid type-wise: ```bash tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table --fast --qsv qsv_path ```",
      },
          'docs/console/list': {
          'name': 'List',
          'path': 'docs/console/list',
          'relpath': 'list',
          'text': "# List ```markdown remark type=info The difference with between `describe` and `list` command: if `datapackage.json` is not provided `describe` will load a sample from every tabular data file in a dataset and infer a schema while `list` is a very lean and quick command operating only with available metadata and not touching actual data files. ``` With Frtictionless `list` command you can get a list of resources from a data source. For more detailed output see [`describe`](describe.html) command. ## Normal Mode By default, it outputs metadata visually formatted: ```bash script frictionless list tables/*.csv ``` ## Yaml/Json Mode It\u0027s possible to output as `YAML` or `JSON`, for example: ```bash script frictionless list tables/*.csv --yaml ```",
      },
          'docs/console/publish': {
          'name': 'Publish',
          'path': 'docs/console/publish',
          'relpath': 'publish',
          'text': "# Publish ```markdown remark type=warning Currently, only publishing to CKAN is supported; Github and Zenodo are in active development. ``` With `publish` command you can publish your dataset to a data publishing platform like CKAN: ```bash frictionless publish data/tables/*.csv --target http://ckan:5000/dataset/my-best --title \"My best dataset\" ``` It will ask for an API Key to upload your metadata and data. As a result: ```yaml image path: ../../assets/publish.png width: unset height: unset ```",
      },
          'docs/console/query': {
          'name': 'Query',
          'path': 'docs/console/query',
          'relpath': 'query',
          'text': "# Query ```markdown remark type=info Wit this command Frictionless will drop all invalid data like type errors in cells. Use `validate` if needed. ``` With `query` command you can explore tabular files within a Sqlite database. ## Installation ```bash tabs=CLI pip install frictionless[sql] pip install frictionless[sql,zenodo] # for examples in this tutorial ``` ## Usage ```bash frictionless query https://zenodo.org/record/3977957 ``` ```yaml image path: ../../assets/query.png width: unset height: unset ```",
      },
          'docs/console/script': {
          'name': 'Script',
          'path': 'docs/console/script',
          'relpath': 'script',
          'text': "# Script ```markdown remark type=info Wit this command Frictionless will drop all invalid data like type errors in cells. Use `validate` if needed. ``` With `script` command you can explore tabular files with Pandas by one console command ## Installation ```bash tabs=CLI pip install frictionless[sql] pip install frictionless[sql,zenodo] # for examples in this tutorial ``` ## Usage ```bash frictionless script https://zenodo.org/record/3977957 ``` ```yaml image path: ../../assets/script.png width: unset height: unset ```",
      },
          'docs/console/validate': {
          'name': 'Validate',
          'path': 'docs/console/validate',
          'relpath': 'validate',
          'text': "# Validate ```markdown remark type=warning For more information for data validation with Frictionless, read this [Validating Data](../guides/validating-data.html) tutorial. ``` With `validate` command you can validate your tabular files (indivisual or the whole dataset). For example: ```bash script tabs=CLI frictionless validate table.csv invalid.csv ```",
      },
          'docs/framework/actions': {
          'name': 'Data Actions',
          'path': 'docs/framework/actions',
          'relpath': '../framework/actions',
          'text': "# Data Actions ## Describe Describe is a high-level function (action) to infer a metadata from a data source. ### Example ```python script tabs=Python from frictionless import describe resource = describe(\u0027table.csv\u0027) print(resource) ``` ### Reference ```yaml reference level: 4 references: - frictionless.describe ``` ## Extract Extract is a high-level function (action) to read tabular data from a data source. The output is encoded in \u0027utf-8\u0027 scheme. ### Example ```python script tabs=Python from pprint import pprint from frictionless import extract rows = extract(\u0027table.csv\u0027) pprint(rows) ``` ### Reference ```yaml reference level: 4 references: - frictionless.extract ``` ## Validate Validate is a high-level function (action) to validate data from a data source. ### Example ```python script tabs=Python from frictionless import validate report = validate(\u0027table.csv\u0027) print(report.valid) ``` ### Reference ```yaml reference level: 4 references: - frictionless.validate ``` ## Transform Transform is a high-level function (action) to transform tabular data from a data source. ### Example ```python script tabs=Python from frictionless import transform, steps resource = transform(\u0027table.csv\u0027, steps=[steps.cell_set(field_name=\u0027name\u0027, value=\u0027new\u0027)]) print(resource.read_rows()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.transform ```",
      },
          'docs/framework/catalog': {
          'name': 'Catalog Class',
          'path': 'docs/framework/catalog',
          'relpath': '../framework/catalog',
          'text': "# Catalog Class ```markdown remark type=danger This feature is currently experimental. The API might change without notice ``` Catalog is a set of data packages. ## Creating Catalog We can create a catalog providing a list of data packages: ```python tabs=Python from frictionless import Catalog, Dataset, Package catalog = Catalog(datasets=[Dataset(name=\u0027name\u0027, package=Package(\u0027tables/*\u0027))]) ``` ## Describing Catalog Usually Catalog is used to describe some external set of datasets like a CKAN instance or a Github user or search. For example: ```python tabs=Python from frictionless import Catalog catalog = Catalog(\u0027https://demo.ckan.org/dataset/\u0027) print(catalog) ``` ## Dataset Management The core purpose of having a catalog is to provide an ability to have a set of datasets. The Catalog class provides useful methods to manage datasets: ```python tabs=Python from frictionless import Catalog catalog = Catalog(\u0027https://demo.ckan.org/dataset/\u0027) catalog.dataset_names catalog.has_dataset catalog.add_dataset catalog.get_dataset catalog.clear_datasets ``` ## Saving Descriptor As any of the Metadata classes the Catalog class can be saved as JSON or YAML: ```python tabs=Python from frictionless import Package catalog = Catalog(\u0027https://demo.ckan.org/dataset/\u0027) catalog.to_json(\u0027datacatalog.json\u0027) # Save as JSON catalog.to_yaml(\u0027datacatalog.yaml\u0027) # Save as YAML ``` ## Reference ```yaml reference references: - frictionless.Catalog - frictionless.Dataset ```",
      },
          'docs/framework/package': {
          'name': 'Package Class',
          'path': 'docs/framework/package',
          'relpath': '../framework/package',
          'text': "# Package Class The Data Package is a core Frictionless Data concept meaning a set of resources with additional metadata provided. You can read [Data Package Standard](https://specs.frictionlessdata.io/data-package/) for more information. ## Creating Package Let\u0027s create a data package: ```python tabs=Python from frictionless import Package, Resource package = Package(\u0027table.csv\u0027) # from a resource path package = Package(\u0027tables/*\u0027) # from a resources glob package = Package([\u0027tables/chunk1.csv\u0027, \u0027tables/chunk2.csv\u0027]) # from a list package = Package(\u0027package/datapackage.json\u0027) # from a descriptor path package = Package({\u0027resources\u0027: {\u0027path\u0027: \u0027table.csv\u0027}}) # from a descriptor package = Package(resources=[Resource(path=\u0027table.csv\u0027)]) # from arguments ``` As you can see it\u0027s possible to create a package providing different kinds of sources which will be detected to have some type automatically (e.g. whether it\u0027s a glob or a path). It\u0027s possible to make this step more explicit: ```python tabs=Python from frictionless import Package, Resource package = Package(resources=[Resource(path=\u0027table.csv\u0027)]) # from arguments package = Package(\u0027datapackage.json\u0027) # from a descriptor ``` ## Describing Package The standards support a great deal of package metadata which is possible to have with Frictionless Framework too: ```python script tabs=Python from frictionless import Package, Resource package = Package( name=\u0027package\u0027, title=\u0027My Package\u0027, description=\u0027My Package for the Guide\u0027, resources=[Resource(path=\u0027table.csv\u0027)], # it\u0027s possible to provide all the official properties like homepage, version, etc ) print(package) ``` If you have created a package, for example, from a descriptor you can access this properties: ```python script tabs=Python from frictionless import Package package = Package(\u0027datapackage.json\u0027) print(package.name) # and others ``` And edit them: ```python script tabs=Python from frictionless import Package package = Package(\u0027datapackage.json\u0027) package.name = \u0027new-name\u0027 package.title = \u0027New Title\u0027 package.description = \u0027New Description\u0027 # and others print(package) ``` ## Resource Management The core purpose of having a package is to provide an ability to have a set of resources. The Package class provides useful methods to manage resources: ```python script tabs=Python from frictionless import Package, Resource package = Package(\u0027datapackage.json\u0027) print(package.resources) print(package.resource_names) package.add_resource(Resource(name=\u0027new\u0027, data=[[\u0027key1\u0027, \u0027key2\u0027], [\u0027val1\u0027, \u0027val2\u0027]])) resource = package.get_resource(\u0027new\u0027) print(package.has_resource(\u0027new\u0027)) package.remove_resource(\u0027new\u0027) ``` ## Saving Descriptor As any of the Metadata classes the Package class can be saved as JSON or YAML: ```python tabs=Python from frictionless import Package package = Package(\u0027tables/*\u0027) package.to_json(\u0027datapackage.json\u0027) # Save as JSON package.to_yaml(\u0027datapackage.yaml\u0027) # Save as YAML ``` ## Reference ```yaml reference references: - frictionless.Package ```",
      },
          'docs/framework/resource': {
          'name': 'Resource Class',
          'path': 'docs/framework/resource',
          'relpath': '../framework/resource',
          'text': "# Resource Class The Resource class is arguable the most important class of the whole Frictionless Framework. It\u0027s based on [Data Resource Standard](https://specs.frictionlessdata.io/data-resource/) and [Tabular Data Resource Standard](https://specs.frictionlessdata.io/data-resource/) ## Creating Resource Let\u0027s create a data resource: ```python tabs=Python from frictionless import Resource resource = Resource(\u0027table.csv\u0027) # from a resource path resource = Resource(\u0027resource.json\u0027) # from a descriptor path resource = Resource({\u0027path\u0027: \u0027table.csv\u0027}) # from a descriptor resource = Resource(path=\u0027table.csv\u0027) # from arguments ``` As you can see it\u0027s possible to create a resource providing different kinds of sources which will be detector to have some type automatically (e.g. whether it\u0027s a descriptor or a path). It\u0027s possible to make this step more explicit: ```python tabs=Python from frictionless import Resource resource = Resource(path=\u0027data/table.csv\u0027) # from a path resource = Resource(\u0027data/resource.json\u0027) # from a descriptor ``` ## Describing Resource The standards support a great deal of resource metadata which is possible to have with Frictionless Framework too: ```python script tabs=Python from frictionless import Resource resource = Resource( name=\u0027resource\u0027, title=\u0027My Resource\u0027, description=\u0027My Resource for the Guide\u0027, path=\u0027table.csv\u0027, # it\u0027s possible to provide all the official properties like mediatype, etc ) print(resource) ``` If you have created a resource, for example, from a descriptor you can access this properties: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027resource.json\u0027) print(resource.name) # and others ``` And edit them: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027resource.json\u0027) resource.name = \u0027new-name\u0027 resource.title = \u0027New Title\u0027 resource.description = \u0027New Description\u0027 # and others print(resource) ``` ## Saving Descriptor As any of the Metadata classes the Resource class can be saved as JSON or YAML: ```python tabs=Python from frictionless import Resource resource = Resource(\u0027table.csv\u0027) resource.to_json(\u0027resource.json\u0027) # Save as JSON resource.to_yaml(\u0027resource.yaml\u0027) # Save as YAML ``` ## Resource Lifecycle You might have noticed that we had to duplicate the `with Resource(...)` statement in some examples. The reason is that Resource is a streaming interface. Once it\u0027s read you need to open it again. Let\u0027s show it in an example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(\u0027capital-3.csv\u0027) resource.open() pprint(resource.read_rows()) pprint(resource.read_rows()) # We need to re-open: there is no data left resource.open() pprint(resource.read_rows()) # We need to close manually: not context manager is used resource.close() ``` At the same you can read data for a resource without opening and closing it explicitly. In this case Frictionless Framework will open and close the resource for you so it will be basically a one-time operation: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027capital-3.csv\u0027) pprint(resource.read_rows()) ``` ## Reading Data The Resource class is also a metadata class which provides various read and stream functions. The `extract` functions always read rows into memory; Resource can do the same but it also gives a choice regarding output data. It can be `rows`, `data`, `text`, or `bytes`. Let\u0027s try reading all of them: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027country-3.csv\u0027) pprint(resource.read_bytes()) pprint(resource.read_text()) pprint(resource.read_cells()) pprint(resource.read_rows()) ``` It\u0027s really handy to read all your data into memory but it\u0027s not always possible if a file is really big. For such cases, Frictionless provides streaming functions: ```python script tabs=Python from frictionless import Resource with Resource(\u0027country-3.csv\u0027) as resource: pprint(resource.byte_stream) pprint(resource.text_stream) pprint(resource.cell_stream) pprint(resource.row_stream) for row in resource.row_stream: print(row) ``` ## Indexing Data ```markdown remark type=warning This functionality has been published in `frictionless@5.5` as a feature preview and request for comments. The implementation is raw and doesn\u0027t cover many edge cases. ``` Indexing resource in Frictionless terms means loading a data table into a database. Let\u0027s explore how this feature works in different modes. \u003e All the example are written for SQLite for simplicity ### Normal Mode This mode is supported for any database that is supported by `sqlalchemy`. Under the hood, Frictionless will infer Table Schema and populate the data table as it normally reads data. It means that type errors will be replaced by `null` values and in-general it guarantees to finish successfully for any data even very invalid. ```bash script tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table frictionless extract sqlite:///index/project.db --table table --json ``` ```python script tabs=Python import sqlite3 from frictionless import Resource, formats resource = Resource(\u0027table.csv\u0027) resource.index(\u0027sqlite:///index/project.db\u0027, name=\u0027table\u0027) print(Resource(\u0027sqlite:///index/project.db\u0027, control=formats.sql.SqlControl(table=\u0027table\u0027)).extract()) ``` ### Fast Mode ```markdown remark type=warning For the SQLite in fast mode, it requires `sqlite3@3.34+` command to be available. ``` Fast mode is supported for SQLite and Postgresql databases. It will infer Table Schema using a data sample and index data using `COPY` in Potgresql and `.import` in SQLite. For big data files this mode will be 10-30x faster than normal indexing but the speed comes with the price -- if there is invalid data the indexing will fail. ```bash script tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table --fast frictionless extract sqlite:///index/project.db --table table --json ``` ```python script tabs=Python import sqlite3 from frictionless import Resource, formats resource = Resource(\u0027table.csv\u0027) resource.index(\u0027sqlite:///index/project.db\u0027, name=\u0027table\u0027, fast=True) print(Resource(\u0027sqlite:///index/project.db\u0027, control=formats.sql.SqlControl(table=\u0027table\u0027)).extract()) ``` #### Solution 1: Fallback To ensure that the data will be successfully indexed it\u0027s possible to use `fallback` option. If the fast indexing fails Frictionless will start over in normal mode and finish the process successfully. ```bash tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table --fast --fallback ``` ```python tabs=Python import sqlite3 from frictionless import Resource, formats resource = Resource(\u0027table.csv\u0027) resource.index(\u0027sqlite:///index/project.db\u0027, name=\u0027table\u0027, fast=True, fallback=True) ``` #### Solution 2: QSV Another option is to provide a path to [QSV](https://github.com/jqnatividad/qsv) binary. In this case, initial schema inferring will be done based on the whole data file and will guarantee that the table is valid type-wise: ```bash tabs=CLI frictionless index table.csv --database sqlite:///index/project.db --name table --fast --qsv qsv_path ``` ```python tabs=Python import sqlite3 from frictionless import Resource, formats resource = Resource(\u0027table.csv\u0027) resource.index(\u0027sqlite:///index/project.db\u0027, name=\u0027table\u0027, fast=True, qsv_path=\u0027qsv_path\u0027) ``` ## Scheme The scheme also know as protocol indicates which loader Frictionless should use to read or write data. It can be `file` (default), `text`, `http`, `https`, `s3`, and others. ```python script tabs=Python from frictionless import Resource with Resource(b\u0027header1,header2\\nvalue1,value2\u0027, format=\u0027csv\u0027) as resource: print(resource.scheme) print(resource.to_view()) ``` ## Format The format or as it\u0027s also called extension helps Frictionless to choose a proper parser to handle the file. Popular formats are `csv`, `xlsx`, `json` and others ```python script tabs=Python from frictionless import Resource with Resource(b\u0027header1,header2\\nvalue1,value2.csv\u0027, format=\u0027csv\u0027) as resource: print(resource.format) print(resource.to_view()) ``` ## Encoding Frictionless automatically detects encoding of files but sometimes it can be inaccurate. It\u0027s possible to provide an encoding manually: ```python script tabs=Python from frictionless import Resource with Resource(\u0027country-3.csv\u0027, encoding=\u0027utf-8\u0027) as resource: print(resource.encoding) print(resource.path) ``` ``` utf-8 data/country-3.csv ``` ## Innerpath By default, Frictionless uses the first file found in a zip archive. It\u0027s possible to adjust this behaviour: ```python script tabs=Python from frictionless import Resource with Resource(\u0027table-multiple-files.zip\u0027, innerpath=\u0027table-reverse.csv\u0027) as resource: print(resource.compression) print(resource.innerpath) print(resource.to_view()) ``` ## Compression It\u0027s possible to adjust compression detection by providing the algorithm explicitly. For the example below it\u0027s not required as it would be detected anyway: ```python script tabs=Python from frictionless import Resource with Resource(\u0027table.csv.zip\u0027, compression=\u0027zip\u0027) as resource: print(resource.compression) print(resource.to_view()) ``` ## Dialect Please read [Table Dialect Guide](dialect.html) for more information. ## Schema Please read [Table Schema Guide](schema.html) for more information. ## Checklist Please read [Checklist Guide](checklist.html) for more information. ## Pipeline Please read [Pipeline Guide](pipeline.html) for more information. ## Stats Resource\u0027s stats can be accessed with `resource.stats`: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027table.csv\u0027) resource.infer(stats=True) print(resource.stats) ``` ## Reference ```yaml reference references: - frictionless.Resource ```",
      },
          'docs/framework/dialect': {
          'name': 'Dialect Class',
          'path': 'docs/framework/dialect',
          'relpath': '../framework/dialect',
          'text': "# Dialect Class The Table Dialect is a core Frictionless Data concept meaning a metadata information regarding tabular data source. The Table Dialect concept give us an ability to manage table header and any details related to specific formats. ## Dialect The Dialect class instance are accepted by many classes and functions: - Resource - describe - extract - validate - and more You just need to create a Dialect instance using desired options and pass to the classed and function from above. We will show it on this examplar table: ```bash script tabs=CLI cat capital-3.csv ``` ## Header It\u0027s a boolean flag which defaults to `True` indicating whether the data has a header row or not. In the following example the header row will be treated as a data row: ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(header=False) with Resource(\u0027capital-3.csv\u0027, dialect=dialect) as resource: print(resource.header.labels) print(resource.to_view()) ``` ## Header Rows If header is `True` which is default, this parameters indicates where to find the header row or header rows for a multiline header. Let\u0027s see on example how the first two data rows can be treated as a part of a header: ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(header_rows=[1, 2, 3]) with Resource(\u0027capital-3.csv\u0027, dialect=dialect) as resource: print(resource.header) print(resource.to_view()) ``` ## Header Join If there are multiple header rows which is managed by `header_rows` parameter, we can set a string to be a separator for a header\u0027s cell join operation. Usually it\u0027s very handy for some \"fancy\" Excel files. For the sake of simplicity, we will show on a CSV file: ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(header_rows=[1, 2, 3], header_join=\u0027/\u0027) with Resource(\u0027capital-3.csv\u0027, dialect=dialect) as resource: print(resource.header) print(resource.to_view()) ``` ## Header Case By default a header is validated in a case sensitive mode. To disable this behaviour we can set the `header_case` parameter to `False`. This option is accepted by any Dialect and a dialect can be passed to `extract`, `validate` and other functions. Please note that it doesn\u0027t affect a resulting header it only affects how it\u0027s validated: ```python script tabs=Python from frictionless import Resource, Schema, Dialect, fields dialect = Dialect(header_case=False) schema = Schema(fields=[fields.StringField(name=\"ID\"), fields.StringField(name=\"NAME\")]) with Resource(\u0027capital-3.csv\u0027, dialect=dialect, schema=schema) as resource: print(f\u0027Header: {resource.header}\u0027) print(f\u0027Valid: {resource.header.valid}\u0027) # without \"header_case\" it will have 2 errors ``` ## Comment Char Specifies char used to comment the rows: ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(comment_char=\"#\") with Resource(b\u0027name\\n#row1\\nrow2\u0027, format=\"csv\", dialect=dialect) as resource: print(resource.read_rows()) ``` ## Comment Rows A list of rows to ignore: ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(comment_rows=[2]) with Resource(b\u0027name\\nrow1\\nrow2\u0027, format=\"csv\", dialect=dialect) as resource: print(resource.read_rows()) ``` ## Skip Blank Rows Ignores rows if they are completely blank. ```python script tabs=Python from frictionless import Resource, Dialect dialect = Dialect(skip_blank_rows=True) with Resource(b\u0027name\\n\\nrow2\u0027, format=\"csv\", dialect=dialect) as resource: print(resource.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.Dialect - frictionless.Control ```",
      },
          'docs/framework/schema': {
          'name': 'Schema Class',
          'path': 'docs/framework/schema',
          'relpath': '../framework/schema',
          'text': "# Schema Class The Table Schema is a core Frictionless Data concept meaning a metadata information regarding tabular data source. You can read [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/) for more information. ## Creating Schema Let\u0027s create a table schema: ```python script tabs=Python from frictionless import Schema, fields, describe schema = describe(\u0027table.csv\u0027, type=\u0027schema\u0027) # from a resource path schema = Schema.from_descriptor(\u0027schema.json\u0027) # from a descriptor path schema = Schema.from_descriptor({\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}]}) # from a descriptor ``` As you can see it\u0027s possible to create a schema providing different kinds of sources which will be detector to have some type automatically (e.g. whether it\u0027s a dict or a path). It\u0027s possible to make this step more explicit: ```python script tabs=Python from frictionless import Schema, Field schema = Schema(fields=[fields.StringField(name=\u0027id\u0027)]) # from fields schema = Schema.from_descriptor(\u0027schema.json\u0027) # from a descriptor ``` ## Describing Schema The standard support some additional schema\u0027s metadata: ```python script tabs=Python from frictionless import Schema, fields schema = Schema( fields=[fields.StringField(name=\u0027id\u0027)], missing_values=[\u0027na\u0027], primary_key=[\u0027id\u0027], # foreign_keys ) print(schema) ``` If you have created a schema, for example, from a descriptor you can access this properties: ```python script tabs=Python from frictionless import Schema schema = Schema.from_descriptor(\u0027schema.json\u0027) print(schema.missing_values) # and others ``` And edit them: ```python script tabs=Python from frictionless import Schema schema = Schema.from_descriptor(\u0027schema.json\u0027) schema.missing_values.append(\u0027-\u0027) # and others print(schema) ``` ## Field Management The Schema class provides useful methods to manage fields: ```python script tabs=Python from frictionless import Schema, fields schema = Schema.from_descriptor(\u0027schema.json\u0027) print(schema.fields) print(schema.field_names) schema.add_field(fields.StringField(name=\u0027new-name\u0027)) field = schema.get_field(\u0027new-name\u0027) print(schema.has_field(\u0027new-name\u0027)) schema.remove_field(\u0027new-name\u0027) ``` ## Saving Descriptor As any of the Metadata classes the Schema class can be saved as JSON or YAML: ```python tabs=Python from frictionless import Schema, fields schema = Schema(fields=[fields.IntegerField(name=\u0027id\u0027)]) schema.to_json(\u0027schema.json\u0027) # Save as JSON schema.to_yaml(\u0027schema.yaml\u0027) # Save as YAML ``` ## Reading Cells During the process of data reading a resource uses a schema to convert data: ```python script tabs=Python from frictionless import Schema, fields schema = Schema(fields=[fields.IntegerField(name=\u0027integer\u0027), fields.StringField(name=\u0027string\u0027)]) cells, notes = schema.read_cells([\u00273\u0027, \u0027value\u0027]) print(cells) ``` ## Writing Cells During the process of data writing a resource uses a schema to convert data: ```python script tabs=Python from frictionless import Schema, fields schema = Schema(fields=[fields.IntegerField(name=\u0027integer\u0027), fields.StringField(name=\u0027string\u0027)]) cells, notes = schema.write_cells([3, \u0027value\u0027]) print(cells) ``` ## Creating Field Let\u0027s create a field: ```python script tabs=Python from frictionless import fields field = fields.IntegerField(name=\u0027name\u0027) print(field) ``` Usually we work with fields which were already created by a schema: ```python script tabs=Python from frictionless import describe resource = describe(\u0027table.csv\u0027) field = resource.schema.get_field(\u0027id\u0027) print(field) ``` ## Field Types Frictionless Framework supports all the [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#types-and-formats) field types along with an ability to create custom types. For some types there are additional properties available: ```python script tabs=Python from frictionless import describe resource = describe(\u0027table.csv\u0027) field = resource.schema.get_field(\u0027id\u0027) # it\u0027s an integer print(field.bare_number) ``` See the complete reference at [Tabular Fields](../fields/any.html). ## Reading Cell During the process of data reading a schema uses a field internally. If needed a user can convert their data using this interface: ```python script tabs=Python from frictionless import fields field = fields.IntegerField(name=\u0027name\u0027) cell, note = field.read_cell(\u00273\u0027) print(cell) ``` ## Writing Cell During the process of data writing a schema uses a field internally. The same as with reading a user can convert their data using this interface: ```python script tabs=Python from frictionless import fields field = fields.IntegerField(name=\u0027name\u0027) cell, note = field.write_cell(3) print(cell) ``` ## Reference ```yaml reference references: - frictionless.Schema - frictionless.Field ```",
      },
          'docs/framework/checklist': {
          'name': 'Checklist Class',
          'path': 'docs/framework/checklist',
          'relpath': '../framework/checklist',
          'text': "# Checklist Class ## Creating Checklist Checklist is a set of validation checks and a few addition settings. Let\u0027s create a checklist: ```python script tabs=Python from frictionless import Checklist, checks checklist = Checklist(checks=[checks.row_constraint(formula=\u0027id \u003e 1\u0027)]) print(checklist) ``` ## Validation Checks The Check concept is a part of the Validation API. You can create a custom Check to be used as part of resource or package validation. ```python title=\"Python\" from frictionless import Check, errors class duplicate_row(Check): code = \"duplicate-row\" Errors = [errors.DuplicateRowError] def __init__(self, descriptor=None): super().__init__(descriptor) self.__memory = {} def validate_row(self, row): text = \",\".join(map(str, row.values())) hash = hashlib.sha256(text.encode(\"utf-8\")).hexdigest() match = self.__memory.get(hash) if match: note = \u0027the same as row at position \"%s\"\u0027 % match yield errors.DuplicateRowError.from_row(row, note=note) self.__memory[hash] = row.row_position # Metadata metadata_profile = { # type: ignore \"type\": \"object\", \"properties\": {}, } ``` It\u0027s usual to create a custom [Error](../../docs/framework/classes.html) along side with a Custom Check. ## Reference ```yaml reference references: - frictionless.Checklist - frictionless.Check ```",
      },
          'docs/framework/pipeline': {
          'name': 'Pipeline Class',
          'path': 'docs/framework/pipeline',
          'relpath': '../framework/pipeline',
          'text': "# Pipeline Class Pipeline is an object containing a list of transformation steps. ## Creating Pipeline Let\u0027s create a pipeline using Python interface: ```python script tabs=Python from frictionless import Pipeline, transform, steps pipeline = Pipeline(steps=[steps.table_normalize(), steps.table_melt(field_name=\u0027name\u0027)]) print(pipeline) ``` ## Running Pipeline To run a pipeline you need to use a transform function or method: ```python script tabs=Python from frictionless import Pipeline, transform, steps pipeline = Pipeline(steps=[steps.table_normalize(), steps.table_melt(field_name=\u0027name\u0027)]) resource = transform(\u0027table.csv\u0027, pipeline=pipeline) print(resource.schema) print(resource.read_rows()) ``` ## Transform Steps The Step concept is a part of the Transform API. You can create a custom Step to be used as part of resource or package transformation. \u003e This step uses PETL under the hood. ```python title=\"Python\" from frictionless import Step class cell_set(Step): code = \"cell-set\" def __init__(self, descriptor=None, *, value=None, field_name=None): self.setinitial(\"value\", value) self.setinitial(\"fieldName\", field_name) super().__init__(descriptor) def transform_resource(self, resource): value = self.get(\"value\") field_name = self.get(\"fieldName\") yield from resource.to_petl().update(field_name, value) ``` ## Reference ```yaml reference references: - frictionless.Pipeline - frictionless.Step ```",
      },
          'docs/framework/detector': {
          'name': 'Detector Class',
          'path': 'docs/framework/detector',
          'relpath': '../framework/detector',
          'text': "# Detector Class The Detector object can be used in various places within the Framework. The main purpose of this class is to tweak how different aspects of metadata are detected. Here is a quick example: ```bash script tabs=CLI frictionless extract table.csv --field-missing-values 1,2 ``` ```python script tabs=Python from frictionless import Detector, Resource detector = Detector(field_missing_values=[\u00271\u0027, \u00272\u0027]) resource = Resource(\u0027table.csv\u0027, detector=detector) print(resource.read_rows()) ``` Many options below have their CLI equivalent. Please consult with the CLI help. ## Detector Usage The detector class instance are accepted by many classes and functions: - Package - Resource - describe - extract - validate - and more You just need to create a Detector instance using desired options and pass to the classed and function from above. ## Buffer Size By default, Frictionless will use the first 10000 bytes to detect encoding. Including more bytes by increasing buffer_size can improve the inference. However, it will be slower, but the encoding detection will be more accurate. ```python script tabs=Python from frictionless import Detector, describe detector = Detector(buffer_size=100000) resource = describe(\"country-1.csv\", detector=detector) print(resource.encoding) ``` ## Sample Size By default, Frictionless will use the first 100 rows to detect field types. Including more samples by increasing sample_size can improve the inference. However, it will be slower, but the result will be more accurate. ```python script tabs=Python from frictionless import Detector, describe detector = Detector(sample_size=1000) resource = describe(\"country-1.csv\", detector=detector) print(resource.schema) ``` ## Encoding Function By default, Frictionless encoding_function is None and user can use built in encoding functions. But user has option to implement their own encoding using this feature. The following example simply returns utf-8 encoding but user can add more complex logic to the encoding function. ```python script tabs=Python from frictionless import Detector, Resource detector = Detector(encoding_function=lambda sample: \"utf-8\") with Resource(\"table.csv\", detector=detector) as resource: print(resource.encoding) ``` ## Field Type This option allows manually setting all the field types to a given type. It\u0027s useful when you need to skip data casting (setting `any` type) or have everything as a string (setting `string` type): ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_type=\u0027string\u0027) resource = describe(\"country-1.csv\", detector=detector) print(resource.schema) ``` ## Field Names Sometimes you don\u0027t want to use existent header row to compose field names. It\u0027s possible to provide custom names: ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_names=[\"f1\", \"f2\", \"f3\", \"f4\"]) resource = describe(\"country-1.csv\", detector=detector) print(resource.schema.field_names) ``` ## Field Confidence By default, Frictionless uses 0.9 (90%) confidence level for data types detection. It means that it there are 9 integers in a field and one string it will be inferred as an integer. If you want a guarantee that an inferred schema will conform to the data you can set it to 1 (100%): ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_confidence=1) resource = describe(\"country-1.csv\", detector=detector) print(resource.schema) ``` ## Field Float Numbers By default, Frictionless will consider that all non integer numbers are decimals. It\u0027s possible to make them float which is a faster data type: ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_float_numbers=True) resource = describe(\"floats.csv\", detector=detector) print(resource.schema) print(resource.read_rows()) ``` ## Field Missing Values Missing Values is an important concept in data description. It provides information about what cell values should be considered as nulls. We can customize the defaults: ```python script tabs=Python from frictionless import Detector, describe detector = Detector(field_missing_values=[\"\", \"1\", \"2\"]) resource = describe(\"table.csv\", detector=detector) print(resource.schema.missing_values) print(resource.read_rows()) ``` As we can see, the textual values equal to \"67\" are now considered nulls. Usually, it\u0027s handy when you have data with values like: \u0027-\u0027, \u0027n/a\u0027, and similar. ## Schema Sync There is a way to sync provided schema based on a header row\u0027s field order. It\u0027s very useful when you have a schema that describes a subset or a superset of the resource\u0027s fields: ```python script tabs=Python from frictionless import Detector, Resource, Schema, fields # Note the order of the fields detector = Detector(schema_sync=True) schema = Schema(fields=[fields.StringField(name=\u0027name\u0027), fields.IntegerField(name=\u0027id\u0027)]) with Resource(\u0027table.csv\u0027, schema=schema, detector=detector) as resource: print(resource.schema) print(resource.read_rows()) ``` ## Schema Patch Sometimes we just want to update only a few fields or some schema\u0027s properties without providing a brand new schema. For example, the two examples above can be simplified as: ```python script tabs=Python from frictionless import Detector, Resource detector = Detector(schema_patch={\u0027fields\u0027: {\u0027id\u0027: {\u0027type\u0027: \u0027string\u0027}}}) with Resource(\u0027table.csv\u0027, detector=detector) as resource: print(resource.schema) print(resource.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.Detector ```",
      },
          'docs/framework/inquiry': {
          'name': 'Inquiry Class',
          'path': 'docs/framework/inquiry',
          'relpath': '../framework/inquiry',
          'text': "# Inquiry Class The Inquiry gives you an ability to create arbitrary validation jobs containing a set of individual validation tasks. ## Creating Inquiry Let\u0027s create an inquiry that includes an individual file validation and a resource validation: ```python script tabs=Python from frictionless import Inquiry inquiry = Inquiry.from_descriptor({\u0027tasks\u0027: [ {\u0027path\u0027: \u0027capital-valid.csv\u0027}, {\u0027path\u0027: \u0027capital-invalid.csv\u0027}, ]}) inquiry.to_yaml(\u0027capital.inquiry-example.yaml\u0027) print(inquiry) ``` ## Validating Inquiry Tasks in the Inquiry accept the same arguments written in camelCase as the corresponding `validate` functions have. As usual, let\u0027 run validation: ```bash script tabs=CLI frictionless validate capital.inquiry-example.yaml ``` At first sight, it\u0027s no clear why such a construct exists but when your validation workflow gets complex, the Inquiry can provide a lot of flexibility and power. Last but not least, the Inquiry will use multiprocessing if there are more than 1 task provided. ## Reference ```yaml reference references: - frictionless.Inquiry - frictionless.InquiryTask ```",
      },
          'docs/framework/report': {
          'name': 'Report Class',
          'path': 'docs/framework/report',
          'relpath': '../framework/report',
          'text': "# Report Class ## Validation Report All the `validate` functions return the Validation Report. It\u0027s an unified object containing information about a validation: source details, found error, etc. Let\u0027s explore a report: ```python script tabs=Python from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027, pick_errors=[\u0027duplicate-label\u0027]) print(report) ``` As we can see, there are a lot of information; you can find its details description in \"API Reference\". Errors are grouped by tables; for some validation there are can be dozens of tables. Let\u0027s use the `report.flatten` function to simplify errors representation: ```python script tabs=Python from pprint import pprint from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027, pick_errors=[\u0027duplicate-label\u0027]) pprint(report.flatten([\u0027rowNumber\u0027, \u0027fieldNumber\u0027, \u0027code\u0027, \u0027message\u0027])) ``` In some situation, an error can\u0027t be associated with a table; then it goes to the top-level `report.errors` property: ```python script tabs=Python from frictionless import validate report = validate(\u0027bad.json\u0027, type=\u0027schema\u0027) print(report) ``` ## Validation Errors The Error object is at the heart of the validation process. The Report has `report.errors` and `report.tables[].errors` properties that can contain the Error object. Let\u0027s explore it: ```python script tabs=Python from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027, pick_errors=[\u0027duplicate-label\u0027]) error = report.task.error # it\u0027s only available for 1 table / 1 error sitution print(f\u0027Type: \"{error.type}\"\u0027) print(f\u0027Title: \"{error.title}\"\u0027) print(f\u0027Tags: \"{error.tags}\"\u0027) print(f\u0027Note: \"{error.note}\"\u0027) print(f\u0027Message: \"{error.message}\"\u0027) print(f\u0027Description: \"{error.description}\"\u0027) ``` Above, we have listed universal error properties. Depending on the type of an error there can be additional ones. For example, for our `duplicate-label` error: ```python script tabs=Python from frictionless import validate report = validate(\u0027capital-invalid.csv\u0027, pick_errors=[\u0027duplicate-label\u0027]) error = report.task.error # it\u0027s only available for 1 table / 1 error sitution print(error) ``` Please explore \"Errors Reference\" to learn about all the available errors and their properties. ## Reference ```yaml reference references: - frictionless.Report - frictionless.ReportTask ```",
      },
          'docs/framework/table': {
          'name': 'Table Classes',
          'path': 'docs/framework/table',
          'relpath': '../framework/table',
          'text': "# Table Classes ## Table Header After opening a resource you get access to a `resource.header` object which describes the resource in more detail. This is a list of normalized labels but also provides some additional functionality. Let\u0027s take a look: ```python script tabs=Python from frictionless import Resource with Resource(\u0027capital-3.csv\u0027) as resource: print(f\u0027Header: {resource.header}\u0027) print(f\u0027Labels: {resource.header.labels}\u0027) print(f\u0027Fields: {resource.header.fields}\u0027) print(f\u0027Field Names: {resource.header.field_names}\u0027) print(f\u0027Field Numbers: {resource.header.field_numbers}\u0027) print(f\u0027Errors: {resource.header.errors}\u0027) print(f\u0027Valid: {resource.header.valid}\u0027) print(f\u0027As List: {resource.header.to_list()}\u0027) ``` The example above shows a case when a header is valid. For a header that contains errors in its tabular structure, this information can be very useful, revealing discrepancies, duplicates or missing cell information: ```python script tabs=Python from pprint import pprint from frictionless import Resource with Resource([[\u0027name\u0027, \u0027name\u0027], [\u0027value\u0027, \u0027value\u0027]]) as resource: pprint(resource.header.errors) ``` ## Table Row The `extract`, `resource.read_rows()` and other functions return or yield row objects. In Python, this returns a dictionary with the following information. Note: this example uses the [Detector object](/docs/guides/framework/detector-guide), which tweaks how different aspects of metadata are detected. ```python script tabs=Python from frictionless import Resource, Detector detector = Detector(schema_patch={\u0027missingValues\u0027: [\u00271\u0027]}) with Resource(\u0027capital-3.csv\u0027, detector=detector) as resource: for row in resource.row_stream: print(f\u0027Row: {row}\u0027) print(f\u0027Cells: {row.cells}\u0027) print(f\u0027Fields: {row.fields}\u0027) print(f\u0027Field Names: {row.field_names}\u0027) print(f\u0027Value of field \"name\": {row[\"name\"]}\u0027) # accessed as a dict print(f\u0027Row Number: {row.row_number}\u0027) # counted row number starting from 1 print(f\u0027Blank Cells: {row.blank_cells}\u0027) print(f\u0027Error Cells: {row.error_cells}\u0027) print(f\u0027Errors: {row.errors}\u0027) print(f\u0027Valid: {row.valid}\u0027) print(f\u0027As Dict: {row.to_dict(json=False)}\u0027) print(f\u0027As List: {row.to_list(json=True)}\u0027) # JSON compatible data types break ``` As we can see, this output provides a lot of information which is especially useful when a row is not valid. Our row is valid but we demonstrated how it can preserve data about missing values. It also preserves data about all cells that contain errors: ```python script tabs=Python from pprint import pprint from frictionless import Resource with Resource([[\u0027name\u0027], [\u0027value\u0027, \u0027value\u0027]]) as resource: for row in resource.row_stream: pprint(row.errors) ``` ## Reference ```yaml reference references: - frictionless.Header - frictionless.Row ```",
      },
          'docs/framework/error': {
          'name': 'Error Class',
          'path': 'docs/framework/error',
          'relpath': '../framework/error',
          'text': "# Error Class The Error class is a metadata with no behavior. It\u0027s used to describe an error that happened during Framework work or during the validation. To create a custom error you basically just need to fill the required class fields: ```python title=\"Python\" from frictionless import errors class DuplicateRowError(errors.RowError): code = \"duplicate-row\" name = \"Duplicate Row\" tags = [\"#table\", \"#row\", \"#duplicate\"] template = \"Row at position {rowPosition} is duplicated: {note}\" description = \"The row is duplicated.\" ``` ## Reference ```yaml reference references: - frictionless.Error ```",
      },
          'docs/advanced/design': {
          'name': 'Design',
          'path': 'docs/advanced/design',
          'relpath': '../advanced/design',
          'text': "# Design This guides provides a high-level overview of the Frictionless Framework architecture. It will be useful for plugin authors and advanced users. ## Reading Flow Frictionless uses modular approach for its architecture. During reading a data source goes through various subsystems which are selected depending on the data characteristics: ![Reading](../../assets/reading.png)",
      },
          'docs/advanced/system': {
          'name': 'System',
          'path': 'docs/advanced/system',
          'relpath': '../advanced/system',
          'text': "# System ```markdown remark type=danger This feature is currently experimental. The API might change without notice ``` ## System Object The most important undelaying object in the Frictionless Framework is `system`. It\u0027s an singleton object avaialble as `frictionless.system`. ## System Context Using the `system` object a user can alter the execution context. It uses a Python context manager so it can be used in anyway that it\u0027s possible in Python, for example, it can be nested or combined. ### trusted If data or metadata comes from a trusted origin, it\u0027s possible to disable safety checks for paths: ```python with system.use_context(trusted=True): extract(\u0027/path/to/file/is/absolute.csv\u0027) ``` ### onerror To raise warning or errors on data problems, it\u0027s possible to use `onerror` context value. It\u0027s default to `ignore` and can be set to `warn` or `error`: ```python with system.use_context(onerror=\u0027error\u0027): extract(\u0027table-with-error-will-raise-an-exeption.csv\u0027) ``` ### standards By default, the framework uses coming `v2` version of the standards for outputing metadata. It\u0027s possible to alter this behaviour: ```python with system.use_context(standards=\u0027v1\u0027): describe(\u0027metadata-will-be-in-v1.csv\u0027) ``` ### http_session It\u0027s possible to provide a custom `requests.Session`: ```python session = requests.Session() with system.use_context(http_session=session): with Resource(BASEURL % \"data/table.csv\") as resource: assert resource.header == [\"id\", \"name\"] ``` ## System methods This object can be used to instantiate different kind of lower-level as though `Check`, `Step`, or `Field`. Here is a quick example of using the `system` object: ```python tabs=Python from frictionless import Resource, system # Create adapter = system.create_adapter(source, control=control) loader = system.create_loader(resource) parser = system.create_parser(resource) # Detect system.detect_resource(resource) field_candidates = system.detect_field_candidates() # Select Check = system.selectCheck(\u0027type\u0027) Control = system.selectControl(\u0027type\u0027) Error = system.selectError(\u0027type\u0027) Field = system.selectField(\u0027type\u0027) Step = system.selectStep(\u0027type\u0027) ``` As an extension author you might use the `system` object in various cases. For example, take a look at this `MultipartLoader` excerpts: ```python tabs=Python def read_line_stream(self): for number, path in enumerate(self.__path, start=1): resource = Resource(path=path) resource.infer(sample=False) with system.create_loader(resource) as loader: for line_number, line in enumerate(loader.byte_stream, start=1): if not self.__headless and number \u003e 1 and line_number == 1: continue yield line ``` It\u0027s important to understand that creating low-level objects in general is more corect using the `system` object than just classes because it will include all the available plugins in the process. ## Plugin API The Plugin API almost fully follows the system object\u0027s API. So as a plugin author you need to hook into the same methods. For example, let\u0027s take a look at a builtin Csv Plugin: ```python tabs=Python class CsvPlugin(Plugin): \"\"\"Plugin for CSV\"\"\" # Hooks def create_parser(self, resource: Resource): if resource.format in [\"csv\", \"tsv\"]: return CsvParser(resource) def detect_resource(self, resource: Resource): if resource.format in [\"csv\", \"tsv\"]: resource.type = \"table\" resource.mediatype = f\"text/{resource.format}\" def select_Control(self, type: str): if type == \"csv\": return CsvControl ``` ## Reference ```yaml reference references: - frictionless.Adapter - frictionless.Loader - frictionless.Mapper - frictionless.Parser - frictionless.Plugin - frictionless.System ```",
      },
          'docs/advanced/extending': {
          'name': 'Extension',
          'path': 'docs/advanced/extending',
          'relpath': '../advanced/extending',
          'text': "# Extension ```markdown remark type=danger This feature is currently experimental. The API might change without notice ``` Frictionless is built on top of a powerful plugins system which is used internally and allows to extend the framework. ## Creating Plugin To create a plugin you need: - create a module called `frictionless_` available in PYTHONPATH - subclass the Plugin class and override one of the methods above Please consult with [System/Plugin](system.html) for in-detail information about the Plugin interface and how these methods can be implemented. ## Plugin Example Let\u0027s say we\u0027re interested in supporting the `csv2k` format that we have just invented. For simplicity, let\u0027s use a format that is exactly the same with CSV. First of all, we need to create a `frictionless_csv2k` module containing a Plugin implementation and a Parser implementation but we\u0027re going to re-use the CsvParser as our new format is the same: \u003e frictionless_csv2k.py ```python tabs=Python from frictionless import Plugin, system from frictionless.plugins.csv import CsvParser class Csv2kPlugin(Plugin): def create_parser(self, resource): if resource.format == \"csv2k\": return Csv2kParser(resource) class Csv2kParser(CsvParser): pass system.register(\u0027csv2k\u0027, Csv2kPlugin()) ``` Now, we can use our new format in any of the Frictionless functions that accept a table source, for example, `extract` or `Table`: ```python tabs=Python from frictionless import extract rows = extract(\u0027data/table.csv2k\u0027) print(rows) ``` This example is over-simplified to show the high-level mechanics but writing Frictionless Plugins is designed to be easy. For inspiration, you can check the `frictionless/plugins` directory and learn from real-life examples. Also, in the Frictionless codebase there are many `Check`, `Control`, `Dialect`, `Loader`, `Parser`, and `Server` implementations - you can read their code for better understanding of how to write your own subclass or reach out to us for support. ## Reference ```yaml reference references: - frictionless.Plugin ```",
      },
          'docs/resources/file': {
          'name': 'File Resource',
          'path': 'docs/resources/file',
          'relpath': '../resources/file',
          'text': "# File Resource A `file` resource is the most basic one. Actually, every data file can be maked as `file`. For example: ```python script tabs=Python from frictionless.resources import FileResource resource = FileResource(path=\u0027text.txt\u0027) resource.infer(stats=True) print(resource) ```",
      },
          'docs/resources/text': {
          'name': 'Text Resource',
          'path': 'docs/resources/text',
          'relpath': '../resources/text',
          'text': "# Text Resource A `text` resource represents a textual file as a markdown document, for example: ```python script tabs=Python from frictionless.resources import TextResource resource = TextResource(path=\u0027article.md\u0027) resource.infer(stats=True) print(resource) ``` We can read the contents: ```python script tabs=Python from frictionless.resources import TextResource resource = TextResource(path=\u0027article.md\u0027) resource.infer(stats=True) print(resource.read_text()) ```",
      },
          'docs/resources/json': {
          'name': 'Json Resource',
          'path': 'docs/resources/json',
          'relpath': '../resources/json',
          'text': "# Json Resource A `json` resource contains a structured data like JSON or YAML (can be validated with JSONSchema -- under development): ```python script tabs=Python from frictionless.resources import JsonResource resource = JsonResource(path=\u0027data.json\u0027) resource.infer(stats=True) print(resource) ``` We can read the contents: ```python script tabs=Python from frictionless.resources import JsonResource resource = JsonResource(path=\u0027data.json\u0027) resource.infer(stats=True) print(resource.read_data()) ```",
      },
          'docs/resources/table': {
          'name': 'Table Resource',
          'path': 'docs/resources/table',
          'relpath': '../resources/table',
          'text': "# Table Resource A `table` resource contains a tabular data file (can be validated with Table Schema): ```python script tabs=Python from frictionless.resources import TableResource resource = TableResource(path=\u0027table.csv\u0027) resource.infer(stats=True) print(resource) ``` We can read the contents: ```python script tabs=Python from frictionless.resources import TableResource resource = TableResource(path=\u0027table.csv\u0027) resource.infer(stats=True) print(resource.read_rows()) ```",
      },
          'docs/schemes/aws': {
          'name': 'AWS Schemes',
          'path': 'docs/schemes/aws',
          'relpath': '../schemes/aws',
          'text': "# AWS Schemes Frictionless supports reading data from a AWS cloud source. You can read files in any format that is available in your S3 bucket. ```bash tabs=CLI pip install frictionless[aws] pip install \u0027frictionless[aws]\u0027 # for zsh shell ``` ## Reading Data You can read from this source using `Package/Resource`, for example: ```python tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(path=\u0027s3://bucket/table.csv\u0027) pprint(resource.read_rows()) ``` For reading from a private bucket you need to setup AWS creadentials as it\u0027s described in the [Boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#environment-variables). ## Writing Data A similiar approach can be used for writing: ```python tabs=Python from frictionless import Resource resource = Resource(path=\u0027data/table.csv\u0027) resource.write(\u0027s3://bucket/table.csv\u0027) ``` ## Configuration There is a `Control` to configure how Frictionless read files in this storage. For example: ```python tabs=Python from frictionless import Resource from frictionless.plugins.s3 import S3Control resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) resource.write(\u0027table.new.csv\u0027, control=controls.S3Control(endpoint_url=\u0027\u0027)) ``` ## Reference ```yaml reference references: - frictionless.schemes.AwsControl ```",
      },
          'docs/schemes/buffer': {
          'name': 'Buffer Scheme',
          'path': 'docs/schemes/buffer',
          'relpath': '../schemes/buffer',
          'text': "# Buffer Scheme Frictionless supports working with bytes loaded into memory. ## Reading Data You can read Buffer Data using `Package/Resource` API, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(b\u0027id,name\\n1,english\\n2,german\u0027, format=\u0027csv\u0027) pprint(resource.read_rows()) ``` ## Writing Data A similiar approach can be used for writing: ```python script tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(scheme=\u0027buffer\u0027, format=\u0027csv\u0027) print(target) print(target.read_rows()) ```",
      },
          'docs/schemes/local': {
          'name': 'Local Scheme',
          'path': 'docs/schemes/local',
          'relpath': '../schemes/local',
          'text': "# Local Scheme You can read and write files locally with Frictionless. This is a basic functionality of Frictionless. ## Reading Data You can read using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(path=\u0027table.csv\u0027) pprint(resource.read_rows()) ``` ## Writing Data A similiar approach can be used for writing: ```python script tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(\u0027table-output.csv\u0027) print(target) print(target.to_view()) ```",
      },
          'docs/schemes/multipart': {
          'name': 'Multipart Scheme',
          'path': 'docs/schemes/multipart',
          'relpath': '../schemes/multipart',
          'text': "# Multipart Scheme You can read and write files split into chunks with Frictionless. ## Reading Data You can read using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(path=\u0027chunk1.csv\u0027, extrapaths=[\u0027chunk2.csv\u0027]) pprint(resource.read_rows()) ``` ## Writing Data A similiar approach can be used for writing: ```python tabs=Python from frictionless import Resource resource = Resource(path=\u0027table.json\u0027) resource.write(\u0027table{number}.json\u0027, scheme=\"multipart\", control={\"chunkSize\": 1000000}) ``` ## Configuration There is a `Control` to configure how Frictionless reads files using this scheme. For example: ```python tabs=Python from frictionless import Resource from frictionless.plugins.multipart import MultipartControl control = MultipartControl(chunk_size=1000000) resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) resource.write(\u0027table{number}.json\u0027, scheme=\"multipart\", control=control) ``` ## Reference ```yaml reference references: - frictionless.schemes.MultipartControl ```",
      },
          'docs/schemes/remote': {
          'name': 'Remote Scheme',
          'path': 'docs/schemes/remote',
          'relpath': '../schemes/remote',
          'text': "# Remote Scheme You can read files remotely with Frictionless. This is a basic functionality of Frictionless. ## Reading Data You can read using `Package/Resource`, for example: ```python tabs=Python from pprint import pprint from frictionless import Resource path=\u0027https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/table.csv\u0027 resource = Resource(path=path) pprint(resource.read_rows()) ``` ``` [{\u0027id\u0027: 1, \u0027name\u0027: \u0027english\u0027}, {\u0027id\u0027: 2, \u0027name\u0027: \u0027\u4e2d\u56fd\u4eba\u0027}] ``` ## Writing Data A similar approach can be used for writing: ```python tabs=Python from frictionless import Resource resource = Resource(path=\u0027data/table.csv\u0027) resource.write(\u0027https://example.com/data/table.csv\u0027) # will POST the file to the server ``` ## Configuration There is a `Control` to configure remote data, for example: ```python tabs=Python from frictionless import Resource from frictionless.plugins.remote import RemoteControl control = RemoteControl(http_timeout=10) path=\u0027https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/table.csv\u0027 resource = Resource(path=path, control=control) print(resource.to_view()) ``` ``` +----+-----------+ | id | name | +====+===========+ | 1 | \u0027english\u0027 | +----+-----------+ | 2 | \u0027\u4e2d\u56fd\u4eba\u0027 | +----+-----------+ ``` ## Reference ```yaml reference references: - frictionless.schemes.RemoteControl ```",
      },
          'docs/schemes/stream': {
          'name': 'Stream Scheme',
          'path': 'docs/schemes/stream',
          'relpath': '../schemes/stream',
          'text': "# Stream Scheme Frictionless supports using data stored as File-Like objects in Python. ## Reading Data \u003e It\u0027s recommended to open files in byte-mode. If the file is opened in text-mode, Frictionless will try to re-open it in byte-mode. You can read Stream using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource with open(\u0027table.csv\u0027, \u0027rb\u0027) as file: resource = Resource(file, format=\u0027csv\u0027) pprint(resource.read_rows()) ``` ## Writing Data A similiar approach can be used for writing: ```python script tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(scheme=\u0027stream\u0027, format=\u0027csv\u0027) print(target) print(target.to_view()) ```",
      },
          'docs/formats/csv': {
          'name': 'Csv Format',
          'path': 'docs/formats/csv',
          'relpath': '../formats/csv',
          'text': "# Csv Format CSV is a file format which you can you in Frictionless for reading and writing. Arguable it\u0027s the main Open Data format so it\u0027s supported very well in Frictionless. ## Reading Data You can read this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(\u0027table.csv\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python script tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(\u0027table-output.csv\u0027) print(target) print(target.to_view()) ``` ## Configuration There is a control to configure how Frictionless read and write files in this format. For example: ```python script tabs=Python from frictionless import Resource, formats resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) resource.write(\u0027tmp/table.csv\u0027, control=formats.CsvControl(delimiter=\u0027;\u0027)) ``` ## Reference ```yaml reference references: - frictionless.formats.CsvControl ```",
      },
          'docs/formats/erd': {
          'name': 'Erd Format',
          'path': 'docs/formats/erd',
          'relpath': '../formats/erd',
          'text': "# Erd Format ```markdown remark type=warning This documentation page is work-in-progress ``` Frictionless supports exporting a data package as an ER-diagram `dot` file. For example: ```python package = Package(\u0027datapackage.zip\u0027) package.to_er_diagram(path=\u0027erd.dot\u0027) ```",
      },
          'docs/formats/excel': {
          'name': 'Excel Format',
          'path': 'docs/formats/excel',
          'relpath': '../formats/excel',
          'text': "# Excel Format Excel is a very popular tabular data format that usually has `xlsx` (newer) and `xls` (older) file extensions. Frictionless supports Excel files extensively. ```bash tabs=CLI pip install frictionless[excel] pip install \u0027frictionless[excel]\u0027 # for zsh shell ``` ## Reading Data You can read this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(path=\u0027table.xlsx\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(\u0027table-output.xlsx\u0027) print(target) print(target.to_view()) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python tabs=Python from frictionless import Resource, formats resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) resource.write(\u0027table-output-sheet.xls\u0027, control=formats.ExcelControl(sheet=\u0027My Table\u0027)) ``` ## Reference ```yaml reference references: - frictionless.formats.ExcelControl ```",
      },
          'docs/formats/gsheets': {
          'name': 'Gsheets Format',
          'path': 'docs/formats/gsheets',
          'relpath': '../formats/gsheets',
          'text': "# Gsheets Format Frictionless supports parsing Google Sheets data as a file format. ```bash tabs=CLI pip install frictionless[gsheets] pip install \u0027frictionless[gsheets]\u0027 # for zsh shell ``` ## Reading Data You can read from Google Sheets using `Package/Resource`, for example: ```python tabs=Python from pprint import pprint from frictionless import Resource path=\u0027https://docs.google.com/spreadsheets/d/1mHIWnDvW9cALRMq9OdNfRwjAthCUFUOACPp0Lkyl7b4/edit?usp=sharing\u0027 resource = Resource(path=path) pprint(resource.read_rows()) ``` ``` [{\u0027id\u0027: 1, \u0027name\u0027: \u0027english\u0027}, {\u0027id\u0027: 2, \u0027name\u0027: \u0027\u4e2d\u56fd\u4eba\u0027}] ``` ## Writing Data The same is actual for writing: ```python tabs=Python from frictionless import Resource, formats control = formats.GsheetsControl(credentials=\".google.json\") resource = Resource(path=\u0027data/table.csv\u0027) resource.write(\"https://docs.google.com/spreadsheets/d//edit\", control=control}) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python tabs=Python from frictionless import Resource, formats control = formats.GsheetsControl(credentials=\".google.json\") resource = Resource(path=\u0027data/table.csv\u0027) resource.write(\"https://docs.google.com/spreadsheets/d//edit\", control=control) ``` ## Reference ```yaml reference references: - frictionless.formats.GsheetsControl ```",
      },
          'docs/formats/html': {
          'name': 'Html Format',
          'path': 'docs/formats/html',
          'relpath': '../formats/html',
          'text': "# Html Format Frictionless supports parsing HTML format: ```bash tabs=CLI pip install frictionless[html] pip install \u0027frictionless[html]\u0027 # for zsh shell ``` ## Reading Data You can this file format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import resources resource = resources.TableResource(path=\u0027table1.html\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python script tabs=Python from frictionless import Resource, resources source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = resources.TableResource(path=\u0027table-output.html\u0027) source.write(target) print(target) print(target.to_view()) ``` ## Configuration There is a dialect to configure HTML, for example: ```python script tabs=Python from frictionless import Resource, resources, formats control=formats.HtmlControl(selector=\u0027#id\u0027) resource = resources.TableResource(path=\u0027table1.html\u0027, control=control) print(resource.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.formats.HtmlControl ```",
      },
          'docs/formats/inline': {
          'name': 'Inline Format',
          'path': 'docs/formats/inline',
          'relpath': '../formats/inline',
          'text': "# Inline Format Frictionless supports working with Inline Data from memory. ## Reading Data You can read data in this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python script tabs=Python from frictionless import Resource source = Resource(\u0027table.csv\u0027) target = source.write(format=\u0027inline\u0027, datatype=\u0027table\u0027) print(target) print(target.to_view()) ``` ## Configuration There is a dialect to configure this format, for example: ```python script tabs=Python from frictionless import Resource, formats control = formats.InlineControl(keyed=True, keys=[\u0027name\u0027, \u0027id\u0027]) resource = Resource(data=[{\u0027id\u0027: 1, \u0027name\u0027: \u0027english\u0027}, {\u0027id\u0027: 2, \u0027name\u0027: \u0027german\u0027}], control=control) print(resource.to_view()) ``` ## Reference ```yaml reference references: - frictionless.formats.InlineControl ```",
      },
          'docs/formats/json': {
          'name': 'Json Format',
          'path': 'docs/formats/json',
          'relpath': '../formats/json',
          'text': "# Json Format Frictionless supports parsing JSON tables (JSON and JSONL/NDJSON). ```bash tabs=CLI pip install frictionless[json] pip install \u0027frictionless[json]\u0027 # for zsh shell ``` ## Reading Data \u003e We use the `path` argument to ensure that it will not be guessed to be a metadata file You can read this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import resources resource = resources.TableResource(path=\u0027table.json\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python script tabs=Python from frictionless import Resource, resources source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = resources.TableResource(path=\u0027table-output.json\u0027) source.write(target) print(target) print(target.to_view()) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python script tabs=Python from pprint import pprint from frictionless import Resource, resources, formats control=formats.JsonControl(keyed=True) resource = resources.TableResource(path=\u0027table.keyed.json\u0027, control=control) pprint(resource.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.formats.JsonControl ```",
      },
          'docs/formats/jsonschema': {
          'name': 'JsonSchema Format',
          'path': 'docs/formats/jsonschema',
          'relpath': '../formats/jsonschema',
          'text': "# JsonSchema Format ```markdown remark type=warning This documentation page is work-in-progress ``` Frictionless supports importing a JsonSchema profile as a Table Schema. For example: ```python schema = Schema.from_jsonschema(\u0027table.jsonschema\u0027) ```",
      },
          'docs/formats/markdown': {
          'name': 'Markdown Format',
          'path': 'docs/formats/markdown',
          'relpath': '../formats/markdown',
          'text': "# Markdown Format ```markdown remark type=warning This documentation page is work-in-progress ``` Frictionless supports exporting a metadata object as a Markdown document. For example: ```python schema = Schema(\u0027schema.json\u0027) schema.to_markdown(\u0027schema.md\u0027) ```",
      },
          'docs/formats/ods': {
          'name': 'Ods Format',
          'path': 'docs/formats/ods',
          'relpath': '../formats/ods',
          'text': "# Ods Format Frictionless supports ODS parsing. ```bash tabs=CLI pip install frictionless[ods] pip install \u0027frictionless[ods]\u0027 # for zsh shell ``` ## Reading Data You can read this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(path=\u0027table.ods\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python tabs=Python from pprint import pprint from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(\u0027table-output.ods\u0027) pprint(target) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python tabs=Python from frictionless import Resource, formats resource = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) resource.write(\u0027table-output-sheet.ods\u0027, control=formats.OdsControl(sheet=\u0027My Table\u0027)) ``` ## Reference ```yaml reference references: - frictionless.formats.OdsControl ```",
      },
          'docs/formats/pandas': {
          'name': 'Pandas Format',
          'path': 'docs/formats/pandas',
          'relpath': '../formats/pandas',
          'text': "# Pandas Format Frictionless supports reading and writing Pandas dataframes. ```bash tabs=CLI pip install frictionless[pandas] pip install \u0027frictionless[pandas]\u0027 # for zsh shell ``` ## Reading Data You can read a Pandas dataframe: ```python tabs=Python from frictionless import Resource resource = Resource(df) pprint(resource.read_rows()) ``` ## Writing Data You can write a dataset to Pandas: ```python tabs=Python from frictionless import Resource resource = Resource(\u0027table.csv\u0027) df = resource.to_pandas() ```",
      },
          'docs/formats/parquet': {
          'name': 'Parquet Format',
          'path': 'docs/formats/parquet',
          'relpath': '../formats/parquet',
          'text': "# Parquet Format Frictionless supports reading and writing Parquet files. ```bash tabs=CLI pip install frictionless[parquet] pip install \u0027frictionless[parquet]\u0027 # for zsh shell ``` ## Reading Data You can read a Parquet file: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027table.parq\u0027) print(resource.read_rows()) ``` ## Writing Data You can write a dataset to Parquet: ```python script tabs=Python from frictionless import Resource resource = Resource(\u0027table.csv\u0027) target = resource.write(\u0027table-output.parq\u0027) print(target) print(target.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.formats.ParquetControl ```",
      },
          'docs/formats/spss': {
          'name': 'Spss Format',
          'path': 'docs/formats/spss',
          'relpath': '../formats/spss',
          'text': "# Spss Format Frictionless supports reading and writing SPSS files. ```bash tabs=CLI pip install frictionless[spss] pip install \u0027frictionless[spss]\u0027 # for zsh shell ``` ## Reading Data You can read SPSS files: ```python tabs=Python from pprint import pprint from frictionless import Resource resource = Resource(\u0027table.sav\u0027) pprint(resource.read_rows()) ``` ## Writing Data You can write SPSS files: ```python tabs=Python from frictionless import Resource source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = source.write(\u0027table-output.sav\u0027) pprint(target) pprint(target.read_rows()) ```",
      },
          'docs/formats/sql': {
          'name': 'Sql Format',
          'path': 'docs/formats/sql',
          'relpath': '../formats/sql',
          'text': "# Sql Format Frictionless supports reading and writing SQL databases. ## Supported Databases Frictionless Framework in-general support many databases that can be used with `sqlalchemy`. Here is a list of the databases with tested support: ### SQLite \u003e https://www.sqlite.org/index.html It\u0027s a well-tested default database used by Frictionless: ```bash tabs=CLI pip install frictionless[sql] ``` ### PostgreSQL \u003e https://www.postgresql.org/ This database is well-tested and provides the most data types: ```bash tabs=CLI pip install frictionless[postgresql] ``` ### MySQL \u003e https://www.mysql.com/ Another popular databases having been tested with Frictionless: ```bash tabs=CLI pip install frictionless[mysql] ``` ### DuckDB \u003e https://duckdb.org/ DuckDB is a reletively new database and, currently, Frictionless support is experimental: ```bash tabs=CLI pip install frictionless[duckdb] ``` ## Reading Data You can read SQL database: ```python tabs=Python from frictionless import Resource, formats control = SqlControl(table=\"test_table\", basepath=\u0027data\u0027) with Resource(path=\"sqlite:///sqlite.db\", control=control) as resource: print(resource.read_rows()) ``` ## Writing Data You can write SQL databases: ```python tabs=Python from frictionless import Package package = Package(\u0027path/to/datapackage.json\u0027) package.publish(\u0027postgresql://database\u0027) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python tabs=Python from frictionless import Resource, formats control = SqlControl(table=\u0027table\u0027, order_by=\u0027field\u0027, where=\u0027field \u003e 20\u0027) resource = Resource(\u0027postgresql://database\u0027, control=control) ``` ## Reference ```yaml reference references: - frictionless.formats.SqlControl ```",
      },
          'docs/formats/yaml': {
          'name': 'YAML Format',
          'path': 'docs/formats/yaml',
          'relpath': '../formats/yaml',
          'text': "# YAML Format Frictionless supports parsing YAML tables. ## Reading Data \u003e We use the `path` argument to ensure that it will not be guessed to be a metadata file You can read this format using `Package/Resource`, for example: ```python script tabs=Python from pprint import pprint from frictionless import Resource, resources resource = resources.TableResource(path=\u0027table.yaml\u0027) pprint(resource.read_rows()) ``` ## Writing Data The same is actual for writing: ```python script tabs=Python from frictionless import Resource, resources source = Resource(data=[[\u0027id\u0027, \u0027name\u0027], [1, \u0027english\u0027], [2, \u0027german\u0027]]) target = resources.TableResource(path=\u0027table-output.yaml\u0027) source.write(target) print(target) print(target.to_view()) ``` ## Configuration There is a dialect to configure how Frictionless read and write files in this format. For example: ```python script tabs=Python from pprint import pprint from frictionless import Resource, resources, formats control=formats.YamlControl(keyed=True) resource = resources.TableResource(path=\u0027table.keyed.yaml\u0027, control=control) pprint(resource.read_rows()) ``` ## Reference ```yaml reference references: - frictionless.formats.YamlControl ```",
      },
          'docs/formats/zip': {
          'name': 'Zip Format',
          'path': 'docs/formats/zip',
          'relpath': '../formats/zip',
          'text': "# Zip Format ```markdown remark type=warning This documentation page is work-in-progress ``` Frictionless supports zipped resources and reading/publishing data packages as a zip archive. For example: ```python package = Package(\u0027datapackage.zip\u0027) package.publish(\u0027otherpackage.zip\u0027) ```",
      },
          'docs/portals/ckan': {
          'name': 'Ckan Portal',
          'path': 'docs/portals/ckan',
          'relpath': '../portals/ckan',
          'text': "# Ckan Portal With CKAN portal feature you can load and publish packages from a [CKAN](https://ckan.org), an open-source Data Management System. ## Installation To install this plugin you need to do: ```bash tabs=CLI pip install frictionless[ckan] --pre pip install \u0027frictionless[ckan]\u0027 --pre # for zsh shell ``` ## Reading a Package To import a Dataset from a CKAN instance as a Frictionless Package you can do as below: ```python tabs=Python from frictionless.portals import CkanControl from frictionless import Package ckan_control = CkanControl() package = Package(\u0027https://legado.dados.gov.br/dataset/bolsa-familia-pagamentos\u0027, control=ckan_control) ``` Where \u0027https://legado.dados.gov.br/dataset/bolsa-familia-pagamentos\u0027 is the URL for the CKAN dataset. This will download the dataset and all its resources metadata. You can pass parameters to CKAN Control to configure it, like the CKAN instance base URL (`baseurl`) and the dataset that you do want to download (`dataset`): ```python tabs=Python from frictionless.portals import CkanControl from frictionless import Package ckan_control = CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, dataset=\u0027bolsa-familia-pagamentos\u0027) package = Package(control=ckan_control) ``` You don\u0027t need to pass the `dataset` parameter to CkanControl. In the case that you pass only the `baseurl` you can download a package as: ```python tabs=Python from frictionless.portals import CkanControl from frictionless import Package ckan_control = CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027) package = Package(\u0027bolsa-familia-pagamentos\u0027, control=ckan_control) ``` ## Ignoring a Resource Schema In case that the CKAN dataset has a resource containing errors in its schema, you still can load the package passing the parameter `ignore_schema=True` to CKAN Control: ```python tabs=Python from frictionless.portals import CkanControl from frictionless import Package ckan_control = CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, ignore_schema=True) package = Package(\u0027bolsa-familia-pagamentos\u0027, control=ckan_control) ``` This will download the dataset and all its resources, saving the resources\u0027 original schemas on `original_schema`. ## Publishing a package To publish a Package to a CKAN instance you will need an API key from an CKAN\u0027s user that has permission to create datasets. This key can be passed to CKAN Control as the parameter `apikey`. ```python tabs=Python from frictionless.portals import CkanControl from frictionless import Package ckan_control = CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, apikey=\u0027YOUR-SECRET-API-KEY\u0027) package = Package(...) # Create your package package.publish(control=ckan_control) ``` ## Reading a Catalog You can download a list of CKAN datasets using the Catalog. ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027) c = Catalog(control=ckan_control) ``` This will download all datasets from the instance, limited only by the maximum number of datasets returned by the instance CKAN API. If the instance returns only 10 datasets as default, you can request more packages passing the parameter `num_packages`. In the example above if you want to download 1000 datasets you can do as: ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, num_packages=1000) c = Catalog(control=ckan_control) ``` It\u0027s possible that when you are requesting a large number of packages from CKAN, that some of them don\u0027t have a valid Package descriptor according to the specifications. In that case the standard behaviour will be to stop downloading a raise an exception. If you want to ignore individual package errors, you can pass the parameter `ignore_package_errors=True`: ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, ignore_package_errors=True, num_packages=1000) c = Catalog(control=ckan_control) ``` And the output of the command above will be the CKAN datasets ids with errors and the total number of packages returned by your query to the CKAN instance: ``` Error in CKAN dataset 8d60eff7-1a46-42ef-be64-e8979117a378: [package-error] The data package has an error: descriptor is not valid (The data package has an error: property \"contributors[].email\" is not valid \"email\") Error in CKAN dataset 933d7164-8128-4e12-97e6-208bc4935bcb: [package-error] The data package has an error: descriptor is not valid (The data package has an error: property \"contributors[].email\" is not valid \"email\") Error in CKAN dataset 93114fec-01c2-4ef5-8dfe-67da5027d568: [package-error] The data package has an error: descriptor is not valid (The data package has an error: property \"contributors[].email\" is not valid \"email\") (The data package has an error: property \"contributors[].email\" is not valid \"email\") Total number of packages: 13786 ``` You can see in the example above that 1000 packages were download from a total 13786 packages. You can download other packages passing an offset as: ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, ignore_package_erros=True, results_offset=1000) c = Catalog(control=ckan_control) ``` This will download 1000 packages after the the first 1000 packages. ## Fetching the datasets from an Organization or Group To fetch all packages from a organization will can use the CKAN Control parameter `organization_name`. e.g. if you want to fetch all datasets from the organization `https://legado.dados.gov.br/organization/agencia-espacial-brasileira-aeb` you can do as follows: ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, organization_name=\u0027agencia-espacial-brasileira-aeb\u0027) c = Catalog(control=ckan_control) ``` Similarly, if you want to download all datasets from a CKAN Group you can pass the parameter `group_id` to the CKAN Control as: ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, group_id=\u0027ciencia-informacao-e-comunicacao\u0027) c = Catalog(control=ckan_control) ``` ## Using CKAN search You can also fetch only the datasets that are returned by the [CKAN Package Search endpoint](https://docs.ckan.org/en/2.9/api/#ckan.logic.action.get.package_search). You can pass the search parameters as the parameter `search` to CKAN Control. ```python tabs=Python import frictionless from frictionless import portals, Catalog ckan_control = portals.CkanControl(baseurl=\u0027https://legado.dados.gov.br\u0027, search={\u0027q\u0027: \u0027name:bolsa*\u0027}) c = Catalog(control=ckan_control) ``` ## Reference ```yaml reference references: - frictionless.portals.CkanControl ```",
      },
          'docs/portals/github': {
          'name': 'Github Portal',
          'path': 'docs/portals/github',
          'relpath': '../portals/github',
          'text': "# Github Portal Github read and publish feature makes easy to share data between frictionless and the github repositories. All read/write functionalities are the wrapper around PyGithub library which is used under the hood to make connection to github api. ## Installation We need to install github extra dependencies to use this feature: ```bash tabs=CLI pip install frictionless[github] --pre pip install \u0027frictionless[github]\u0027 --pre # for zsh shell ``` ## Reading Package You can read data from a github repository as follows: ```python tabs=Python from frictionless import Package package = Package(\"https://github.com/fdtester/test-repo-with-datapackage-json\") print(package) ``` ``` {\u0027name\u0027: \u0027test-package\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027first-resource\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027scheme\u0027: \u0027file\u0027, \u0027format\u0027: \u0027xls\u0027, \u0027mediatype\u0027: \u0027application/vnd.ms-excel\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027number\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]} ``` To increase the access limit, pass \u0027apikey\u0027 as the param to the reader function as follows: ```python tabs=Python from frictionless import portals, Package control = portals.GithubControl(apikey=apikey) package = Package(\"https://github.com/fdtester/test-repo-with-datapackage-json\", control=control) print(package) ``` The `reader` function can read package from repos with/without data package descriptor. If the repo does not have the descriptor it will create the descriptor with the same name as the repo name. By default, the function reads files of type csv, xlsx and xls but we can set the file types using control parameters. If the repo has a descriptor it simply returns the descriptor as shown above. Once you read the package from the repo, you can then easily access the resources and its data, for example: ```python tabs=Python from frictionless import Package package = Package(\"https://github.com/fdtester/test-repo-with-datapackage-json\") print(package.get_resource(\u0027first-resource\u0027).read_rows()) ``` ``` [{\u0027id\u0027: 1, \u0027name\u0027: \u0027english\u0027}, {\u0027id\u0027: 2, \u0027name\u0027: \u0027\u4e2d\u56fd\u4eba\u0027}] ``` ## Reading Catalog Catalog is a container for the packages. We can read single/multiple repositories from github and create a catalog. ```python tabs=Python from frictionless import portals, Catalog control = portals.GithubControl(search=\"\u0027TestAction: Read\u0027 in:readme\", apikey=apikey) catalog = Catalog( \"https://github.com/fdtester\", control=control ) print(\"Total packages\", len(catalog.packages)) print(catalog.packages[:2]) ``` ``` Total packages 4 [{\u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027data/capitals.csv\u0027, \u0027scheme\u0027: \u0027file\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027csv\u0027: {\u0027skipInitialSpace\u0027: True}}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}, {\u0027name\u0027: \u0027test-repo-jquery\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027country-1\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-jquery/main/country-1.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}]}] ``` To read catalog, we need authenticated user so we have to pass the token as \u0027apikey\u0027 to the function. In the above example we are using search text to filter the repositories to small number. The search field is not mandatory. We can simply use \u0027control\u0027 parameters and get the same result as above, for example: ```python tabs=Python from frictionless import portals, Catalog control = portals.GithubControl(search=\"\u0027TestAction: Read\u0027 in:readme\", user=\"fdtester\", apikey=apikey) catalog = Catalog(control=control) print(\"Total packages\", len(catalog.packages)) print(catalog.packages[:2]) ``` As shown in the example above, we can use different qualifiers to search the repos. The above example searches for all the repos which has \u0027TestAction: Read\u0027 text in readme files. Similary we can use many different qualifiers and combination of those. To get full list of qualifiers you can check the github document [here](https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories). Some examples of the qualifiers: ``` \u2018jquery\u2019 in:name \u2018jquery\u2019 in:name user:name sort:updated-asc \u2018TestAction: Read\u2019 in:readme ``` If we want to read the list of repositories of user \u0027fdtester\u0027 which has \u0027jquery\u0027 in its name then we write search query as follows: ```python tabs=Python from frictionless import portals, Catalog control = portals.GithubControl(apikey=apikey, search=\"user:fdtester jquery in:name\") catalog = Catalog(control=control) print(catalog.packages) ``` ``` [{\u0027name\u0027: \u0027test-repo-jquery\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027country-1\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-jquery/main/country-1.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}]}] ``` There is only one repository having \u0027jquery\u0027 in name for this user\u0027s account, so it returned only one repository. We can also read repositories in defined order using \u0027sort\u0027 param or qualifier. Here we are trying to read the repos with \u0027TestAction: Read\u0027 text in readme file in recently updated order, for example: ```python tabs=Python from frictionless import portals, Catalog control = portals.GithubControl(apikey=apikey, search=\"user:fdtester sort:updated-desc \u0027TestAction: Read\u0027 in:readme\") catalog = Catalog(control=control) for index,package in enumerate(catalog.packages): print(f\"package:{index}\", \"\\n\") print(package) ``` ``` package:0 {\u0027name\u0027: \u0027test-repo-jquery\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027country-1\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-jquery/main/country-1.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}]} package:1 {\u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027data/capitals.csv\u0027, \u0027scheme\u0027: \u0027file\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027csv\u0027: {\u0027skipInitialSpace\u0027: True}}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]} package:2 {\u0027name\u0027: \u0027test-tabulator\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027first-resource\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027number\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}, {\u0027name\u0027: \u0027number-two\u0027, \u0027path\u0027: \u0027table-reverse.csv\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]} ``` ## Publishing Data To write data to the repository, we use `Package.publish` function as follows: ```python tabs=Python from frictionless import portals, Package package = Package(\u00271174/datapackage.json\u0027) control = portals.GithubControl(repo=\"test-new-repo-doc\", name=\u0027FD\u0027, email=email, apikey=apikey) response = package.publish(control=control) print(response) ``` ``` Repository(full_name=\"fdtester/test-new-repo-doc\") ``` We need to mention `name` and `email` explicitly if the user doesn\u0027t have name set in his github account, and if email is private and hidden. Otherwise, it will take these info from the user account. In order to be able to publish/write to respository, we need to have the api token with \u0027repository write\u0027 access. If the package is successfully published, the response is a \u0027Repository\u0027 instance. ## Configuration We can control the behavior of all the above three functions using various params. For example, to read only \u0027csv\u0027 files in package we use the following code: ```python tabs=Python from frictionless import portals, Package control = portals.GithubControl(user=\"fdtester\", formats=[\"csv\"], repo=\"test-repo-without-datapackage\") package = Package(\"https://github.com/fdtester/test-repo-with-datapackage-json\") print(package) ``` ``` {\u0027name\u0027: \u0027test-package\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027first-resource\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027scheme\u0027: \u0027file\u0027, \u0027format\u0027: \u0027xls\u0027, \u0027mediatype\u0027: \u0027application/vnd.ms-excel\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027number\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]} ``` In order to read first page of the search result and create a catalog, we use `per_page` and `page` params as follows: ```python tabs=Python from frictionless import portals, Catalog control = portals.GithubControl(apikey=apikey, search=\"user:fdtester sort:updated-desc \u0027TestAction: Read\u0027 in:readme\", per_page=1, page=1) catalog = Catalog(control=control) ``` ``` [{\u0027name\u0027: \u0027test-repo-jquery\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027country-1\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-jquery/main/country-1.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}]}] ``` Similary, we can also control the write function using params as follows: ``` from frictionless import portals, Package package = Package(\u0027datapackage.json\u0027) control = portals.GithubControl(repo=\"test-repo\", name=\u0027FD Test\u0027, email=\"test@gmail\", apikey=apikey) response = package.publish(control=control) print(response) ``` ``` Repository(full_name=\"fdtester/test-repo\") ``` ## Reference ```yaml reference references: - frictionless.portals.GithubControl ```",
      },
          'docs/portals/zenodo': {
          'name': 'Zenodo Portal',
          'path': 'docs/portals/zenodo',
          'relpath': '../portals/zenodo',
          'text': "# Zenodo Portal Zenodo API makes data sharing between frictionless framework and Zenodo easy. The data from the Zenodo repo can be read from as well as written to zenodo seamlessly. The api uses \u0027zenodopy\u0027 library underneath to communicate with Zenodo REST API. ## Installation We need to install zenodo extra dependencies to use this feature: ```bash tabs=CLI pip install frictionless[zenodo] --pre pip install \u0027frictionless[zenodo]\u0027 --pre # for zsh shell ``` ## Reading Package You can read data from a zenodo repository as follows: ```python tabs=Python from pprint import pprint from frictionless import portals, Package package = Package(\"https://zenodo.org/record/7078768\") package.infer() print(package) ``` ``` {\u0027title\u0027: \u0027Frictionless Data Test Dataset Without Descriptor\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027capitals.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027csv\u0027: {\u0027skipInitialSpace\u0027: True}}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}, {\u0027name\u0027: \u0027table\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027xls\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027application/vnd.ms-excel\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]} ``` To increase the access limit, pass \u0027apikey\u0027 as the param to the reader function as follows: ```python tabs=Python from pprint import pprint from frictionless import portals, Package control = portals.ZenodoControl(apikey=apikey) package = Package(\"https://zenodo.org/record/7078768\", control=control) print(package) ``` The `reader` function can read package from repos with/without data package descriptor. If the repo does not have the descriptor it will create the descriptor with the name same as the repo name as shown in the example above. By default, the function reads files of type csv, xlsx, xls etc which is supported by frictionless framework but we can set the file types using control parameters also. If the repo has a descriptor it simply returns the descriptor as shown below: ```python tabs=Python from pprint import pprint from frictionless import portals, Package package = Package(\"https://zenodo.org/record/7078760\") package.infer() print(package) ``` ``` {\u0027name\u0027: \u0027testing\u0027, \u0027title\u0027: \u0027Frictionless Data Test Dataset\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027data\u0027, \u0027path\u0027: \u0027data.csv\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027string\u0027, \u0027constraints\u0027: {\u0027required\u0027: True}}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027description\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027amount\u0027, \u0027type\u0027: \u0027number\u0027}], \u0027primaryKey\u0027: [\u0027id\u0027]}}, {\u0027name\u0027: \u0027data2\u0027, \u0027path\u0027: \u0027data2.csv\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027parent\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027comment\u0027, \u0027type\u0027: \u0027string\u0027}], \u0027foreignKeys\u0027: [{\u0027fields\u0027: [\u0027parent\u0027], \u0027reference\u0027: {\u0027resource\u0027: \u0027data\u0027, \u0027fields\u0027: [\u0027id\u0027]}}]}}]} ``` Once you read the package from the repo, you can then easily access the resources and its data, for example: ```python tabs=Python from pprint import pprint from frictionless import portals, Package package = Package(\"https://zenodo.org/record/7078760\") pprint(package.get_resource(\u0027data\u0027).read_rows()) ``` ``` [{\u0027amount\u0027: Decimal(\u002710000.5\u0027), \u0027description\u0027: \u0027Taxes we collect\u0027, \u0027id\u0027: \u0027A3001\u0027, \u0027name\u0027: \u0027Taxes\u0027}, {\u0027amount\u0027: Decimal(\u00272000.5\u0027), \u0027description\u0027: \u0027Parking fees we collect\u0027, \u0027id\u0027: \u0027A5032\u0027, \u0027name\u0027: \u0027Parking Fees\u0027}] ``` You can apply any functions available in frictionless framework. Here is an example of applying validation to the package that was read. ```python tabs=Python from pprint import pprint from frictionless import portals, Package package = Package(\"https://zenodo.org/record/7078760\") report = catalog.packages[0].validate() pprint(report) ``` ``` {\u0027valid\u0027: True, \u0027stats\u0027: {\u0027tasks\u0027: 1, \u0027warnings\u0027: 0, \u0027errors\u0027: 0, \u0027seconds\u0027: 0.655}, \u0027warnings\u0027: [], \u0027errors\u0027: [], \u0027tasks\u0027: [{\u0027valid\u0027: True, \u0027name\u0027: \u0027first-http-resource\u0027, \u0027type\u0027: \u0027table\u0027, \u0027place\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-with-datapackage-yaml/master/data/capitals.csv\u0027, \u0027labels\u0027: [\u0027id\u0027, \u0027cid\u0027, \u0027name\u0027], \u0027stats\u0027: {\u0027md5\u0027: \u0027154d822b8c2aa259867067f01c0efee5\u0027, \u0027sha256\u0027: \u00275ec3d8a4d137891f2f19ab9d244cbc2c30a7493f895c6b8af2506d9b229ed6a8\u0027, \u0027bytes\u0027: 76, \u0027fields\u0027: 3, \u0027rows\u0027: 5, \u0027warnings\u0027: 0, \u0027errors\u0027: 0, \u0027seconds\u0027: 0.651}, \u0027warnings\u0027: [], \u0027errors\u0027: []}]} ``` ## Reading Catalog Catalog is a container for the packages. We can read single/multiple repositories from Zenodo repo and create a catalog. ```python tabs=Python from pprint import pprint from frictionless import portals, Catalog control = portals.ZenodoControl(search=\u0027notes:\"TDWD\"\u0027) catalog = Catalog(control=control) catalog.infer() print(\"Total packages\", len(catalog.packages)) print(catalog.packages) ``` ``` Total packages 2 [{\u0027title\u0027: \u0027Frictionless Data Test Dataset Without Descriptor\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027countries\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027countries.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027headerRows\u0027: [2]}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027neighbor_id\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027population\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}, {\u0027title\u0027: \u0027Frictionless Data Test Dataset Without Descriptor\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027capitals.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027csv\u0027: {\u0027skipInitialSpace\u0027: True}}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}, {\u0027name\u0027: \u0027table\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027xls\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027application/vnd.ms-excel\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}] ``` In the above example we are using search text to filter the repositories to reduce the result size to small number. However, the search field is not mandatory. We can simply use \u0027control\u0027 parameters and create the catalog from a single repo, for example: ```python tabs=Python from pprint import pprint from frictionless import portals, Catalog control = portals.ZenodoControl(record=\"7078768\") catalog = Catalog(control=control) catalog.infer() print(\"Total packages\", len(catalog.packages)) print(catalog.packages) ``` ``` Total packages 1 [{\u0027title\u0027: \u0027Frictionless Data Test Dataset Without Descriptor\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027capitals.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027, \u0027dialect\u0027: {\u0027csv\u0027: {\u0027skipInitialSpace\u0027: True}}, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}, {\u0027name\u0027: \u0027table\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027table.xls\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027xls\u0027, \u0027encoding\u0027: \u0027utf-8\u0027, \u0027mediatype\u0027: \u0027application/vnd.ms-excel\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}] ``` As shown in the first catalog example above, we can use different search queries to filter the repos. The above example searches for all the repos which has \u0027notes:\"TDWD\"\u0027 text in readme files. Similary we can use many different queries combining many terms, phrases or field search. To get full list of different queries you can check the zenodo official document [here](https://help.zenodo.org/guides/search). Some examples of the search queries are: ``` \"open science\" title:\"open science\" +description:\"frictionless\" +title:\"Bionomia\" +publication_date:[2022-10-01 TO 2022-11-01] +title:\"frictionless\" ``` We can search for different terms such as \"open science\" and also use \u0027+\u0027 to specify mandatory. If \"+\" is not specified, it will be optional and will apply \u0027OR\u0027 logic to the search. We can also use field search. All the search queries supported by Zenodo Rest API can be used. If we want to read the list of repositories which has term \"+frictionlessdata +science\" then we write search query as follows: ```python tabs=Python from pprint import pprint from frictionless import portals, Catalog control = portals.ZenodoControl(search=\u0027+frictionlessdata +science\u0027) catalog = Catalog(control=control) print(\"Total Packages\", len(catalog.packages)) ``` ``` Total Packages 1 ``` There is only one repository having terms \u0027+frictionlessdata +science\u0027, so it returned only one repository. We can also read repositories in defined order using \u0027sort\u0027 param. Here we are trying to read the repos with \u0027creators.name:\"FD Tester\"\u0027 in recently updated order, for example: ```python tabs=Python from pprint import pprint from frictionless import portals, Catalog catalog = Catalog( control=portals.ZenodoControl( search=\u0027creators.name:\"FD Tester\"\u0027, sort=\"mostrecent\", page=1, size=1, ), ) catalog.infer() ``` ``` [{\u0027name\u0027: \u0027test-repo-resources-with-http-data-csv\u0027, \u0027title\u0027: \u0027Test Write File - Remote\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027first-http-resource\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-with-datapackage-yaml/master/data/capitals.csv\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}] ``` ## Publishing Data To write data to the repository, we use `Package.publish` function as follows: ```python tabs=Python from pprint import pprint from frictionless import portals, Package control = portals.ZenodoControl( metafn=\"data/zenodo/meta.json\", apikey=apikey ) package = Package(\"484/package-to-write/datapackage.json\") deposition_id = package.publish(control=control) print(deposition_id) ``` ``` 1123500 ``` To publish the data, we need to provide metadata for the Zenodo repo which we are sending using \"meta.json\". In order to be able to publish/write to respository, we need to have the api token with \u0027repository write\u0027 access. If the package is successfully published, the deposition_id will be returned as shown in the example above. For testing, we can pass sandbox url using base_url param ```python tabs=Python from pprint import pprint from frictionless import portals, Package control = portals.ZenodoControl( metafn=\"data/zenodo/meta.json\", apikey=apikey_sandbox, base_url=\"https://sandbox.zenodo.org/api/\" ) package = Package(\"484/package-to-write/datapackage.json\") deposition_id = package.publish(control=control) ``` If the metadata file is not provided, then the api will read available data from the package file. Metadata will be generated using title, contributors and description from Package descriptor. ```python tabs=Python from pprint import pprint from frictionless import portals, Package control = portals.ZenodoControl( apikey=apikey_sandbox, base_url=\"https://sandbox.zenodo.org/api/\" ) package = Package(\"484/package-to-write/datapackage.json\") deposition_id = package.publish(control=control) ``` ## Configuration We can control the behavior of all the above three functions using various params. For example, to read only \u0027csv\u0027 files in package we use the following code: ```python tabs=Python from pprint import pprint from frictionless import portals, Package control = portals.ZenodoControl(formats=[\"csv\"], record=\"7078725\", apikey=apikey) package = Package(control=control) print(package) ``` ``` {\u0027name\u0027: \u0027test-repo-without-datapackage\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027capitals\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-without-datapackage/master/data/capitals.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}, {\u0027name\u0027: \u0027countries\u0027, \u0027type\u0027: \u0027table\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-without-datapackage/master/data/countries.csv\u0027, \u0027scheme\u0027: \u0027https\u0027, \u0027format\u0027: \u0027csv\u0027, \u0027mediatype\u0027: \u0027text/csv\u0027}]} ``` In order to read first page of the search result and create a catalog, we use `page` and `size` params as follows: ```python tabs=Python from pprint import pprint from frictionless import portals, Catalog catalog = Catalog( control=portals.ZenodoControl( search=\u0027creators.name\"FD Tester\"\u0027, sort=\"mostrecent\", page=1, size=1, ), ) print(catalog.packages) ``` ``` [{\u0027name\u0027: \u0027test-repo-resources-with-http-data-csv\u0027, \u0027title\u0027: \u0027Test Write File - Remote\u0027, \u0027resources\u0027: [{\u0027name\u0027: \u0027first-http-resource\u0027, \u0027path\u0027: \u0027https://raw.githubusercontent.com/fdtester/test-repo-with-datapackage-yaml/master/data/capitals.csv\u0027, \u0027schema\u0027: {\u0027fields\u0027: [{\u0027name\u0027: \u0027id\u0027, \u0027type\u0027: \u0027integer\u0027}, {\u0027name\u0027: \u0027cid\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027name\u0027, \u0027type\u0027: \u0027string\u0027}]}}]}] ``` ## Reference ```yaml reference references: - frictionless.portals.ZenodoControl ``` ``` ```",
      },
          'docs/checks/baseline': {
          'name': 'Baseline Check',
          'path': 'docs/checks/baseline',
          'relpath': '../checks/baseline',
          'text': "# Baseline Check ## Overview The Baseline Check is always enabled. It makes various small checks that reveal a great deal of tabular errors. You can create an empty `Checklist` to see the baseline check scope: \u003e Download [`capital-invalid.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/main/data/capital-invalid.csv) to reproduce the examples (right-click and \"Save link as\").. ```python script tabs=Python from pprint import pprint from frictionless import Checklist, validate checklist = Checklist() pprint(checklist.scope) report = validate(\u0027capital-invalid.csv\u0027) # we don\u0027t pass the checklist as the empty one is default pprint(report.flatten([\u0027type\u0027, \u0027message\u0027])) ``` The Baseline Check is incorporated into base Frictionless classes as though Resource, Header, and Row. There is no exact order in which those errors are revealed as it\u0027s highly optimized. One should consider the Baseline Check as one unit of validation. ## Reference ```yaml reference references: - frictionless.checks.baseline ```",
      },
          'docs/checks/table': {
          'name': 'Table Checks',
          'path': 'docs/checks/table',
          'relpath': '../checks/table',
          'text': "# Table Checks ## Table Dimensions This check is used to validate if your data has expected dimensions as: exact number of rows , minimum and maximum number of rows, exact number of fields , minimum and maximum number of fields. ### Basic Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = [ [\"row\", \"salary\", \"bonus\"], [2, 1000, 200], [3, 2500, 500], [4, 1300, 500], [5, 5000, 1000], ] report = validate(source, checks=[checks.table_dimensions(num_rows=5)]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Multiple Limits You can also give multiples limits at the same time: ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = [ [\"row\", \"salary\", \"bonus\"], [2, 1000, 200], [3, 2500, 500], [4, 1300, 500], [5, 5000, 1000], ] report = validate(source, checks=[checks.table_dimensions(num_rows=5, num_fields=4)]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Using Declaratively It is possible to use de check declaratively as: ```python script tabs=Python from pprint import pprint from frictionless import Check, validate, checks source = [ [\"row\", \"salary\", \"bonus\"], [2, 1000, 200], [3, 2500, 500], [4, 1300, 500], [5, 5000, 1000], ] check = Check.from_descriptor({\"type\": \"table-dimensions\", \"minFields\": 4, \"maxRows\": 3}) report = validate(source, checks=[check]) pprint(report.flatten([\"type\", \"message\"])) ``` But the table dimensions check arguments `num_rows`, `min_rows`, `max_rows`, `num_fields`, `min_fields`, `max_fields` must be passed in camelCase format as the example above i.e. `numRows`, `minRows`, `maxRows`, `numFields`, `minFields` and `maxFields`. ### Reference ```yaml reference level: 4 references: - frictionless.checks.table_dimensions ```",
      },
          'docs/checks/row': {
          'name': 'Row Checks',
          'path': 'docs/checks/row',
          'relpath': '../checks/row',
          'text': "# Row Checks ## Duplicate Row This checks for duplicate rows. You need to take into account that checking for duplicate rows can lead to high memory consumption on big files. Here is an example. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = b\"header\\nvalue\\nvalue\" report = validate(source, format=\"csv\", checks=[checks.duplicate_row()]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.duplicate_row ``` ## Row Constraint This check is the most powerful one as it uses the external `simpleeval` package allowing you to evaluate arbitrary Python expressions on data rows. Let\u0027s show on an example. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = [ [\"row\", \"salary\", \"bonus\"], [2, 1000, 200], [3, 2500, 500], [4, 1300, 500], [5, 5000, 1000], ] report = validate(source, checks=[checks.row_constraint(formula=\"salary == bonus * 5\")]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.row_constraint ```",
      },
          'docs/checks/cell': {
          'name': 'Cell Checks',
          'path': 'docs/checks/cell',
          'relpath': '../checks/cell',
          'text': "# Cell Checks ## ASCII Value If you want to skip non-ascii characters, this check helps to notify if there are any in data during validation. Here is how we can use this check. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source=[[\"s.no\",\"code\"],[1,\"ss\u00b5\"]] report = validate(source, checks=[checks.ascii_value()]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.ascii_value ``` ## Deviated Cell This check identifies deviated cells from the normal ones. To flag the deviated cell, the check compares the length of the characters in each cell with a threshold value. The threshold value is either 5000 or value calculated using Python\u0027s built-in `statistics` module which is average plus(+) three standard deviation. The exact algorithm can be found [here](https://github.com/frictionlessdata/frictionless-py/blob/main/frictionless/checks/cell/deviated_value.py). For example: ### Example \u003e Download [`issue-1066.csv`](https://raw.githubusercontent.com/frictionlessdata/frictionless-py/master/data/issue-1066.csv) to reproduce the examples (right-click and \"Save link as\").. ```python script tabs=Python from pprint import pprint from frictionless import validate, checks report = validate(\"issue-1066.csv\", checks=[checks.deviated_cell()]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.deviated_cell ``` ## Deviated Value This check uses Python\u0027s built-in `statistics` module to check a field\u0027s data for deviations. By default, deviated values are outside of the average +- three standard deviations. Take a look at the [API Reference](../../docs/checks/cell.html#reference-checks.deviated_value) for more details about available options and default values. The exact algorithm can be found [here](https://github.com/frictionlessdata/frictionless-py/blob/7ae8bae9a9197adbfe443233a6bad8a94e065ece/frictionless/checks/heuristic.py#L94). For example: ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = [[\"temperature\"], [1], [-2], [7], [0], [1], [2], [5], [-4], [1000], [8], [3]] report = validate(source, checks=[checks.deviated_value(field_name=\"temperature\")]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.deviated_value ``` ## Forbidden Value This check ensures that some field doesn\u0027t have any forbidden or denylist values. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = b\u0027header\\nvalue1\\nvalue2\u0027 checks = [checks.forbidden_value(field_name=\u0027header\u0027, values=[\u0027value2\u0027])] report = validate(source, format=\u0027csv\u0027, checks=checks) pprint(report.flatten([\u0027type\u0027, \u0027message\u0027])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.forbidden_value ``` ## Sequential Value This check gives us an opportunity to validate sequential fields like primary keys or other similar data. It doesn\u0027t need to start from 0 or 1. We\u0027re providing a field name. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = b\u0027header\\n2\\n3\\n5\u0027 report = validate(source, format=\u0027csv\u0027, checks=[checks.sequential_value(field_name=\u0027header\u0027)]) pprint(report.flatten([\u0027type\u0027, \u0027message\u0027])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.sequential_value ``` ## Truncated Value Sometime during data export from a database or other storage, data values can be truncated. This check tries to detect such truncation. Let\u0027s explore some truncation indicators. ### Example ```python script tabs=Python from pprint import pprint from frictionless import validate, checks source = [[\"int\", \"str\"], [\"a\" * 255, 32767], [\"good\", 2147483647]] report = validate(source, checks=[checks.truncated_value()]) pprint(report.flatten([\"type\", \"message\"])) ``` ### Reference ```yaml reference level: 4 references: - frictionless.checks.truncated_value ```",
      },
          'docs/steps/resource': {
          'name': 'Resource Steps',
          'path': 'docs/steps/resource',
          'relpath': '../steps/resource',
          'text': "# Resource Steps The Resource steps are only available for a package transformation (except for `steps.resource_update` available for standalone resources). This includes some basic resource management operations like adding or removing resources along with the hierarchical `transform_resource` step. ## Add Resource This step add a new resource to a data package. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Package(resources=[Resource(name=\u0027main\u0027, path=\"transform.csv\")]) target = transform( source, steps=[ steps.resource_add(name=\u0027extra\u0027, descriptor={\u0027path\u0027: \u0027transform.csv\u0027}), ], ) print(target.resource_names) print(target.get_resource(\u0027extra\u0027).schema) print(target.get_resource(\u0027extra\u0027).to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.resource_add ``` ## Remove Resource This step remove an existent resource from a data package. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Package(resources=[Resource(name=\u0027main\u0027, path=\"transform.csv\")]) target = transform( source, steps=[ steps.resource_remove(name=\u0027main\u0027), ], ) print(target) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.resource_remove ``` ## Transform Resource It\u0027s a hierarchical step allowing to transform a data package\u0027s resource. It\u0027s possible to use any resource steps as a part of this package step. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Package(resources=[Resource(name=\u0027main\u0027, path=\"transform.csv\")]) target = transform( source, steps=[ steps.resource_transform(name=\u0027main\u0027, steps=[ steps.row_sort(field_names=[\u0027name\u0027]) ]), ], ) print(target.resource_names) print(target.get_resource(\u0027main\u0027).schema) print(target.get_resource(\u0027main\u0027).to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.resource_transform ``` ## Update Resource This step update a resource\u0027s metadata. It can be used for both resource and package transformations. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Package(resources=[Resource(name=\u0027main\u0027, path=\"transform.csv\")]) target = transform( source, steps=[ steps.resource_update( name=\u0027main\u0027, descriptor={\u0027title\u0027: \u0027Main Resource\u0027, \u0027description\u0027: \u0027For the docs\u0027} ), ], ) print(target.get_resource(\u0027main\u0027)) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.resource_update ```",
      },
          'docs/steps/table': {
          'name': 'Table Steps',
          'path': 'docs/steps/table',
          'relpath': '../steps/table',
          'text': "# Table Steps These steps are meant to be used on a table level of a resource. This includes various different operations from simple validation or writing to the disc to complex re-shaping like pivoting or melting. ## Aggregate Table Group rows under the given group_name then apply aggregation functions provided as aggregation dictionary (see example) ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform-groups.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_aggregate( group_name=\"name\", aggregation={\"sum\": (\"population\", sum)} ), ], ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_aggregate ``` ## Attach Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_attach(resource=Resource(data=[[\"note\"], [\"large\"], [\"mid\"]])), ], ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_attach ``` ## Debug Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_debug(function=print), ], ) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_debug ``` ## Diff Tables ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_diff( resource=Resource( data=[ [\"id\", \"name\", \"population\"], [1, \"germany\", 83], [2, \"france\", 50], [3, \"spain\", 47], ] ) ), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_diff ``` ## Intersect Tables ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_intersect( resource=Resource( data=[ [\"id\", \"name\", \"population\"], [1, \"germany\", 83], [2, \"france\", 50], [3, \"spain\", 47], ] ), ), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_intersect ``` ## Join Tables ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_join( resource=Resource(data=[[\"id\", \"note\"], [1, \"beer\"], [2, \"vine\"]]), field_name=\"id\", ), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_join ``` ## Melt Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_melt(field_name=\"name\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_melt ``` ## Merge Tables ### Example ```markdown remark type=danger This functionality is currently disabled as being fixed in [#1221](https://github.com/frictionlessdata/frictionless-py/issues/1221) ``` ```python tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_merge( resource=Resource(data=[[\"id\", \"name\", \"note\"], [4, \"malta\", \"island\"]]) ), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_merge ``` ## Normalize Table The `table_normalize` step normalizes an underlaying tabular stream (cast types and fix dimensions) according to a provided or inferred schema. If your data is not really big it\u0027s recommended to normalize a table before any others steps. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(\"table.csv\") print(source.read_cells()) target = transform( source, steps=[ steps.table_normalize(), ] ) print(target.read_cells()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_normalize ``` ## Pivot Table ### Example ```markdown remark type=danger This functionality is currently disabled as being fixed in [#1220](https://github.com/frictionlessdata/frictionless-py/issues/1220) ``` ```python tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform-pivot.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_pivot(f1=\"region\", f2=\"gender\", f3=\"units\", aggfun=sum), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_pivot ``` ## Print Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_print(), ] ) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_print ``` ## Recast Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_melt(field_name=\"id\"), steps.table_recast(field_name=\"id\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_recast ``` ## Transpose Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.table_transpose(), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_transpose ``` ## Validate Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_set(field_name=\"population\", value=\"bad\"), steps.table_validate(), ] ) pprint(target.schema) try: pprint(target.to_view()) except Exception as exception: pprint(exception) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_validate ``` ## Write Table ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_write(path=\u0027transform.json\u0027), ] ) ``` Let\u0027s read the output: ```bash script tabs=CLI cat transform.json ``` ```python script tabs=Python with open(\u0027transform.json\u0027) as file: print(file.read()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.table_write ```",
      },
          'docs/steps/field': {
          'name': 'Field Steps',
          'path': 'docs/steps/field',
          'relpath': '../steps/field',
          'text': "# Field Steps The Field steps are responsible for managing a Table Schema\u0027s fields. You can add or remove them along with more complex operations like unpacking. ## Add Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_add(name=\"note\", value=\"eu\", descriptor={\"type\": \"string\"}), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_add ``` ## Filter Fields ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_filter(names=[\"id\", \"name\"]), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_filter ``` ## Merge Fields ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ # seperator argument can be used to set delimeter. Default value is \u0027-\u0027 # preserve argument keeps the original fields steps.field_merge(name=\"details\", from_names=[\"name\", \"population\"], preserve=True) ], ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_merge ``` ## Move Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_move(name=\"id\", position=3), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_move ``` ## Pack Fields ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ # field_type returns packed fields as JSON Object. Default value for field_type is \u0027array\u0027 # preserve argument keeps the original fields steps.field_pack(name=\"details\", from_names=[\"name\", \"population\"], as_object=True, preserve=True) ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_pack ``` ## Remove Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_remove(names=[\"id\"]), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_remove ``` ## Split Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_split(name=\"name\", to_names=[\"name1\", \"name2\"], pattern=\"a\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_split ``` ## Unpack Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_update(name=\"id\", value=[1, 1], descriptor={\"type\": \"string\"}), steps.field_unpack(name=\"id\", to_names=[\"id2\", \"id3\"]), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_unpack ``` ## Update Field ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_update(name=\"id\", value=str, descriptor={\"type\": \"string\"}), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.field_update ```",
      },
          'docs/steps/row': {
          'name': 'Row Steps',
          'path': 'docs/steps/row',
          'relpath': '../steps/row',
          'text': "# Row Steps These steps are row-based including row filtering, slicing, and many more. ## Filter Rows This step filters rows based on a provided formula or function. ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.row_filter(formula=\"id \u003e 1\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_filter ``` ## Search Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.row_search(regex=r\"^f.*\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_search ``` ## Slice Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.row_slice(head=2), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_slice ``` ## Sort Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.row_sort(field_names=[\"name\"]), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_sort ``` ## Split Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.row_split(field_name=\"name\", pattern=\"a\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_split ``` ## Subset Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.field_update(name=\"id\", value=1), steps.row_subset(subset=\"conflicts\", field_name=\"id\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_subset ``` ## Ungroup Rows ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform-groups.csv\") target = transform( source, steps=[ steps.row_ungroup(group_name=\"name\", selection=\"first\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.row_ungroup ```",
      },
          'docs/steps/cell': {
          'name': 'Cell Steps',
          'path': 'docs/steps/cell',
          'relpath': '../steps/cell',
          'text': "# Cell Steps The Cell steps are responsible for cell operations like converting, replacing, or formating, along with others. ## Convert Cells Converts cell values of one or more fields using arbitrary functions, method invocations or dictionary translations. ### Using Value We can provide a value to be set as a value of all cells of this field. Take into account that the value type needs to conform to the field type otherwise it will lead to a validation error: ```python script tabs=Python from frictionless import Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_convert(field_name=\u0027population\u0027, value=\"100\"), ], ) print(target.to_view()) ``` ### Using Mapping Another option to modify the field\u0027s cell is to provide a mapping. It\u0027s a translation table that uses literal matching to replace values. It\u0027s usually used for string fields: ```python script tabs=Python from frictionless import Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_convert(field_name=\u0027name\u0027, mapping = {\u0027germany\u0027: \u0027GERMANY\u0027}), ], ) print(target.to_view()) ``` ### Using Function ```markdown remark type=info Functions are not supported in declarative pipelines ``` We can provide an arbitrary function to update the field cells. If you want to modify a non-string field it\u0027s really important to normalize the table first otherwise the function will be applied to a non-parsed value: ```python script tabs=Python from frictionless import Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.table_normalize(), steps.cell_convert(field_name=\u0027population\u0027, function=lambda v: v*2), ], ) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.cell_convert ``` ## Fill Cells ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_replace(pattern=\"france\", replace=None), steps.cell_fill(field_name=\"name\", value=\"FRANCE\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.cell_fill ``` ## Format Cells ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_format(template=\"Prefix: {0}\", field_name=\"name\"), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.cell_format ``` ## Interpolate Cells ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_interpolate(template=\"Prefix: %s\", field_name=\"name\"), ] ) pprint(target.schema) pprint(target.read_rows()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.cell_interpolate ``` ## Replace Cells ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_replace(pattern=\"france\", replace=\"FRANCE\"), ] ) print(target.schema) print(target.to_view()) ``` ```yaml reference level: 4 references: - frictionless.steps.cell_replace ``` ## Set Cells ### Example ```python script tabs=Python from pprint import pprint from frictionless import Package, Resource, transform, steps source = Resource(path=\"transform.csv\") target = transform( source, steps=[ steps.cell_set(field_name=\"population\", value=100), ] ) print(target.schema) print(target.to_view()) ``` ### Reference ```yaml reference level: 4 references: - frictionless.steps.cell_set ```",
      },
          'docs/fields/any': {
          'name': 'Any Field',
          'path': 'docs/fields/any',
          'relpath': '../fields/any',
          'text': "# Any Field ## Overview AnyField provides an ability to skip any cell parsing. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#any). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [1], [\u00271\u0027]] rows = extract(data, schema=Schema(fields=[fields.AnyField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.AnyField ```",
      },
          'docs/fields/array': {
          'name': 'Array Field',
          'path': 'docs/fields/array',
          'relpath': '../fields/array',
          'text': "# Array Field ## Overview The field contains data that is a valid JSON format arrays. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#array). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027[\"value1\", \"value2\"]\u0027]] rows = extract(data, schema=Schema(fields=[fields.ArrayField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.ArrayField ```",
      },
          'docs/fields/boolean': {
          'name': 'Boolean Field',
          'path': 'docs/fields/boolean',
          'relpath': '../fields/boolean',
          'text': "# Boolean Field ## Overview The field contains boolean (true/false) data. In the physical representations of data where boolean values are represented with strings, the values set in trueValues and falseValues are to be cast to their logical representation as booleans. trueValues and falseValues are arrays which can be customised to user need. The default values for these are in the additional properties section below. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#boolean). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027true\u0027], [\u0027false\u0027]] rows = extract(data, schema=Schema(fields=[fields.BooleanField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.BooleanField ```",
      },
          'docs/fields/date': {
          'name': 'Date Field',
          'path': 'docs/fields/date',
          'relpath': '../fields/date',
          'text': "# Date Field ## Overview A date without a time (by default in ISO8601 format). Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#date). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00272022-08-22\u0027]] rows = extract(data, schema=Schema(fields=[fields.DateField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.DateField ```",
      },
          'docs/fields/datetime': {
          'name': 'Datetime Field',
          'path': 'docs/fields/datetime',
          'relpath': '../fields/datetime',
          'text': "# Datetime Field ## Overview A date with a time (by default in ISO8601 format). Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#datetime). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00272022-08-22T12:00:00\u0027]] rows = extract(data, schema=Schema(fields=[fields.DatetimeField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.DatetimeField ```",
      },
          'docs/fields/duration': {
          'name': 'Duration Field',
          'path': 'docs/fields/duration',
          'relpath': '../fields/duration',
          'text': "# Duration Field ## Overview A duration of time. We follow the definition of XML Schema duration datatype directly and that definition is implicitly inlined here. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#duration). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027P1Y\u0027]] rows = extract(data, schema=Schema(fields=[fields.DurationField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.DurationField ```",
      },
          'docs/fields/geojson': {
          'name': 'Geojson Field',
          'path': 'docs/fields/geojson',
          'relpath': '../fields/geojson',
          'text': "# Geojson Field The field contains a JSON object according to GeoJSON or TopoJSON spec. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#geojson). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027{\"geometry\": null, \"type\": \"Feature\", \"properties\": {\"k\": \"v\"}}\u0027]] rows = extract(data, schema=Schema(fields=[fields.GeojsonField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.GeojsonField ```",
      },
          'docs/fields/geopoint': {
          'name': 'Geopoint Field',
          'path': 'docs/fields/geopoint',
          'relpath': '../fields/geopoint',
          'text': "# Geopoint Field The field contains data describing a geographic point. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#geopoint). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\"180, -90\"]] rows = extract(data, schema=Schema(fields=[fields.GeopointField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.GeopointField ```",
      },
          'docs/fields/integer': {
          'name': 'Integer Field',
          'path': 'docs/fields/integer',
          'relpath': '../fields/integer',
          'text': "# Integer Field The field contains integers - that is whole numbers. Integer values are indicated in the standard way for any valid integer. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#integer). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00271\u0027], [\u00272\u0027], [\u00273\u0027]] rows = extract(data, schema=Schema(fields=[fields.IntegerField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.IntegerField ```",
      },
          'docs/fields/number': {
          'name': 'Number Field',
          'path': 'docs/fields/number',
          'relpath': '../fields/number',
          'text': "# Number Field ## Overview The field contains numbers of any kind including decimals. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#number). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00271.1\u0027], [\u00272.2\u0027], [\u00273.3\u0027]] rows = extract(data, schema=Schema(fields=[fields.NumberField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.NumberField ```",
      },
          'docs/fields/object': {
          'name': 'Object Field',
          'path': 'docs/fields/object',
          'relpath': '../fields/object',
          'text': "# Object Field ## Overview The field contains data which is valid JSON. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#object). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027{\"key\": \"value\"}\u0027]] rows = extract(data, schema=Schema(fields=[fields.ObjectField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.ObjectField ```",
      },
          'docs/fields/string': {
          'name': 'String Field',
          'path': 'docs/fields/string',
          'relpath': '../fields/string',
          'text': "# String Field ## Overview The field contains strings, that is, sequences of characters. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#string). Currently supported formats: - default - uri - email - uuid - binary - wkt (doesn\u0027t work in Python3.10+) ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u0027value\u0027]] rows = extract(data, schema=Schema(fields=[fields.StringField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.StringField ```",
      },
          'docs/fields/time': {
          'name': 'Time Field',
          'path': 'docs/fields/time',
          'relpath': '../fields/time',
          'text': "# Time Field ## Overview A time without a date. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#time). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u002715:00:00\u0027]] rows = extract(data, schema=Schema(fields=[fields.TimeField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.TimeField ```",
      },
          'docs/fields/year': {
          'name': 'Year Field',
          'path': 'docs/fields/year',
          'relpath': '../fields/year',
          'text': "# Year Field ## Overview A calendar year as per XMLSchema gYear. Usual lexical representation is YYYY. There are no format options. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#year). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00272022\u0027]] rows = extract(data, schema=Schema(fields=[fields.YearField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.YearField ```",
      },
          'docs/fields/yearmonth': {
          'name': 'Yearmonth Field',
          'path': 'docs/fields/yearmonth',
          'relpath': '../fields/yearmonth',
          'text': "# Yearmonth Field ## Overview A specific month in a specific year as per XMLSchema gYearMonth. Usual lexical representation is: YYYY-MM. Read more in [Table Schema Standard](https://specs.frictionlessdata.io/table-schema/#yearmonth). ## Example ```python script tabs=Python from frictionless import Schema, extract, fields data = [[\u0027name\u0027], [\u00272022-08\u0027]] rows = extract(data, schema=Schema(fields=[fields.YearmonthField(name=\u0027name\u0027)])) print(rows) ``` ## Reference ```yaml reference references: - frictionless.fields.YearmonthField ```",
      },
          'docs/errors/metadata': {
          'name': 'Metadata Errors',
          'path': 'docs/errors/metadata',
          'relpath': '../errors/metadata',
          'text': "# Metadata Errors {% set errors = frictionless.platform.frictionless_errors %} {% set exclude = [errors.ResourceError] %} {% for Error in errors.MetadataError.list_children(root=True, exclude=exclude) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.MetadataError ```",
      },
          'docs/errors/resource': {
          'name': 'Resource Errors',
          'path': 'docs/errors/resource',
          'relpath': '../errors/resource',
          'text': "# Resource Errors {% set errors = frictionless.platform.frictionless_errors %} {% for Error in errors.ResourceError.list_children(root=True) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.ResourceError ```",
      },
          'docs/errors/data': {
          'name': 'Data Errors',
          'path': 'docs/errors/data',
          'relpath': '../errors/data',
          'text': "# Data Errors {% set errors = frictionless.platform.frictionless_errors %} {% set exclude = [errors.FileError, errors.TableError] %} {% for Error in errors.DataError.list_children(root=True, exclude=exclude) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.DataError ```",
      },
          'docs/errors/file': {
          'name': 'File Errors',
          'path': 'docs/errors/file',
          'relpath': '../errors/file',
          'text': "# File Errors {% set errors = frictionless.platform.frictionless_errors %} {% for Error in errors.FileError.list_children(root=True) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.FileError ```",
      },
          'docs/errors/table': {
          'name': 'Table Errors',
          'path': 'docs/errors/table',
          'relpath': '../errors/table',
          'text': "# Table Errors {% set errors = frictionless.platform.frictionless_errors %} {% set exclude = [errors.HeaderError, errors.RowError] %} {% for Error in errors.TableError.list_children(root=True, exclude=exclude) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.TableError ```",
      },
          'docs/errors/header': {
          'name': 'Header Errors',
          'path': 'docs/errors/header',
          'relpath': '../errors/header',
          'text': "# Header Errors {% set errors = frictionless.platform.frictionless_errors %} {% set exclude = [errors.LabelError] %} {% for Error in errors.HeaderError.list_children(root=True, exclude=exclude) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.HeaderError ```",
      },
          'docs/errors/label': {
          'name': 'Label Errors',
          'path': 'docs/errors/label',
          'relpath': '../errors/label',
          'text': "# Label Errors {% set errors = frictionless.platform.frictionless_errors %} {% for Error in errors.LabelError.list_children(root=True) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.LabelError ```",
      },
          'docs/errors/row': {
          'name': 'Row Errors',
          'path': 'docs/errors/row',
          'relpath': '../errors/row',
          'text': "# Row Errors {% set errors = frictionless.platform.frictionless_errors %} {% set exclude = [errors.CellError] %} {% for Error in errors.RowError.list_children(root=True, exclude=exclude) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.RowError - frictionless.errors.ForeignKeyError ```",
      },
          'docs/errors/cell': {
          'name': 'Cell Errors',
          'path': 'docs/errors/cell',
          'relpath': '../errors/cell',
          'text': "# Cell Errors {% set errors = frictionless.platform.frictionless_errors %} {% for Error in errors.CellError.list_children(root=True) %} ## {{ Error.title }} | Name | Value | | ----------- | -------------------------- | | Type | {{ Error.type }} | | Title | {{ Error.title }} | | Description | {{ Error.description }} | | Template | {{ Error.template }} | | Tags | {{ Error.tags|join(\u0027 \u0027) }} | {% endfor %} ## Reference ```yaml reference references: - frictionless.errors.CellError ```",
      },
          'docs/codebase/authors': {
          'name': 'Authors',
          'path': 'docs/codebase/authors',
          'relpath': '../codebase/authors',
          'text': "# Authors \u003e This page is powered by [contributors-img](https://contributors-img.web.app) This package is a collective effort made by many great people working on various projects. You can click on the pictures below to see their contribution in detail. ## frictionless-py ## datapackage-py ## tableschema-py ## tableschema-bigquery-py ## tableschema-ckan-datastore-py ## tableschema-elasticsearch-py ## tableschema-pandas-py ## tableschema-sql-py ## tableschema-spss-py ## tabulator-py",
      },
          'docs/codebase/license': {
          'name': 'The MIT License (MIT)',
          'path': 'docs/codebase/license',
          'relpath': '../codebase/license',
          'text': "# The MIT License (MIT) Copyright \u00a9 `2020` `Open Knowledge Foundation` Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
      },
          'docs/codebase/migration': {
          'name': 'Migration',
          'path': 'docs/codebase/migration',
          'relpath': '../codebase/migration',
          'text': "# Migration Frictionless is a logical continuation of many existing packages created for Frictionless Data as though `datapackage` or `tableschema`. Although, most of these packages will be supported going forward, you can migrate to Frictionless, which is Python 3.8+, as it improves many aspects of working with data and metadata. This document also covers migration from one framework\u0027s version to another. ## From v4 to v5 Since the initial Frictionless Framework release we\u0027d been collecting feedback and analyzing both high-level users\u0027 needs and bug reports to identify shortcomings and areas that can be improved in the next version of the framework. Read about a new version of the framework and migration details in this blog: - [Welcome Frictionless Framework (v5)](../../blog/2022/08-22-frictionless-framework-v5.html) ## From dataflows Frictionless Framework provides the `frictionless transform` function for data transformation. It can be used to migrate from `dataflows` or `datapackage-pipelines`: - [Transforming Data](../guides/transforming-data.html) - [Transform Steps](../steps/resource.html) ## From goodtables Frictionless Framework provides the `frictionless validate` function which is in high-level exactly the same as `goodtables validate`. Also `frictionless describe` is an improved version of `goodtables init`. You instead need to use the `frictionless` command instead of the `goodtables` command: - [Validating Data](../guides/validating-data.html) - [Validation Checks](../checks/baseline.html) - [Validation Errors](../errors/metadata.html) ## From datapackage Frictionless Framework has `Package` and `Resource` classes which is almost the same as `datapackage` has: - [Describing Data](../guides/describing-data.html) - [Extracting Data](../guides/extracting-data.html) - [Package Class](../framework/package.html) - [Resource Class](../framework/resource.html) ## From tableschema Frictionless Framework has `Schema` and `Field` classes which is almost the same as `tableschema` has: - [Describing Data](../guides/describing-data.html) - [Extracting Data](../guides/extracting-data.html) - [Schema Class](../framework/schema.html) - [Tabular Fields](../fields/any.html) ## From tabulator Frictionless has `Resource` class which is an equivalent of the tabulator\u0027s `Stream` class: - [Extracting Data](../guides/extracting-data.html) - [Resource Class](../framework/resource.html) - [File Schemes](../schemes/aws.html) - [File Formats](../formats/csv.html)",
      },
          'docs/codebase/changelog': {
          'name': 'Changelog',
          'path': 'docs/codebase/changelog',
          'relpath': '../codebase/changelog',
          'text': "# Changelog Here described only the breaking and most significant changes. The full changelog and documentation for all released versions could be found in nicely formatted [commit history](https://github.com/frictionlessdata/frictionless-py/commits/main). ## v5.18.1 - feat: add pass_row as configurable parameter for field_update ([#1729](https://github.com/frictionlessdata/frictionless-py/pull/1729)) - fix: vendor unmaintained stringcase library ([#1727](https://github.com/frictionlessdata/frictionless-py/pull/1727)) - Various bug fixes ## v5.18 - Support `ignore_constraints` option for the `Indexer` (#1691) - Various bug fixes ## v5.17.1 - fix: deprecated dependencies ([PR 1674](https://github.com/frictionlessdata/frictionless-py/pull/1674)) - fix: unexpected \"missing-label\" error with option `header_case = False` ([#1635](https://github.com/frictionlessdata/frictionless-py/issues/1635)) - fix: KeyError when a \"primaryKey\" is missing ([#1633](https://github.com/frictionlessdata/frictionless-py/issues/1633)) - fix: unexpected field-error for a boolean \"example\" with \"trueValues\" or \"falseValues\" properties ([#1610](https://github.com/frictionlessdata/frictionless-py/issues/1610)) ## v5.15 - Local development has been migrated to using [Hatch](https://hatch.pypa.io/latest/) ## v5.14 - Rebased packaging on PEP 621 - Extracted experimental application/server from the codebase ## v5.13 - Implemented \"Metadata.from_descriptor(allow_invalid=False)\" (#1501) ## v5.10 - Various architectural and standards-compatibility improvements (minor breaking changes): - Added new Console commands: - list - explore - query - script - convert - publish - Rebased Console commands on Rich (nice output in the Console) - Fixed `extract` returning the results depends on the source type (now it\u0027s always a dictionary indexed by the resource name) - Enforced type safety -- many tabular command will be marked as impossible for non-tabular resources if a type checker is used - Improved `frictionless.Resource(source)` guessing abilities; if you just like to open a table resource use `frictionless.resources.TableResource(path=path)` ## v5.8 - Implemented Implemented `catalog/dataset/package/resource.deference` (#1451) ## v5.7 - Various architectural and standards-compatibility improvements (minor breaking changes): - Improved type detection mechanism (including remote descriptors) - Added `resources` module including `File/Text/Json/TableResource` - Deprecated `resource.type` argument -- use the classes above - Changed `catalog.packages[]` to `catalog.datasets[].package` - Made `resource.schema` optional (`resource.has_schema` is removed) - Made `resource.normpath` optional (`resource.normdata` is removed) - Standards-compatability improvements: profile, stats - Renamed `system/plugin.select_Check/etc` to `system/plugin.select_check_class/etc` ## v5.6 - Added support for `sqlalchemy@2` (#1427) ## v5.5 - Implemented `program/resource.index` preview (#1395) ## v5.4 - Support `dialect.skip_blank_rows` (#1387) ## v5.3 - Support `steps.resource_update` for resource transformations (#1381) ## v5.2 - Added support for `wkt` format in `fields.StringField` (#1363 by @jze) ## v5.1 - Support `descriptor` argument for `actions/program.extract` (#1372) ## v5.0 - Frictionless Framework (v5) is out of Beta and released on PyPi ## v5.0.0b19 - Implemented [CKAN Integration](https://framework.frictionlessdata.io/docs/portals/ckan.html) ([#1185](https://github.com/frictionlessdata/frictionless-py/issues/1332)) ## v5.0.0b8 - ForeignKeyError has been extended with additional information: `fieldNames`, `fieldCells`, `referenceName`, and `referenceFieldNames` ## v5.0.0b2 - Implemented [Github Integration](https://framework.frictionlessdata.io/docs/portals/github.html) ([#1185](https://github.com/frictionlessdata/frictionless-py/issues/1185)) ## v5.0.0b1 - First beta version of [Frictionless Framework (v5)](https://framework.frictionlessdata.io/blog/2022/08-22-frictionless-framework-v5.html) ## v4.40 - Added Dialect support to packages (#1137) ## v4.39 - Fixed processing of incompatible decimal char in table schema and data (#1089) - Added support for Time Zone data (#1097) - Improved validation messages by adding `summary` and partial validation details (#1106) - Implemented new feature `summary` (#1127) - `schema.to_summary` - `report.to_summary` - Added CLI command `summary` - Fixed file compression `package.to_zip` (#1104) - Implemented feature to validate single resource (#1112) - Improved error message to notify about invalid fields (#1117) - Fixed type conversion of NaN values for data of type Int64 (#1115) - Exposed valid/invalid flags in CLI `extract` command (#1130) - Implemented feature `package.to_er_diagram` (#1135) ## v4.38 - Implemented `checks.ascii_value` (#1064) - Implemented `checks.deviated_cell` (#1069) - Implemented `detector.field_true/false_values` (#1074) ## v4.37 - Deprecated high-level legacy actions (use class-based alternatives): - `describe_*` - `extract_*` - `transform_*` - `validate_*` ## v4.36 - Implemented pipeline actions: - `pipeline.validate` (will replace `validate_pipeline` in v5) - `pipeline.transform` (will replace `transform_pipeline` in v5) - Implemented inqiury actions: - `inqiury.validate` (will replace `validate_inqiury` in v5) ## v4.35 - Implemented schema actions: - `Schema.describe` (will replace `describe_schema` in v5) - `schema.validate` (will replace `validate_schema` in v5) - Implemented new transform steps: - `steps.field_merge` - `steps.field_pack` ## v4.34 - Implemented package actions: - `Package.describe` (will replace `describe_package` in v5) - `package.extract` (will replace `extract_package` in v5) - `package.validate` (will replace `validate_package` in v5) - `package.transform` (will replace `transform_package` in v5) ## v4.33 - Implemented resource actions: - `Resource.describe` (will replace `describe_resource` in v5) - `resource.extract` (will replace `extract_resource` in v5) - `resource.validate` (will replace `validate_resource` in v5) - `resource.transform` (will replace `transform_resource` in v5) ## v4.32 - Added to_markdown() feature to metadata (#1052) ## v4.31 - Added a feature that allows to export table schema as excel (#1040) - Added nontabular note to validation results to indicate nontabular file (#1046) - Excel stats now shows bytes and hash (#1045) - Added pprint feature which displays metadata in a readable and pretty way (#1039) - Improved error message if resource.data is not a string (#1036) ## v4.29 - Made Detector\u0027s private properties public and writable (#1025) ## v4.28 - Improved an order of the metadata in YAML representation ## v4.27 - Exposed Dialect options via CLI such as `sheet`, `table`, `keys`, and `keyed` (#886) ## v4.26 - Validate \u0027schema.fields[].example\u0027 (#998) ## v4.25 - Allows descriptors that subclass collections.abc.Mapping (#985) ## v4.24 - Added support for `SqlDialect.basepath` (#982) (https://framework.frictionlessdata.io/docs/tutorials/formats/sql-tutorial) ## v4.23 - Added table dimensions check (#985) ## v4.22 - Added \"extract --trusted\" flag ## v4.21 - Added \"--json/yaml\" CLI options for transform ## v4.20 - Improved layout/schema detection algorithms (#945) ## v4.19 - Renamed `inlineDialect.keys` to `inlineDialect.data_keys` due to a conflict with `dict.keys` property ## v4.18 - Normalized metadata properties (increased type safety) ## v4.17 - Add fields, limit, sort and filter options to CkanDialect (#912) ## v4.16 - Implemented `system/plugin.create_candidates` (#893) ## v4.15 - Implemented `system.get/use_http_session` (#892) ## v4.14 - SQL Where Clause (#882) ## v4.13 - Implemented descriptor type detection for `extract/validate` (#881) ## v4.12 - Support external profiles for data package (#864) ## v4.11 - Added `json` argument to `resource.to_snap` ## v4.10 - Support resource/field renaming in transform (#843) ## v4.9 - Support `--path` CLI argument (#829) ## v4.8 - Added support for `Package(innerpath)` argument for unzipping a data package\u0027s descriptor ## v4.7 - Support control/dialect as JSON in CLI (#806) ## v4.6 - Implemented `describe_dialect` and `describe(path, type=\"dialect\")` - Support `--dialect` argument in CLI ## v4.5 - Implemented `Schema.from_jsonschema` (#797) ## v4.4 - Use `field.constraints.maxLength` for SQL\u0027s VARCHAR (#795) ## v4.3 - Implemented `resource.to_view()` (#781) ## v4.2 - Make `fields[].arrayItem` errors more granular (#767) ## v4.1 - Added support for `fields[].arrayItem` (#750) ## v4.0 - Released `frictionless@4` :tada: ## v4.0.0a15 - Updated loaders (#658) (BREAKING) - Renamed `filelike` loader to `stream` loader - Migrated from `text` loader to `buffer` loader ## v4.0.0a14 - Improve transform API (#657) (BREAKING) - Swithed to the `transform_resource(resource)` signature - Swithed to the `transform_package(package)` signature ## v4.0.0a13 - Improved resource/package import/export (#655) (BREAKING) - Reworked `parser.write_row_stream` API - Reworked `resource.from/to` API - Reworked `package.from/to` API - Reworked `Storage` API - Reworked `system.create_storage` API - Merged `PandasStorage` into `PandasParser` - Merged `SpssStorage` into `SpssParser` ## v4.0.0a12 - Improved transformation steps (#650) (BREAKING) - Split value/formula/function concepts - Renamed a few minor step arguments ## v4.0.0a11 - Improved layout and data streams concepts (#648) (BREAKING) - Renamed `data_stream` to `list_stream` - Renamed `readData` to `readLists` - Renamed `sample` to `fragment` (`sample` now is raw lists) - Implemented loader.buffer - Implemented parser.sample - Added support for function based checks - Added support for function based steps ## v4.0.0a10 - Reworked Error.tags (BREAKING) - Reworked Check API and split labels/header (BREAKING) ## v4.0.0a9 - Rebased on `Detector` class (BREAKING) - Migrated all infer\\_\\*, sync/patch_schema and detect_encoding parameters to `Detector` - Made `resource.infer` omit empty objects - Added `resource.read_*(size)` argument - Added `resource.labels` property ## v4.0.0a8 - Improved checks/steps API (#621) (BREAKING) - Updated `validate(extra_checks=[...])` to `validate(checks=[{\"code\": \u0027code\u0027, ...}])` ## v4.0.0a7 - Updated describe/extract/transform/validate APIs (BREAKING) - Removed `validate_table` (use `validate_resource`) - Removed legacy `Table` and `File` classes - Removed `dataflows` plugin - Replaced `nopool` by `parallel` (not parallel by default) - Renamed `report.tables` to `report.tasks` - Rebased on `report.tasks[].resource` (instead of plain path/scheme/format/etc) - Flatten Pipeline steps signature ## v4.0.0a6 - Introduced Layout class (BREAKING) - Renamed `Query` class and arguments/properties to `Layout` - Moved `header` options from `Dialect` to `Layout` ## v4.0.0a5 - Updated transform API - Added `transform(type)` argument ## v4.0.0a4 - Updated describe API (BREAKING) - Renamed `describe(source_type)` argument to `type` ## v4.0.0a3 - Updated extract API (BREAKING) - Removed `extract_table` (use `extract_resource` with the same API) - Renamed `extract(source_type)` argument to `type` ## v4.0.0a1 - Initial API/codebase improvements for v4 (BREAKING) - Allow `Package/Resource(source)` notation (guess descriptor/path/etc) - Renamed `schema.infer` -\u003e `Schema.from_sample` - Renamed `resource.inline` -\u003e `resource.memory` - Renamed `compression_path` -\u003e `innerpath` - Renamed `compression: no` -\u003e `compression: \"\"` - Updated `Package/Resource.infer` not to infer stats (use `stats=True`) - Removed `Package/Resource.infer(only_sample)` argument - Removed `Resouce.from/to_zip` (use `Package.from/to_zip`) - Removed `Resouce.source` (use `Resource.data` or `Resource.fullpath`) - Removed `package/resource.infer(source)` argument (use constructors) - Added some new API (will be covered in the updated docs after the v4 release) ## v3.48 - Make Resource independent from Table/File (#607) (BREAKING) - Resource can be opened like Table (it\u0027s recommended to use Resource instead of Table) - Renamed `resource.read_sample()` to `resource.sample` - Renamed `resource.read_header()` to `resource.header` - Renamed `resource.read_stats()` to `resource.stats` - Removed `resource.to_table()` - Removed `resource.to_file()` ## v3.47 - Optimize Row/Header/Table and rename header errors (#601) (BREAKING) - Row object is now lazy; it casts data on-demand preserving the same API - Method `resource/table.read_data(_stream)` now includes a header row if present - Renamed `errors.ExtraHeaderError-\u003eExtraLabelError` (`extra-label-error`) - Renamed `errors.MissingHeaderError-\u003eMissingLabelError` (`missing-label-error`) - Renamed `errors.BlankHeaderError-\u003eBlankLabelError` (`blank-label-error`) - Renamed `errors.DuplicateHeaderError-\u003eDuplicateLabelError` (`duplicate-label-error`) - Renamed `errors.NonMatchingHeaderError-\u003eIncorrectLabelError` (`incorrect-label-error`) - Renamed `schema.read/write_data-\u003eread/write_cells` ## v3.46 - Renamed aws plugin to s3 (#594) (BREAKING) ```bash $ pip install frictionless[aws] # before $ pip install frictionless[s3] # after ``` ## v3.45 - Drafted support for writing Multipart Data (#583) ## v3.44 - Added support for writing to Remote Data (#582) ## v3.43 - Add support to writing to Google Sheets (#581) - Renamed `gsheet` plugin/format to `gsheets` (BREAKING: minor) ## v3.42 - Added support for writing to S3 (#580) ## v3.41 - Update Loader/Parser API to write to different targets (#579) (BREAKING: minor) ## v3.40 - Implemented a standalone multipart loader (#573) ## v3.39 - Fixed Header not being an original one (#572) - Fix bad format validation (#571) - Added default errors limit equals to 1000 (#570) - Added support for field.float_number (#569) ## v3.38 - Improved ckan plugin (#560) ## v3.37 - Remove not working elastic plugin draft (#558) ## v3.36 - Support custom types (#557) ## v3.35 - Added \"resolve\" option to \"resource/package.to_zip\" (#556) ## v3.34 - Moved `frictionless.controls` to `frictionless.plugins.*` (BREAKING) - Moved `frictionless.dialects` to `frictionless.plugins.*` (BREAKING) - Moved `frictionless.exceptions.FrictionlessException` to `frictionless.FrictionlessException` (BREAKING) - Moved `excel` dependencies to `frictionless[excel]` extras (BREAKING) - Moved `json` dependencies to `frictionless[json]` extras (BREAKING) - Consider `json` files to be a metadata by default (BREAKING) Code example: ```python # Before # pip install frictionless from frictionless import dialects, exceptions excel_dialect = dialects.ExcelDialect() json_dialect = dialects.JsonDialect() exception = exceptions.FrictionlessException() # After # pip install frictionless[excel,json] from frictionless import FrictionlessException from frictionless.plugins.excel import ExcelDialect from frictionless.plugins.json import JsonDialect excel_dialect = dialects.ExcelDialect() json_dialect = dialects.JsonDialect() exception = FrictionlessException() ``` ## v3.33 - Implemented resource.write (#537) ## v3.32 - Added url parameter to SQL import/export (#535) ## v3.31 - Made tables with header and no data rows valid (#534) (BREAKING: minor) ## v3.30 - Various CLI improvements (#532) - Added autocompletion - Added stdin support - Added \"extract --csv\" - Exposed more options ## v3.29 - Added experimental CKAN support (#528) ## v3.28 - Add a \"nopool\" argument to validate (#527) ## v3.27 - Stop sorting keyed sources as the order is now guaranteed by Python (#512) (BREAKING) ## v3.26 - Added \"nolookup\" argument for validate_package (#515) ## v3.25 - Add transform functionality (#505) - Methods `schema.get/remove_field` now raise if not found (#505) (BREAKING) - Methods `package.get/remove_resource` now raise if not found (#505) (BREAKING) ## v3.24 - Lower case resource.scheme/format/hashing/encoding/compression (#499) (BREAKING) ## v3.23 - Support \"header_case\" option for dialects (#488) ## v3.22 - Added suppport for DB2 format (#485) ## v3.21 - Improved SPSS plugin (#483) - Improved BigQuery plugin (#470) ## v3.20 - Added support for SQL Views (#466) ## v3.19 - Rebased AwsLoader on streaming (#460) ## v3.18 - Added `hashing` parameter to `describe/describe_package` - Removed `table.onerror` property (BREAKING) ## v3.17 - Added timezone for datetime/time parsing (#457) (BREAKING) ## v3.16 - Fixed metadata.to_yaml (#455) - Removed the `expand` argument from `metadata.to_dict` (BREAKING) ## v3.15 - Added native schema support to SqlParser (#452) ## v3.14 - Make Resource the main internal interface (#446) (BREAKING: for plugin authors) - Move Resource\u0027s stats to `resource.stats` (BREAKING) - Rename `on_error` to `onerror` (BREAKING) - Added `resource.stats.fields` ## v3.13 - Add an `on_error` argument to Table/Resource/Package (#445) ## v3.12 - Added streaming to the extract functions (#442) ## v3.11 - Added experimental BigQuery support (#424) ## v3.10 - Added experimental SPSS support (#421) ## v3.9 - Rebased on a `goodtables` successor versioning ## v3.8 - Add support SQL/Pandas import/export (#31) ## v3.7 - Add support for custom JSONEncoder classes (#24) ## v3.6 - Normalize header terminology ## v3.5 - Initial public version",
      },
          'docs/codebase/contributing': {
          'name': 'Contributing',
          'path': 'docs/codebase/contributing',
          'relpath': '../codebase/contributing',
          'text': "# Contributing We welcome contributions from anyone! Please read the following guidelines, and feel free to reach out to us if you have questions. Thanks for your interest in helping make Frictionless awesome! ## Introduction We use Github as a code and issues hosting platform. To report a bug or propose a new feature, please open an issue. For pull requests, we would ask you initially create an issue and then create a pull requests linked to this issue. ## Prerequisites To start working on the project: - Python 3.10+ Install Python headers if they are missing: ```bash sudo apt-get install libpython3.10-dev ``` ## Enviroment For development orchestration we use [Hatch](https://github.com/pypa/hatch) for Python (defined in `pyproject.toml`). We use `make` to run high-level commands (defined in `Makefile`) ```bash pip3 install hatch ``` Before starting with the project we recommend configuring `hatch`. The following line will ensure that all the virtual environments will be stored in the `.python` directory in the project root: ```bash hatch config set \u0027dirs.env.virtual\u0027 \u0027.python\u0027 ``` Now you can setup you IDE to use a proper Python path: ```bash .python/frictionless/bin/python ``` Enter the virtual environment before starting the work. It will ensure that all the development dependencies are installed into a virtual environment: ```bash hatch shell ``` ### Using Docker Use the following command to build the container: ```bash tabs=CLI hatch run image ``` This should take care of setting up everything. If the container is built without errors, you can then run commands like `hatch` inside the container to accomplish various tasks (see the next section for details). To make things easier, we can create an alias: ```bash tabs=CLI alias \"frictionless-dev=docker run --rm -v $PWD:/home/frictionless -it frictionless-dev\" ``` Then, for example, to run the tests, we can use: ```bash tabs=CLI frictionless-dev hatch run test ``` ## Development ### Codebase Frictionless is a Python3.8+ framework, and it uses some common Python tools for the development process (we recommend enabling support of these tools in your IDE): - linting/formatting: `ruff` - type checking: `pyright` - code testing: `pytest` You also need `git` to work on the project. ### Documentation To contribute to the documentation, please find an article in the `docs` folder and update its contents. We write our documentation using [Livemark](https://livemark.frictionlessdata.io). Livemark provides an ability to provide examples without providing an output as it\u0027s generated automatically. It\u0027s possible to run this documentation portal locally: ```bash tabs=CLI livemark start ``` ### Running tests offline VCR library records the response from HTTP requests locally as cassette in its first run. All subsequent calls are run using recorded metadata from previous HTTP request, so it speeds up the testing process. To record a unit test(as cassette), we mark it with a decorator: ```python @pytest.mark.vcr def test_connect_with_server(): pass ``` Cassettee will be recorded as \"test_connect_with_server.yaml\". A new call is made when params change. To skip sensitive data, we can use filters: ```python @pytest.fixture(scope=\"module\") def vcr_config(): return {\"filter_headers\": [\"authorization\"]} ``` #### Regenerating cassettes for CKAN - Setup CKAN local instance: https://github.com/okfn/docker-ckan - Create a sysadmin account and generate api token - Set apikey token in .env file ``` CKAN_APIKEY=*************************** ``` #### Regenerating cassettes for Zenodo **Read** - To read, we need to use live site, the api library uses it by default. - Login to zenodo if you have an account and create an access token. - Set access token in .env file. ``` ZENODO_ACCESS_TOKEN=*************************** ``` **Write** - To write we can use either live site or sandbox. We recommend to use sandbox (https://sandbox.zenodo.org/api/). - Login to zenodo(sandbox) if you have an account and create an access token. - Set access token in .env file. ``` ZENODO_SANDBOX_ACCESS_TOKEN=*************************** ``` - Set base_url in the control params ``` base_url=\u0027base_url=\"https://sandbox.zenodo.org/api/\u0027 ``` #### Regenerating cassettes for Github - Login to github if you have an account and create an access token(Developer settings \u003e Personal access tokens \u003e Tokens). - Set access token and other details in .env file. If email/name of the user is hidden we need to provide those details as well. ``` GITHUB_NAME=FD GITHUB_EMAIL=frictionlessdata@okfn.org GITHUB_ACCESS_TOKEN=*************************** ``` ## Releasing To release a new version: - check that you have push access to the `main` branch - run `hatch version ` to update the version - add changes to `CHANGELOG.md` if it\u0027s not a patch release (major or minor) - run `hatch run release` which create a release commit and tag and push it to Github - an actual release will happen on the Github CI platform after running the tests",
      },
          'docs/universe': {
          'name': 'Universe',
          'path': 'docs/universe',
          'relpath': '../universe',
          'text': "# Universe ## Notebooks - [Frictionless Cars](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/frictionless-cars.ipynb) - [Frictionless Biology](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/reshaping-data-frictionless.ipynb) - [Frictionless Describe and Extract](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/frictionless-tutorial-describe-extract.ipynb) - [Frictionless Excel](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/frictionless-excel.ipynb) - [Frictionless Research Data Management Workflows](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/frictionless-RDM-workflows.ipynb) - [Frictionless Export, Markdown and Other](https://colab.research.google.com/github/frictionlessdata/frictionless-py/blob/v4/site/docs/tutorials/notebooks/tutorial-markdown-export-feature.ipynb)",
      },
          'blog/index': {
          'name': 'Blog',
          'path': 'blog/index',
          'relpath': '../../blog/index',
          'text': "# Blog ```html markup {% for item in document.get_plugin(\u0027blog\u0027).items %} {{ item.document.name }} By {{ item.document.get_plugin(\u0027blog\u0027).author }} on {{ item.document.get_plugin(\u0027blog\u0027).date }} {{ item.document.get_plugin(\u0027blog\u0027).description }} Read more \u00bb {% endfor %} ```",
      },
          'blog/2022/11-07-zenodo-integration': {
          'name': 'Zenodo Integration',
          'path': 'blog/2022/11-07-zenodo-integration',
          'relpath': '../../blog/2022/11-07-zenodo-integration',
          'text': "# Zenodo Integration Zenodo integration was very highly requested feature and we are happy to share our first draft of the plugin which makes sharing data between frictionless and zenodo easier without any extra work and configuration. This plugin uses zenodopy library underneath to communicate with Zenodo REST API. A frictionless user can use the framework functionalities and then easily publish data to zenodo and viceversa. Here is a short description of the features with examples. ## Reading from the repo You can simply read the package or create a new package from the zenodo repository if package does not exists. No additional configuration is required. The existing ```Package``` class identifies zenodo url and reads the packages and resources from the repo. Example of reading package from the zenodo repo is as follows: ```python tabs=Python from frictionless import Package package = Package(\"https://zenodo.org/record/7078760\") print(package) ``` Once read you can apply all the available functions to the package such as validation, transformation etc. ## Writing/Publishing to the repo To write the package we can simply use `publish` function, which will write the package and resource files to zenodo repository. We need to provide meta data for the repository while publishing data which we pass as meta.json as shown in the example below: ```python tabs=Python from frictionless import Package, portals control = portals.ZenodoControl( metafn=\"data/zenodo/metadata.json\", apikey=apikey ) package = Package(\"data/datapackage.json\") deposition_id = package.publish(control=control) print(deposition_id) ``` Once the package is published, deposition_id will be returned. ## Creating catalog Catalog can be created from a single repository or from multiple repositories. Repositories can be searched using any search terms, phrase, field search or combination of all. A simple example of creating catalog from search is as follows: ```python tabs=Python from frictionless import Catalog, portals control=portals.ZenodoControl(search=\u0027title:\"open science\"\u0027) catalog = Catalog( control=control, ) ``` ### Happy Contributors We will have more updates in future and would love to hear from you about this new feature. Let\u0027s chat in our [Slack](https://join.slack.com/t/frictionlessdata/shared_invite/zt-17kpbffnm-tRfDW_wJgOw8tJVLvZTrBg) if you have questions or just want to say hi. Read [Zenodo Plugin Docs](../../docs/portals/zenodo.html) for more information.",
      },
          'blog/2022/09-07-github-integration': {
          'name': 'Github Integration',
          'path': 'blog/2022/09-07-github-integration',
          'relpath': '../../blog/2022/09-07-github-integration',
          'text': "# Github Integration We are happy to announce github plugin which makes sharing data between frictionless and github easier without any extra work and configuration. All the github plugin functionalities are wrapped around the PyGithub library. The main idea is to make the interaction between the framework and github seamless using read and write functions developed on top of the Frictionless python library. Here is a short introduction and examples of the features. ## Reading from the repo Reading package from github repository is made easy! The existing ```Package``` class can identify the github url and read the packages and resources from the repo. It can read packages from repos with or without packages descriptors. If a package descriptor is not defined, it will create a package descriptor with resources that it finds in the repo. ```python tabs=Python from frictionless import Package package = Package(\"https://github.com/fdtester/test-repo-with-datapackage-json\") print(package) ``` ## Writing/Publishing to the repo Writing and publishing can be easily done by passing the repository link using `publish` function. ```python tabs=Python from frictionless import Package, portals apikey = \u0027YOUR-GITHUB-API-KEY\u0027 package = Package(\u0027data/datapackage.json\u0027) response = package.publish(\"https://github.com/fdtester/test-repo-write\", control=portals.GithubControl(apikey=apikey) ) ``` ## Creating catalog Catalog can be created from a single repository by using \u0027search\u0027 queries. Repositories can be searched using combination of any search text and github qualifiers. A simple example of creating catalog from search is as follows: ```python tabs=Python from frictionless import Catalog, portals catalog = Catalog( control=portals.GithubControl(search=\"user:fdtester\", per_page=1, page=1), ) ``` ### Happy Contributors We will have more updates in future and would love to hear from you about this new feature. Let\u0027s chat in our [Slack](https://join.slack.com/t/frictionlessdata/shared_invite/zt-17kpbffnm-tRfDW_wJgOw8tJVLvZTrBg) if you have questions or just want to say hi. Read [Github Plugin Docs](../../docs/portals/github.html) for more information.",
      },
          'blog/2022/08-22-frictionless-framework-v5': {
          'name': 'Welcome Frictionless Framework (v5)',
          'path': 'blog/2022/08-22-frictionless-framework-v5',
          'relpath': '../../blog/2022/08-22-frictionless-framework-v5',
          'text': "# Welcome Frictionless Framework (v5) We\u0027re releasing a first beta of Frictionless Framework (v5)! Since the initial Frictionless Framework release we\u0027d been collecting feedback and analyzing both high-level users\u0027 needs and bug reports to identify shortcomings and areas that can be improved in the next version for the framework. Once that process had been done we started working on a new v5 with a goal to make the framework more bullet-proof, easy to maintain and simplify user interface. Today, this version is almost stable and ready to be published. Let\u0027s go through the main improvements we have made: ## Improved Metadata This year we started working on the Frictionless Application, at the same time, we were thinking about next steps for the [Frictionless Standards](https://specs.frictionlessdata.io/). For both we need well-defined and an easy-to-understand metadata model. Partially it\u0027s already published as standards like Table Schema and partially it\u0027s going to be published as standards like File Dialect and possibly validation/transform metadata. ### Dialect In v4 of the framework we had Control/Dialect/Layout concepts to describe resource details related to different formats and schemes, as well as tabular details like header rows. In v5 it\u0027s merged into the only one concept called Dialect which is going to be standardised as a File Dialect spec. Here is an example: ```yaml tabs=YAML header: true headerRows: [2, 3] commentChar: \u0027#\u0027 csv: delimiter: \u0027;\u0027 ``` ```python tabs=Python from frictionless import Dialect, Control, formats dialect = Dialect(header=True, header_rows=[2, 3], comment_char=\u0027#\u0027) dialect.add_control(formats.CsvControl(delimiter=\u0027;\u0027)) print(dialect) ``` A dialect descriptor can be saved and reused within a resource. Technically, it\u0027s possible to provide different schemes and formats settings within one Dialect (e.g. for CSV and Excel) so it\u0027s possible to create e.g. one re-usable dialect for a data package. A legacy CSV Dialect spec is supported and will be supported forever so it\u0027s possible to provide CSV properties on the root level: ```yaml tabs=YAML header: true delimiter: \u0027;\u0027 ``` ```python tabs=Python from frictionless import Dialect, Control, formats dialect = Dialect.from_descriptor({\"header\": True, \"delimiter\": \u0027;\u0027}) print(dialect) ``` For performance and codebase maintainability reasons some marginal Layout features have been removed completely such as `skip/pick/limit/offsetFields/etc`. It\u0027s possible to achieve the same results using the Pipeline concept as a part of the transformation workflow. Read an article about [Dialect Class](../../docs/framework/dialect.html) for more information. ### Checklist Checklist is a new concept introduced in v5. It\u0027s basically a collection of validation steps and a few other settings to make \"validation rules\" sharable. For example: ```yaml tabs=YAML checks: - type: ascii-value - type: row_constraint formula: id \u003e 1 skipErrors: - duplicate-label ``` ```python tabs=Python from frictionless import Checklist, checks checklist = Checklist( checks=[checks.ascii_value(), checks.row_constraint(formula=\u0027id \u003e 1\u0027)], skip_errors=[\u0027duplicate-label\u0027], ) print(checklist) ``` Having and sharing this checklist it\u0027s possible to tune data quality requirements for some data file or set of data files. This concept will provide an ability for creating data quality \"libraries\" within projects or domains. We can use a checklist for validation: ```bash tabs=CLI frictionless validate table1.csv --checklist checklist.yaml frictionless validate table2.csv --checklist checklist.yaml ``` Here is a list of another changes: From (v4) | To (v5) ------------------- | ------------------------------ Check(descriptor) | Check.from_descriptor(descriptor) check.code | check.type Read an article about [Checklist Class](../../docs/framework/checklist.html) for more information. ### Pipeline In v4 Pipeline was a complex concept similar to validation Inquiry. We reworked it for v5 to be a lightweight set of validation steps that can be applied to a data resource or a data package. For example: ```yaml tabs=YAML steps: - type: table-normalize - type: cell-set fieldName: version value: v5 ``` ```python tabs=Python from frictionless import Pipeline, steps pipeline = Pipeline( steps=[steps.table_normalize(), steps.cell_set(field_name=\u0027version\u0027, value=\u0027v5\u0027)], ) print(pipeline) ``` Similar to the Checklist concept, Pipeline is a reusable (data-abstract) object that can be saved to a descriptor and used in some complex data workflow: ```bash tabs=CLI frictionless transform table1.csv --pipeline pipeline.yaml frictionless transform table2.csv --pipeline pipeline.yaml ``` Here is a list of another changes: From (v4) | To (v5) ------------------ | ------------------------------ Step(descriptor) | Step.from_descriptor(descriptor) step.code | step.type Read an article about [Pipeline Class](../../docs/framework/pipeline.html) for more information. ### Resource ```markdown remark type=warning Since `frictionless@5.7` this experimental feature (`resource.checklist/pipeline`) has been disabled to conform better with the standards. ``` There are no changes in the Resource related to the standards although currently by default instead of `profile` the `type` property will be used to mark a resource as a table. It can be changed using the `--standards v1` flag. It\u0027s now possible to set Checklist and Pipeline as a Resource property similar to Dialect and Schema: ```yaml tabs=YAML path: table.csv # ... checklist: checks: - type: ascii-value - type: row_constraint formula: id \u003e 1 pipeline: pipeline.yaml steps: - type: table-normalize - type: cell-set fieldName: version value: v5 ``` Or using dereference: ```yaml tabs=YAML path: table.csv # ... checklist: checklist.yaml pipeline: pipeline.yaml ``` In this case the validation/transformation will use it by default providing an ability to ship validation rules and transformation pipelines within resources and packages. This is an important development for data publishers who want to define what they consider to be valid for their datasets as well as sharing raw data with a cleaning pipeline steps: ```bash tabs=CLI frictionless validate resource.yaml # will use the checklist above frictionless transform resource.yaml # will use the pipeline above ``` There are minor changes in the `stats` property. Now it uses named keys to simplify hash distinction (md5/sha256 are calculated by default and it\u0027s not possible to change for performance reasons as it was in v4): ```python tabs=Python from frictionless import describe resource = describe(\u0027table.csv\u0027, stats=True) print(resource.stats) ``` Here is a list of another changes: From (v4) | To (v5) -------------------- | ------------------------------ for row in resource: | for row in resource.row_stream Read an article about [Resource Class](../../docs/framework/resource.html) for more information. ### Package There are no changes in the Package related to the standards although it\u0027s now possible to use resource dereference: ```yaml tabs=YAML name: package resources: - resource1.yaml - resource2.yaml ``` Read an article about [Package Class](../../docs/framework/package.html) for more information. ### Catalog ```markdown remark type=warning Since `frictionless@5.7` this experimental feature is changes and now it requires `catalog.datasets[].package` structure. ``` Catalog is a new concept that is a collection of data packages that can be written inline or using dereference: ```yaml tabs=YAML name: catalog packages: - package1.yaml - package2.yaml ``` Read an article about [Catalog Class](../../docs/framework/catalog.html) for more information. ### Detector Detector is now a metadata class (it wasn\u0027t in v4) so it can be saved and shared as other metadata classes: ```python tabs=Python from frictionless import Detector detector = Detector(sample_size=1000) print(detector) ``` Read an article about [Detector Class](../../docs/framework/detector.html) for more information. ### Inquiry There are few changes in the Inquiry concept which is known for using in the [Frictionless Repository](https://repository.frictionlessdata.io/) project: From (v4) | To (v5) ------------------ | ------------------- inquiryTask.source | inquiryTask.path inquiryTask.source | inquiryTask.resource inquiryTask.source | inquiryTask.package Read an article about [Inquiry Class](../../docs/framework/inquiry.html) for more information. ### Report The Report concept has been significantly simplified by removing the `resource` property from `reportTask`. It\u0027s been replaced by `name/type/place/labels` properties. Also `report.time` is now `report.stats.seconds`. The `report/reportTask.warnings: List[str]` have been added to provide non-error information like reached limits: ```bash tabs=CLI output=yaml frictionless validate table.csv --yaml ``` Here is a list of changes: From (v4) | To (v5) ----------------- | ------------------- report.time | report.stats.seconds reportTask.time | reportTask.stats.seconds reportTask.resource.name | reportTask.name reportTask.resource.profile | reportTask.type reportTask.resource.path | reportTask.place reportTask.resource.schema | reportTask.labels Read an article about [Report Class](../../docs/framework/report.html) for more information. ### Schema Changes in the Schema class: From (v4) | To (v5) -------------------- | ------------------------------ Schema(descriptor) | Schema.from_descriptor(descriptor) ### Error There are a few changes in the Error data structure: From (v4) | To (v5) ---------------- | ------------------- error.code | error.type error.name | error.title error.rowPosition | error.rowNumber error.fieldPosition | error.fieldNumber ### Types Note that all the metadata entities that have multiple implementations in v5 are based on a unified type model. It means that they use the `type` property to provide type information: From (v4) | To (v5) ---------------- | ------------------- resource.profile | resource.type check.code | check.type control.code | control.type error.code | error.type field.type | field.type step.type | step.type The new v5 version still supports old notation in descriptors for backward-compatibility. ## Improved Model It\u0027s been many years that Frictionless were mixing declarative metadata and object model for historical reasons. Since the first implementation of `datapackage` library we used different approaches to sync internal state to provide both interfaces descriptor and object model. In Frictionless Framework v4 this technique had been taken to a really sophisticated level with special observables dictionary classes. It was quite smart and nice-to-use for quick prototyping in REPL but it was really hard to maintain and error-prone. In Framework v5 we finally decided to follow the \"right way\" for handling this problem and split descriptors and object model completely. ### Descriptors In the Frictionless World we deal with a lot of declarative metadata descriptors such as packages, schemas, pipelines, etc. Nothing changes in v5 regarding this. So for example here is a Table Schema: ```yaml tabs=YAML fields: - name: id type: integer - name: name type: string ``` ### Object Model The difference comes here we we create a metadata instance based on this descriptor. In v4 all the metadata classes were a subclasses of the dict class providing a mix between a descriptor and object model for state management. In v5 there is a clear boundary between descriptor and object model. All the state are managed as it should be in a normal Python class using class attributes: ```python tabs=Python from frictionless import Schema schema = Schema.from_descriptor(\u0027schema.yaml\u0027) # Here we deal with a proper object model descriptor = schema.to_descriptor() # Here we export it back to be a descriptor ``` There are a few important traits of the new model: - it\u0027s not possible to create a metadata instance from an invalid descriptor - it\u0027s almost always guaranteed that a metadata instance is valid - it\u0027s not possible to mix dicts and classes in methods like `package.add_resource` - it\u0027s not possible to export an invalid descriptor This separation might make one to add a few additional lines of code, but it gives us much less fragile programs in the end. It\u0027s especially important for software integrators who want to be sure that they write working code. At the same time, for quick prototyping and discovery Frictionless still provides high-level actions like `validate` function that are more forgiving regarding user input. ### Static Typing One of the most important consequences of \"fixing\" state management in Frictionless is our new ability to provide static typing for the framework codebase. This work is in progress but we have already added a lot of types and it successfully pass `pyright` validation. We highly recommend enabling `pyright` in your IDE to see all the type problems in-advance: ```yaml image path: ../../assets/type-error.png height: unset width: unset ``` ## Livemark Docs We\u0027re happy to announce that we\u0027re finally ready to drop a JavaScript dependency for the docs generation as we migrated it to Livemark. Moreover, Livemark\u0027s ability to execute scripts inside the documentation and other nifty features like simple Tabs or a reference generator will save us hours and hours for writing better docs. ### Script Execution ```yaml image path: ../../assets/livemark-1.png height: unset width: unset ``` ### Reference Generation ```yaml image path: ../../assets/livemark-2.png height: unset width: unset ``` ### Happy Contributors We hope that Livemark docs writing experience will make our contributors happier and allow to grow our community of Frictionless Authors and Users. Let\u0027s chat in our [Slack](https://join.slack.com/t/frictionlessdata/shared_invite/zt-17kpbffnm-tRfDW_wJgOw8tJVLvZTrBg) if you have questions or just want to say hi. Read [Livemark Docs](https://livemark.frictionlessdata.io/) for more information.",
      },
      };
  const searchIndex = lunr(function () {
    this.ref("path")
    this.field("name", { boost: 10 })
    this.field("text")
    for (const item of Object.values(searchItems)) {
      this.add(item)
    }
  });
  const searchOutput = document.getElementById('livemark-search-output')
  const searchInput = document.getElementById('livemark-search-input')
  searchInput.addEventListener('input', search)
  prepare()
  search()
});

</script>

<div id="livemark-search">
  <div id="livemark-search-output"></div>
  <input id="livemark-search-input" type="search" placeholder="Search...">
</div>
<script>

document.addEventListener("DOMContentLoaded", function () {
  // Add buttons
  $("h2").append('<a href="" class="livemark-source-button">Source</a>');

  // Enable buttons
  $(".livemark-source-button").click(async (ev) => {
    ev.preventDefault();

    // Close open
    if ($(".livemark-source-section").length) {
      $(".livemark-source-section").remove();
      return;
    }

    // Load content
    let source = location.href.replace(".html", ".md");
    if (!source.endsWith(".md")) source = `${source}index.md`;
    const heading = $(ev.target).parent().contents().get(0).nodeValue;
    response = await fetch(source);
    content = await response.text();

    // Extract section
    let isCapture;
    const lines = [];
    for (const line of content.split(/\r?\n/)) {
      if (line.startsWith("##")) {
        isCapture = line.startsWith(`## ${heading}`) ? true : false;
        continue;
      }
      if (isCapture) {
        lines.push(line);
      }
    }
    const section = _.escape(lines.join("\n").trim());

    // Show section
    $(ev.target)
      .parent()
      .after(
        `<pre class="livemark-source-section" style="white-space: pre-wrap;">${section}</pre>`
      );
  });
});

</script>
</body>
</html>